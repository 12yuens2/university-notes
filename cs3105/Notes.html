<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="Notes.tex"> 
<meta name="date" content="2017-06-12 04:14:00"> 
<link rel="stylesheet" type="text/css" href="Notes.css"> 
</head><body 
>
   <h3 class="likesectionHead"><a 
 id="x1-1000"></a>Contents</h3>
   <div class="tableofcontents">
   <span class="sectionToc" >1 <a 
href="#x1-20001" id="QQ2-1-2">History and Philosophy of AI</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.1 <a 
href="#x1-30001.1" id="QQ2-1-3">The Mechanical Turk</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.2 <a 
href="#x1-40001.2" id="QQ2-1-4">Asimov&#8217;s Three Laws of Robotics</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.3 <a 
href="#x1-50001.3" id="QQ2-1-5">The Turing Test</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.4 <a 
href="#x1-60001.4" id="QQ2-1-6">Birth of Artificial Intelligence</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.5 <a 
href="#x1-70001.5" id="QQ2-1-7">Success, Hubris, Nemesis</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.6 <a 
href="#x1-110001.6" id="QQ2-1-11">Weak and Strong AI</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.7 <a 
href="#x1-120001.7" id="QQ2-1-12">Attacks on the Turing Test</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.8 <a 
href="#x1-150001.8" id="QQ2-1-15">The Chinese Room</a></span>
<br />   <span class="sectionToc" >2 <a 
href="#x1-160002" id="QQ2-1-16">Search</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.1 <a 
href="#x1-170002.1" id="QQ2-1-17">What is search in AI?</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.2 <a 
href="#x1-210002.2" id="QQ2-1-21">Presenting Search Abstractly</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.3 <a 
href="#x1-220002.3" id="QQ2-1-22">Search Trees</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.4 <a 
href="#x1-230002.4" id="QQ2-1-23">Measuring the performance of search algorithms</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.5 <a 
href="#x1-240002.5" id="QQ2-1-24">Complete search algorithms</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.6 <a 
href="#x1-300002.6" id="QQ2-1-30">Heuristic search strategies</a></span>
<br />   &#x00A0;<span class="subsectionToc" >2.7 <a 
href="#x1-340002.7" id="QQ2-1-34">Incomplete Search</a></span>
<br />   <span class="sectionToc" >3 <a 
href="#x1-410003" id="QQ2-1-42">Machine Learning</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.1 <a 
href="#x1-420003.1" id="QQ2-1-43">Forms of learning</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.2 <a 
href="#x1-470003.2" id="QQ2-1-48">Decision Trees</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.3 <a 
href="#x1-520003.3" id="QQ2-1-54">Neural networks</a></span>
<br />   <span class="sectionToc" >4 <a 
href="#x1-530004" id="QQ2-1-55">Games playing</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.1 <a 
href="#x1-540004.1" id="QQ2-1-56">Game trees</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.2 <a 
href="#x1-550004.2" id="QQ2-1-57">The minimax algorithm</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.3 <a 
href="#x1-560004.3" id="QQ2-1-58">Alpha-Beta pruning</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.4 <a 
href="#x1-570004.4" id="QQ2-1-59">Endgame database</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.5 <a 
href="#x1-580004.5" id="QQ2-1-60">Monte Carlo Tree Search</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.6 <a 
href="#x1-590004.6" id="QQ2-1-61">Solving checkers</a></span>
<br />   <span class="sectionToc" >5 <a 
href="#x1-600005" id="QQ2-1-62">Automated Reasoning</a></span>
<br />   &#x00A0;<span class="subsectionToc" >5.1 <a 
href="#x1-610005.1" id="QQ2-1-63">What is logic?</a></span>
<br />   &#x00A0;<span class="subsectionToc" >5.2 <a 
href="#x1-630005.2" id="QQ2-1-65">Propositional logic</a></span>
<br />   &#x00A0;<span class="subsectionToc" >5.3 <a 
href="#x1-670005.3" id="QQ2-1-69">First-Order logic</a></span>
<br />   &#x00A0;<span class="subsectionToc" >5.4 <a 
href="#x1-700005.4" id="QQ2-1-72">Resolution</a></span>
<br />   <span class="sectionToc" >6 <a 
href="#x1-730006" id="QQ2-1-75">Bayesian probabilistic reasoning</a></span>
<br />   &#x00A0;<span class="subsectionToc" >6.1 <a 
href="#x1-740006.1" id="QQ2-1-76">Probability theory</a></span>
<br />   &#x00A0;<span class="subsectionToc" >6.2 <a 
href="#x1-800006.2" id="QQ2-1-82">Bayes&#8217; Rule</a></span>
<br />   &#x00A0;<span class="subsectionToc" >6.3 <a 
href="#x1-820006.3" id="QQ2-1-84">Bayesian Networks</a></span>
   </div>
                                                                  

                                                                  
<!--l. 51--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-20001"></a>History and Philosophy of AI</h3>
<!--l. 52--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">1.1   </span> <a 
 id="x1-30001.1"></a>The Mechanical Turk</h4>
<!--l. 53--><p class="noindent" >The Mechanical Turk was a chess playing automaton built in 1770. It was quite good
at chess but didn&#8217;t beat <span 
class="cmti-10">everybody</span>. It&#8217;s creator would always opened the doors and
drawers for everyone to see the machinery inside before it started playing.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 56--><p class="noindent" ><img 
src="imgs/turk.jpg" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 57--><p class="indent" >   </div><hr class="endfigure">
<!--l. 58--><p class="noindent" >However, this was a hoax as there was actually a small human player hiding inside
who moved around inside to hide as different drawers were opened. He could see the
moves being made through a magnetic chess set and could make moves by
moving the Turk&#8217;s arms. This was using human intelligence to <span 
class="cmti-10">fake </span>artificial
intelligence.<br 
class="newline" />Despite being fake, this is an example of people&#8217;s interest very early in history of
Artificial Intelligence.
   <h4 class="subsectionHead"><span class="titlemark">1.2   </span> <a 
 id="x1-40001.2"></a>Asimov&#8217;s Three Laws of Robotics</h4>
<!--l. 63--><p class="noindent" >The famous science fiction author <span 
class="cmbx-10">Issac Asimov </span>developed the <span 
class="cmbx-10">Three Laws of</span>
<span 
class="cmbx-10">Robotics </span>in the 1940s. The laws are as follows:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem">A robot may not injure a human being or, through inaction, allow a human
     being to come to harm.
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem">A robot must obey the orders given it by human beings except where such
     orders would conflict with the First Law
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem">A robot must protect its own existence as long as such protection does
     not conflict with the First or Second Laws</dd></dl>
<!--l. 69--><p class="noindent" >Artificial intelligence and robots turning on humanity was a popular topic in science
fiction novels and Asimov&#8217;s laws was his take on these plots, not to have the robots
&#8220;turn stupidly on [their] creator for no purpose&#8221;. The laws pose an interesting
question in AI ethics. For example the famous trolley ethical experiment of whether
one should pull the lever to save five but kill one - what should an AI do given the
situation?
<!--l. 72--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 74--><p class="noindent" ><img 
src="imgs/trolley.jpg" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 75--><p class="indent" >   </div><hr class="endfigure">
<!--l. 76--><p class="noindent" >Would we program our self-driving cars to kill ourselves in a situation like
this?
   <h4 class="subsectionHead"><span class="titlemark">1.3   </span> <a 
 id="x1-50001.3"></a>The Turing Test</h4>
<!--l. 80--><p class="noindent" >In 1950, Turing wrote about what is now called the <span 
class="cmbx-10">Turing test </span>and posing the
question <span 
class="cmbx-10">Can Machines Think? </span>However, instead of answer the question, Turing
poses to replace the question by another.<br 
class="newline" />He comes up with what he calls the <span 
class="cmbx-10">Imitation game</span>, where a computer has to try
to imitate a human so the person talking to it can&#8217;t tell the difference. In the original
paper, the imitation game consists of three people: a man (A), a woman
(B) and an interrogator (C). The interrogator stays in a room apart from
the other two. The object of the game for the interrogator is to determine
which of the other two is the man and which is the woman. A tries to get
the interrogator to guess wrong while B tries to help the interrogator. The
interrogator is allowed to put questions to A and B. Now he asks the question,
&#8220;What will happen when a machine takes the part of A in this game?&#8221;,
&#8220;Will the interrogator decide wrongly as often when the game is played like
this as he does when the game is played between a man and a woman?&#8221;.
However he is <span 
class="cmti-10">not saying </span>this automatically means the computer can think.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 85--><p class="noindent" ><img 
src="imgs/turingtest.jpg" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 86--><p class="indent" >   </div><hr class="endfigure">
<!--l. 87--><p class="noindent" >Nowadays, the imitation game is usually about whether we can tell if we are talking
to a human or a computer, not the man/woman version in Turing&#8217;s original paper.
Turing also had some sample Q&amp;As on how this should go:<br 
class="newline" /><span 
class="cmbx-10">Q: </span>Write me a sonnet on the subject of the Forth Bridge <br 
class="newline" /><span 
class="cmbx-10">A: </span>Count me out on this one. I never could write poetry.<br 
class="newline" /><span 
class="cmbx-10">Q: </span>Add 34957 to 70764 <br 
class="newline" /><span 
class="cmbx-10">A: </span>(Pause about 30 seconds and then give as answer) 105621<br 
class="newline" /><span 
class="cmbx-10">Q: </span>Do you play chess? <br 
class="newline" /><span 
class="cmbx-10">A: </span>Yes <br 
class="newline" /><span 
class="cmbx-10">Q: </span>I have K at my K1 and no other pieces. You have only K at K6 and R at R1. It is
your move. What do you play? <br 
class="newline" /><span 
class="cmbx-10">A: </span>(After a pause of 15 seconds) R-R8 mate.<br 
class="newline" />The Turing test has been used widely as a test for artificial intelligence and for a long
time there has been an annual competition (The Loebner prize) for chat bots.
However, the Turing test as been criticized to not properly test of artificial
intelligence as people build bots specifically to pass the Turing test (such as <span 
class="cmtt-10">PARRY</span>)
that are not intelligent at all and instead reflect or play the role of a child or a
schizophrenic.
   <h4 class="subsectionHead"><span class="titlemark">1.4   </span> <a 
 id="x1-60001.4"></a>Birth of Artificial Intelligence</h4>
<!--l. 101--><p class="noindent" >In the summer of 1956, a two-month workshop was organized at Dartmouth College
for U.S researchers in the field. This was where the field of AI research was born. Not
much was accomplished at the workshop, but for the next 20 years, the people who
were there, their student and colleagues at MIT, CMU, Stanford and IBM would
dominate the field of AI.<br 
class="newline" />There were 10 attendees in total, and all were &#8217;superstars&#8217; in the field of computer
science. This was also where the term <span 
class="cmbx-10">Artificial Intelligence </span>first started being
used, thanks of John McCarthy.
<!--l. 105--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">1.5   </span> <a 
 id="x1-70001.5"></a>Success, Hubris, Nemesis</h4>
<!--l. 106--><p class="noindent" >In the history of AI, there has been a common repeated pattern of <span 
class="cmbx-10">Success </span><span 
class="cmsy-10">&#x2192;</span>
                                                                  

                                                                  
<span 
class="cmbx-10">Hubris </span><span 
class="cmsy-10">&#x2192; </span><span 
class="cmbx-10">Nemesis</span>.<br 
class="newline" /><span 
class="cmbx-10">Success </span><br 
class="newline" />Something amazing happened, like a breakthrough, or perhaps unrelated that makes
everyone think we can do something amazing.<br 
class="newline" /><span 
class="cmbx-10">Hubris </span><br 
class="newline" />Something very big and ambitious must be easily achievable thanks to AI and it
shouldn&#8217;t take very long because AI is amazing.<br 
class="newline" /><span 
class="cmbx-10">Nemesis </span><br 
class="newline" />It was much harder than we thought and it may not be possible at all. It currently
seems like we are in an AI Success/Hubris position again with AI coming up
everywhere (AlphaGo, driverless cards etc.), but is this just yet another loop of
Hubris/Nemesis?
<!--l. 117--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">1.5.1   </span> <a 
 id="x1-80001.5.1"></a>Maching Learning</h5>
<!--l. 118--><p class="noindent" ><span 
class="cmbx-10">Success </span><br 
class="newline" />1943 - Neural networks proposed by McCulloch and Pitts <br 
class="newline" />1951 - Marvin Minsky built first artificial neural network <br 
class="newline" />1957 - Frank Rosenblatt invented the <span 
class="cmti-10">perceptron </span>and was able to teach it to learn
some boolean functions<br 
class="newline" /><span 
class="cmbx-10">Hubris </span><br 
class="newline" />With artificial neurons we should be able to do anything, since they were inspired by
the brain so we can just simulate the brain and then we&#8217;ve built an artificial
brain!<br 
class="newline" /><span 
class="cmbx-10">Nemesis </span><br 
class="newline" />Minsky and Seymour Papert wrote a book <span 
class="cmti-10">Perceptrons, 1969 </span>which argued
that neural networks were limited in scope, including some impossibility
theorems. This led to neural network research moribund for the next 15
years.<br 
class="newline" />However now since the invention of back propagation by Werbos, neural
networks are a big part of machine learning (for example AlphaGo) and
again we have success and hubris for the potential that neural networks can
                                                                  

                                                                  
achieve.
<!--l. 131--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">1.5.2   </span> <a 
 id="x1-90001.5.2"></a>Automated Reasoning</h5>
<!--l. 132--><p class="noindent" ><span 
class="cmbx-10">Success </span><br 
class="newline" />In 1956, Newell, Simon and Shaw were working on a logic theorem prover that proved
some of the theorems in &#8220;Principia Mathematica&#8221;. Not all the theorems were proved,
but the approach seemed valid.<br 
class="newline" /><span 
class="cmbx-10">Hubris </span><br 
class="newline" />In 1959, Newell, Simon and Shaw generalised their approach from logic to all
problems to create a&#8220;General Problem Solver&#8221; using a similar approach as their logic
machine in the hopes it can solve <span 
class="cmti-10">any </span>problem.<br 
class="newline" /><span 
class="cmbx-10">Nemesis </span><br 
class="newline" />The General Problem Solver never solved anything interesting as the combinatorial
explosion was just too big and they did no have good heuristics. Many of the
theorems in Principia were still not solved as well.
<!--l. 141--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">1.5.3   </span> <a 
 id="x1-100001.5.3"></a>Games Playing</h5>
<!--l. 142--><p class="noindent" ><span 
class="cmbx-10">Success </span><br 
class="newline" />In 1959, Arthur Samuel produced a learning checkers program with techniques like
minimax and alpha-beta that beat a &#8220;master&#8221; player. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 146--><p class="noindent" ><img 
src="imgs/samuel.jpg" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 147--><p class="indent" >   </div><hr class="endfigure">
<!--l. 148--><p class="noindent" ><span 
class="cmbx-10">Hubris </span><br 
class="newline" />It looked like games playing had been cracked, if a computer could beat a human in
checkers, surely it would apply to all other games easily. People predicted in 10 years
a computer would beat the world chess champion.<br 
class="newline" /><span 
class="cmbx-10">Nemesis </span><br 
class="newline" />Chess was a much more complicated game that took longer to solve. Additionally,
computers playing Go were horrible due to the large search space and other games
like Bridge and Poker were also bad.<br 
class="newline" />As we know recently, AlphaGo beat a top Go player in 2016 and Kasparov (Chess
champion) was beaten by &#8220;Deep Blue&#8221; in 1997.
   <h4 class="subsectionHead"><span class="titlemark">1.6   </span> <a 
 id="x1-110001.6"></a>Weak and Strong AI</h4>
<!--l. 158--><p class="noindent" ><span 
class="cmbx-10">Weak AI </span><br 
class="newline" />Weak AI takes the view that computers are power tools that can do things human
otherwise do and can be used to study the nature of mind in general. Weak AI are
focused on one narrow task, for example chess playing AIs which can only play
chess. Most currently existing systems considered to be AI are weak at the
moment.<br 
class="newline" /><span 
class="cmbx-10">Strong AI </span><br 
class="newline" />Strong AI takes the view that a computer <span 
class="cmti-10">is </span>a mind like a human mind. Seale
identified a philosophical position he called &#8220;strong AI&#8221;:
     <div class="quote">
     <!--l. 164--><p class="noindent" ><span 
class="cmti-10">The appropriately programmed computer with the right inputs and</span>
     <span 
class="cmti-10">outputs would thereby have a mind in exactly the same sense human</span>
     <span 
class="cmti-10">beings have minds.</span></div>
<!--l. 166--><p class="noindent" >He also ascribes the following positions to advocates of strong AI:
     <ul class="itemize1">
     <li class="itemize">AI systems can be used to explain the mind
     </li>
     <li class="itemize">The study of the brain is irrelevant to the study of the mind
     </li>
     <li class="itemize">The Turing test is adequate for establishing the existence of mental states</li></ul>
                                                                  

                                                                  
<!--l. 172--><p class="noindent" ><span 
class="cmbx-10">Artificial General Intelligence (AGI) </span><br 
class="newline" />More recently, there is a second &#8220;strong AI&#8221; definition as an artificial general
intelligence which is a machine with the ability to apply intelligence to any problem,
rather than just one specific problem and could successfully perform any intellectual
task that a human being can. The idea is making an AI system that can do
anything.<br 
class="newline" />Ben Goertzel proposed a <span 
class="cmbx-10">Robot Coffee Test </span>for AGI. Can you make a robot that
can go into any house and make some coffee? The house should be just any ordinary
house. This is quite a difficult test as it involves finding coffee, finding water, boiling
the water... However, if there is a robot that was built to do this, does it count as
AGI? Can the same robot learn how to build a brick wall without being
reprogramming?<br 
class="newline" />This leads to an analogy with NP-Complete problems with &#8220;AI-Complete&#8221; problems.
This implies the difficulty of these problems is equivalent to that of solving the
central artificial intelligence problem of creating an AGI. An AI-Complete
problem reflects an attitude that it would not be solved by a simple specific
algorithm.
<!--l. 179--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">1.7   </span> <a 
 id="x1-120001.7"></a>Attacks on the Turing Test</h4>
<!--l. 180--><p class="noindent" >There have been many arguments and objections to the Turing Test as a way to
signify machine intelligence. Turing himself as responded to many of these
objections.<br 
class="newline" /><span 
class="cmbx-10">Theological Objection: </span>A Man has a soul, machines do not <br 
class="newline" /><span 
class="cmbx-10">Alan Turing: </span>Can we deny His power to give a soul to a machine?<br 
class="newline" /><span 
class="cmbx-10">Argument from various disabilities: </span>No machine can X (e.g. tell right from
wrong) <br 
class="newline" /><span 
class="cmbx-10">Alan Turing: </span>Becomes a less powerful argument each day as machines are capable
of doing more as time goes on<br 
class="newline" /><span 
class="cmbx-10">Lady Lovelace&#8217;s [Ada&#8217;s] objection: </span>Computers do whatever we know
how to order them to perform, so computers cannot do anything really new
<br 
class="newline" /><span 
class="cmbx-10">Alan Turing: </span>Machines constantly surprise us<br 
class="newline" /><span 
class="cmbx-10">Argument from informality of behaviour: </span>Impossible to write down formal rules
for every situation <br 
class="newline" /><span 
class="cmbx-10">Alan Turing: </span>Impossible to prove people not rule-driven<br 
class="newline" /><span 
class="cmbx-10">Argument from ESP: </span>Telepathy would let humans win imitation game
<br 
class="newline" /><span 
class="cmbx-10">Alan Turing: </span>Put competitors in &#8217;telepathy-proof&#8217; room<br 
class="newline" /><span 
class="cmbx-10">Argument from Consciousness: </span>No mechanism could <span 
class="cmti-10">feel </span>pleasure, grief...
<br 
class="newline" /><span 
class="cmbx-10">Alan Turing: </span>Danger of Solipsism(A theory in philosophy that your own existence
is the only thing that is real or that can be known)<br 
class="newline" /><span 
class="cmbx-10">Argument from continuity in the nervous system: </span>The brain does not operate
digitally <br 
class="newline" /><span 
class="cmbx-10">Alan Turing: </span>Computers can simulate continuous behaviour, e.g. Statistically,
graphically, numerically...
   <h5 class="subsubsectionHead"><span class="titlemark">1.7.1   </span> <a 
 id="x1-130001.7.1"></a>Godel&#8217;s theorem</h5>
<!--l. 196--><p class="noindent" >Godel&#8217;s theorem states that any consistent and powerful format system must be
limited. There must be true statements it cannot prove.<br 
class="newline" />Because computers are formal systems and minds have no limit on their abilities,
therefore computers cannot have minds. This point seems to prove that strong AI
cannot exist as it certainly applies to computers.<br 
class="newline" />However, Alan Turing had two points to counter this:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Although it is established that there are limitations to the powers of any</span>
     <span 
class="cmti-10">particular machine, it has only been stated without any sort of proof, that</span>
     <span 
class="cmti-10">no such limitations apply to the human intellect</span>. I.e, it has never been
     proven that humans have no limitations. Are we sure humans can prove
     <span 
class="cmti-10">all </span>true theorems?
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">We too often give wrong answers ourselves to be justified in being very</span>
     <span 
class="cmti-10">pleased at such evidence of fallibility on the part of machines. </span>Godel&#8217;s
     theorem applies only to consistent formal system, however, humans often
     utter untrue statements, so we might be unlimited format systems which
     make errors.</dd></dl>
                                                                  

                                                                  
<!--l. 206--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">1.7.2   </span> <a 
 id="x1-140001.7.2"></a>The problem with the Turing Test</h5>
<!--l. 207--><p class="noindent" >The issue with the actual Turing test is that the chatbots trying to pass the test use
<span 
class="cmti-10">lots of tricks </span>to try and trick the human on the other end.<br 
class="newline" />For example <span 
class="cmtt-10">ELIZA </span>used simple pattern matching:
     <ul class="itemize1">
     <li class="itemize">&#8220;Well, <span class="underline">my</span> boyfriend made <span class="underline">me</span> come here&#8221;
     </li>
     <li class="itemize">&#8220;<span class="underline">Your</span> boyfriend made <span class="underline">you</span> come here?&#8221;</li></ul>
<!--l. 214--><p class="noindent" ><hr class="figure"><div class="figure" 
><img 
src="imgs/loebner-prize.jpg" alt="PIC"  
>
</div><hr class="endfigure">
<!--l. 218--><p class="indent" >   Jason Hutchens, who won the Loebner Prize in 1996 wrote an article &#8220;How to
pass the Turing test by cheating&#8221; where he demonstrated the techniques his chatbot
used and how it was not very clever. He also states that &#8220;Turing&#8217;s imitation game
in general is inadequate as a test of intelligence, as it relies solely on the
ability to fool people, and this can be very easy to achieve, as Weizenbaum
found.&#8221;<br 
class="newline" />Another example is the 2014 winner, &#8220;Eugene&#8221; who posed as a 13-year old,
non-native English speaking Ukrainian, which gives lots of ways for judges to explain
away his poor English.<br 
class="newline" />Ultimately, whether or not it has been passed, the Turing Test does not drive much
AI research.
<!--l. 224--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">1.8   </span> <a 
 id="x1-150001.8"></a>The Chinese Room</h4>
<!--l. 225--><p class="noindent" >The Chinese Room is one of the most famous attacks on AI. It was created by John
Searle.<br 
class="newline" />The Chinese Room argument holds that a program cannot give a computer a mind or
understanding, regardless of how intelligently or human-like the program may make
the computer behave. The argument specifically refutes strong AI having a mind like
a human being. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 230--><p class="noindent" ><img 
src="imgs/chinese-room.jpg" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 231--><p class="indent" >   </div><hr class="endfigure">
<!--l. 232--><p class="noindent" >Suppose there is a computer program that behaves as if it understands Chinese. It
can take in Chinese characters as input and by following it&#8217;s instructions, produce
other Chinese characters as output and suppose it was convincing enough to pass the
Turing Test: it convinces a human Chinese speakers that the program is itself a live
Chinese speaker and to all questions that the person asks, it makes appropriate
responses. Searle then asks the question: &#8220;Does the machine <span 
class="cmti-10">literally </span>understand
Chinese? Or is it merely <span 
class="cmti-10">simulating </span>the ability to understand Chinese?&#8221; The first
would be a strong AI, whereas the latter would be weak AI. Then, the thought
experiment is as follows:<br 
class="newline" />Suppose someone is in a closed room with a book in English of the computer
program. The person can receive Chinese characters through a slot in the door,
process them according to the program&#8217;s instructions, and produce Chinese
characters as output. This is just like running the program manually. The argument
is that even though the person is producing behaviour which is interpreted as
demonstrating intelligent conversation, the person would <span 
class="cmti-10">still not be able to</span>
<span 
class="cmti-10">understand Chinese </span>and therefore Searle argues that the computer would not be able
to understand the conversation either.
   <h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-160002"></a>Search</h3>
<!--l. 237--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.1   </span> <a 
 id="x1-170002.1"></a>What is search in AI?</h4>
<!--l. 238--><p class="noindent" >A <span 
class="cmbx-10">search problem </span>is defined rigorously. For example it is problems on propositional
satisfiability, graph colouring, playing chess etc.<br 
class="newline" />A search algorithm is given an <span 
class="cmbx-10">Instance </span>of the problem and the algorithm has to
find a <span 
class="cmbx-10">Solution </span>to that instance (or report that there is guaranteed to be no solution
to that instance or report a timeout if it cannot determine).
<!--l. 242--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">2.1.1   </span> <a 
 id="x1-180002.1.1"></a>SAT problem</h5>
<!--l. 243--><p class="noindent" >The boolean SATisfiability problem is an example search problem. An example
<span 
class="cmbx-10">Instance </span>is a list of words: <span 
class="cmtt-10">ABC, ABc, AbC, Abc, aBC, abC, abc</span>. A <span 
class="cmbx-10">Solution </span>is a
choice of upper/lower case letters where each word must contain at least one of
our choices. For example <span 
class="cmtt-10">AbC</span>, which is a unique solution to the previous
instance.<br 
class="newline" />SAT is a search problem because there is no efficient algorithm known for SAT. In
fact, 3-SAT is NP-Complete. 3-SAT is where each word contains exactly 3 letters.
Many AI problems fall into the NP-Complete class. DPLL is one of the algorithms to
find a solution to that SAT problem.
<!--l. 247--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">2.1.2   </span> <a 
 id="x1-190002.1.2"></a>Travelling Salesman problem</h5>
<!--l. 248--><p class="noindent" >The Travelling Salesman problem is another famous search problem. It involves a
salesman travelling across different cities trying to visit each node. An <span 
class="cmbx-10">Instance </span>of
the problem would be a graph with a cost on each edge and a <span 
class="cmbx-10">Solution </span>as a tour
visiting all nodes and returning to base or meeting some cost limit. Another variant
is to find the minimum possible cost.
<!--l. 250--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 252--><p class="noindent" ><img 
src="imgs/tsp.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 253--><p class="indent" >   </div><hr class="endfigure">
<!--l. 254--><p class="noindent" >The travelling salesman problem is also NP-Complete if you want to check that the
tour costs no more than some limit. A complete solution might need to check every
possible path. Of course there are many applications of this problem in real life,
especially for delivery companies or moving efficiently around a warehouse for
Amazon.
   <h5 class="subsubsectionHead"><span class="titlemark">2.1.3   </span> <a 
 id="x1-200002.1.3"></a>Games</h5>
<!--l. 258--><p class="noindent" >Board games like chess and checkers could be thought of as a search problem where
the <span 
class="cmbx-10">Instance </span>is the current board position of the game and the <span 
class="cmbx-10">Solution </span>is a
winning strategy from that position. Such games are usually PSPACE-Complete.
<!--l. 260--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2   </span> <a 
 id="x1-210002.2"></a>Presenting Search Abstractly</h4>
<!--l. 261--><p class="noindent" >There are two main kinds of search algorithm:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Complete </span>search algorithms which are guaranteed to find a solution or
     prove there is none. Though they may have to be given enough time and
     so may timeout.
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Incomplete </span>search algorithms which may not find a solution even if a
     solution exists. However, these algorithms are often more efficient as they
     do not have to search through the entire search space.</dd></dl>
<!--l. 266--><p class="noindent" >Search states summarise the state of search, in SAT is might be represented by <span 
class="cmtt-10">aB</span>. In
TSP, a search state might specify some of the order of visits. In Checkers, a search
sate might be represented by the board position. With search states we can generalise
such to not just finding a solution to a problem.<br 
class="newline" />A <span 
class="cmbx-10">Search space </span>is a logical space composed of:
     <ul class="itemize1">
     <li class="itemize"><span 
class="cmbx-10">Nodes </span>are search states
     </li>
     <li class="itemize"><span 
class="cmbx-10">Links </span>are all legal connections between search states</li></ul>
<!--l. 274--><p class="noindent" >Think of a search algorithm as trying to navigate this <span 
class="cmti-10">extremely </span>complex space. It is
always just an abstraction, we don&#8217;t store the whole search space and study
it.
                                                                  

                                                                  
<!--l. 277--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.3   </span> <a 
 id="x1-220002.3"></a>Search Trees</h4>
<!--l. 278--><p class="noindent" >Search trees do not summarise all possible searches, instead it is an abstraction of
one possible search. The <span 
class="cmbx-10">root </span>of the search tree is the initial state, the branches are
actions and the <span 
class="cmbx-10">nodes </span>correspond to states in the state space of the problem. We can
<span 
class="cmbx-10">expand </span>the current state; that is, applying each legal action to the current
state, thereby generating a new set of states. We add new branches from the
parent node leading to new child nodes where are the new states. At the very
bottom of the search tree, the leaf nodes represent solutions or failures.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 281--><p class="noindent" ><img 
src="imgs/searchtree.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 282--><p class="indent" >   </div><hr class="endfigure">
<!--l. 283--><p class="noindent" >Search trees are a very useful concept, but as an abstraction. We do <span 
class="cmti-10">not </span>want
algorithms to store whole search trees as that would require exponential space. We
should also discard any nodes in the search tree that are already explored. Search
algorithms only store the <span 
class="cmti-10">frontier </span>of the search, that is the set of all lead nodes
available for expansion at any given point.<br 
class="newline" />We must also be careful of <span 
class="cmbx-10">loops </span>in a search tree, where a child node in the tree is
the same state as a previous node. Loops can cause certain algorithms to fail, making
otherwise solvable problems unsolvable. Usually, there is no need to consider such
loops as path costs are additive and step costs are non-negative. A loop to any given
state is never better than the same path with the loop removed. Loops are
a special case of the more general concept of <span 
class="cmbx-10">redundant paths</span>, which
are simply heuristically worse off paths than the optimal. In some cases
redundant paths are unavoidable. The way to avoid them is to remember
where one has been and augment the search algorithm with a list of explored
nodes. Newly generated nodes that match previously visited nodes can be
discarded. This is known as a <span 
class="cmbx-10">Graph search </span>in comparison to a normal tree
search.
   <h4 class="subsectionHead"><span class="titlemark">2.4   </span> <a 
 id="x1-230002.4"></a>Measuring the performance of search algorithms</h4>
<!--l. 288--><p class="noindent" >We can evaluate an algorithm&#8217;s performance in four ways:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Completeness</span>: Is the algorithm guaranteed to find a solution when there
     is one?
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Optimality</span>: Does the strategy find the optimal solution?
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Time complexity</span>: How long does it take to find a solution?
     </dd><dt class="enumerate-enumitem">
  4. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Space complexity</span>: How much memory is needed to perform the search?</dd></dl>
<!--l. 295--><p class="noindent" >The time and space complexity are always considered with respect to some measure of
the problem size or difficulty. As such the complexity is expressed in terms of three
quantities:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">b, the branching factor </span>- the maximum number of children of any node
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">d, the depth </span>- the shallowest goal node (the least number of steps along
     the path from the root)
                                                                  

                                                                  
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">m, maximum length </span>- the maximum length of any path in the search
     space</dd></dl>
<!--l. 302--><p class="noindent" >Time is often measured in terms of the number of nodes generated during the search,
and space in terms of the maximum number of nodes stored in memory. The <span 
class="cmbx-10">search</span>
<span 
class="cmbx-10">cost </span>is the cost taken by the algorithm to find the solution, but there is also the
<span 
class="cmbx-10">solution cost</span>, which is the length of the path, or for example moves made in a
board game.
<!--l. 305--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.5   </span> <a 
 id="x1-240002.5"></a>Complete search algorithms</h4>
<!--l. 307--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">2.5.1   </span> <a 
 id="x1-250002.5.1"></a>Breadth-first search</h5>
<!--l. 308--><p class="noindent" ><span 
class="cmbx-10">Breadth-first search </span>is a simple strategy where the root node is expanded first,
then all the children of the root node are expanded next, then their children and so
on. In general, all nodes of the current depth are expanded before moving on to the
next depth. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 311--><p class="noindent" ><img 
src="imgs/breadth-first-search.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 312--><p class="indent" >   </div><hr class="endfigure">
<!--l. 313--><p class="noindent" >Implementation of the breadth-first search is very simple using a FIFO queue.
Expanded nodes are put at the back of the queue, so all nodes of the same depth are
expanded first before the next depth. The problem is that this list can be exponential
in size as it contains all nodes at a given depth. We can also use a heuristic to decide
what order to add the new states.<br 
class="newline" />There are two major problems with breadth-first search. First, the <span 
class="cmti-10">memory</span>
<span 
class="cmti-10">requirements </span>are a bigger problem than is the <span 
class="cmti-10">execution time</span>. However,
time is still a major factor as well. In general, <span 
class="cmti-10">exponential-complexity search</span>
<span 
class="cmti-10">problems cannot be solved by uninformed methods for any but the smallest</span>
<span 
class="cmti-10">instances</span>.
   <h5 class="subsubsectionHead"><span class="titlemark">2.5.2   </span> <a 
 id="x1-260002.5.2"></a>Depth-first search</h5>
<!--l. 318--><p class="noindent" >In a <span 
class="cmbx-10">depth-first </span>search, always expand the <span 
class="cmti-10">deepest </span>node in the current frontier of
the search tree. Unlike breadth-first search, we can treat the list as a stack
instead of a queue. So new search nodes are put at the <span 
class="cmti-10">front </span>of the list.
Again we may need a heuristic to decide what order to push new nodes to
the stack is. New states will always be in front of all old states in the list.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 321--><p class="noindent" ><img 
src="imgs/depth-first-search.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 322--><p class="indent" >   </div><hr class="endfigure">
<!--l. 323--><p class="noindent" >Depth-first search is also non-optimal as it will always search the left subtree first.
This means if the solution is somewhere on the right, the search will take longer as it
has to go through everything on the left first. Furthermore, if it finds a valid solution,
it will return. This may miss out more optimal solutions as it returns after
finding the first one. The advantage of depth-first search over breadth-first
search is the space complexity. A depth-first search needs to store only a
single path from the root to a leaf node, along with any unexpanded sibling
nodes for each node on the path. Once a node has been expanded, it can
be removed from memory as soon as all its descendants have been fully
explored.<br 
class="newline" />The depth of the search is still a problem for depth-first search. If we do not know
the depth the solution is found at, the time could take very long to find the solution.
If the problem we are trying to solve has an unknown depth, we could go very deep
into the tree. This also means we are going very deep down one path of the three
before exploring other paths.
   <h5 class="subsubsectionHead"><span class="titlemark">2.5.3   </span> <a 
 id="x1-270002.5.3"></a>Depth-first depth-bounded search</h5>
<!--l. 328--><p class="noindent" >A variant on depth-first search, <span 
class="cmbx-10">depth-first depth-bounded </span>search limits the
depth that the search can go to. This prevents us from going too deep without
exploring other branches and also limits the time complexity of the search. However,
because it is depth-bounded, the search is not complete as we may search all nodes
up to the depth limit without finding the solution. The way to expand and all child
nodes is the same as depth-first search, only we do not expand the child nodes once
we reach the depth limit.<br 
class="newline" />Compared to normal depth-first, depth-first depth-bounded will never go down an
infinite branch and always find a solution at depth ¡= the limit. Sometimes, depth
limits can be based on knowledge of the problem, or example we know we can mate
in three moves. But if we don&#8217;t know what depth to choose, what should we do
then?
   <h5 class="subsubsectionHead"><span class="titlemark">2.5.4   </span> <a 
 id="x1-280002.5.4"></a>Iterative deepening search</h5>
<!--l. 332--><p class="noindent" ><span 
class="cmbx-10">Iterative deepening </span>is a variant of depth-first depth-bounded where we increase
the depth limit on every iteration. This ensures completeness as the search will
eventually terminate once it finds a solution at a sufficient depth. However, this
search does lots of redundant work as it <span 
class="cmbx-10">re-evaluates </span>on <span 
class="cmti-10">every </span>iteration, so
the first few depths are re-evaluated again and again on every iteration.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 335--><p class="noindent" ><img 
src="imgs/iterative-deepening.jpg" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 336--><p class="indent" >   </div><hr class="endfigure">
<!--l. 337--><p class="noindent" >This may seem very wasteful because states are generated multiple times. It turns
out this is not too costly. The reason is that in a search tree with the same branching
factor at each level, most of the nodes are in the bottom level, so it does not matter
much that the upper levels are generated multiple times. The nodes at the bottom
(or rather, the depth where the solution exists) is only generated once, the level
above twice and so on. This gives the time complexity as asymptotically the same as
breadth-first search. In general, <span 
class="cmti-10">iterative deepening is the preferred uninformed search</span>
<span 
class="cmti-10">method when the search space is large and the depth of the solution is not</span>
<span 
class="cmti-10">known.</span>
   <h5 class="subsubsectionHead"><span class="titlemark">2.5.5   </span> <a 
 id="x1-290002.5.5"></a>Comparison of search algorithms</h5>
   <table id="TBL-3" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-3-1g"><col 
id="TBL-3-1"></colgroup><colgroup id="TBL-3-2g"><col 
id="TBL-3-2"><col 
id="TBL-3-3"><col 
id="TBL-3-4"><col 
id="TBL-3-5"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-1-"><td  style="white-space:wrap; text-align:left;" id="TBL-3-1-1"  
class="td11"> <!--l. 350--><p class="noindent" >Criterion    </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-1-2"  
class="td11"> <!--l. 350--><p class="noindent" >Breadth-first </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-1-3"  
class="td11"> <!--l. 350--><p class="noindent" >Depth-first </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-1-4"  
class="td11"> <!--l. 350--><p class="noindent" >Depth-first
  depth-bounded </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-1-5"  
class="td11"><!--l. 350--><p class="noindent" >Iterative
  deepening  </td>
   </tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-2-"><td  style="white-space:wrap; text-align:left;" id="TBL-3-2-1"  
class="td11"> <!--l. 350--><p class="noindent" >Complete?   </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-2-2"  
class="td11"> <!--l. 350--><p class="noindent" >Yes           </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-2-3"  
class="td11"> <!--l. 350--><p class="noindent" >No            </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-2-4"  
class="td11"> <!--l. 350--><p class="noindent" >No            </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-2-5"  
class="td11"> <!--l. 350--><p class="noindent" >Yes           </td>
   </tr><tr  
 style="vertical-align:baseline;" id="TBL-3-3-"><td  style="white-space:wrap; text-align:left;" id="TBL-3-3-1"  
class="td11"> <!--l. 350--><p class="noindent" >Time          </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-3-2"  
class="td11"> <!--l. 350--><p class="noindent" ><span 
class="cmmi-10">O</span>(<span 
class="cmmi-10">b</span><sup><span 
class="cmmi-7">d</span></sup>)        </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-3-3"  
class="td11"> <!--l. 350--><p class="noindent" ><span 
class="cmmi-10">O</span>(<span 
class="cmmi-10">b</span><sup><span 
class="cmmi-7">d</span></sup>)        </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-3-4"  
class="td11"> <!--l. 350--><p class="noindent" ><span 
class="cmmi-10">O</span>(<span 
class="cmmi-10">b</span><sup><span 
class="cmmi-7">d</span></sup>)        </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-3-5"  
class="td11"> <!--l. 350--><p class="noindent" ><span 
class="cmmi-10">O</span>(<span 
class="cmmi-10">b</span><sup><span 
class="cmmi-7">d</span></sup>)        </td>
   </tr><tr  
 style="vertical-align:baseline;" id="TBL-3-4-"><td  style="white-space:wrap; text-align:left;" id="TBL-3-4-1"  
class="td11"> <!--l. 350--><p class="noindent" >Space         </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-4-2"  
class="td11"> <!--l. 350--><p class="noindent" ><span 
class="cmmi-10">O</span>(<span 
class="cmmi-10">b</span><sup><span 
class="cmmi-7">d</span></sup>)        </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-4-3"  
class="td11"> <!--l. 350--><p class="noindent" ><span 
class="cmmi-10">O</span>(<span 
class="cmmi-10">bd</span>)        </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-4-4"  
class="td11"> <!--l. 350--><p class="noindent" ><span 
class="cmmi-10">O</span>(<span 
class="cmmi-10">bd</span>)        </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-4-5"  
class="td11"> <!--l. 350--><p class="noindent" ><span 
class="cmmi-10">O</span>(<span 
class="cmmi-10">bd</span>)        </td>
   </tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-5-"><td  style="white-space:wrap; text-align:left;" id="TBL-3-5-1"  
class="td11"> <!--l. 350--><p class="noindent" >           </td></tr></table>
<!--l. 352--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.6   </span> <a 
 id="x1-300002.6"></a>Heuristic search strategies</h4>
<!--l. 353--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">2.6.1   </span> <a 
 id="x1-310002.6.1"></a>Best-first search</h5>
<!--l. 354--><p class="noindent" >The idea is to explore the frontier heuristically instead of in a purely algorithmic way.
<span 
class="cmbx-10">Best-first </span>search is an algorithm in which a node is selected for expansion based on
an <span 
class="cmbx-10">evaluation function </span><span 
class="cmmi-10">f</span>(<span 
class="cmmi-10">n</span>). New states can be sorted in order of the score of that
state from the evaluation function. The list always contains the most promising state
first. The actual search itself can use the search algorithms from before, like
depth-first or iterative deepening. Most best-first algorithms include as a component
of <span 
class="cmmi-10">f </span>a <span 
class="cmbx-10">heuristic function </span><span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>).<br 
class="newline" /><span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>) = estimated cost of the cheapest path from the state at node <span 
class="cmmi-10">n </span>to a goal state.
In this case, if <span 
class="cmmi-10">n </span>is a goal node, then <span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>) = 0.<br 
class="newline" />A <span 
class="cmbx-10">greedy best-first search </span>algorithm will always try to expand the node that is
closest to the goal. It evaluates nodes by using just the heuristic function, that is
<span 
class="cmmi-10">f</span>(<span 
class="cmmi-10">n</span>) = <span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>). Greedy best-first can be incomplete if we don&#8217;t keep track of which
nodes we have visited before.
                                                                  

                                                                  
<!--l. 360--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">2.6.2   </span> <a 
 id="x1-320002.6.2"></a>A* search</h5>
<!--l. 361--><p class="noindent" >A widely known and popular search algorithm is the <span 
class="cmbx-10">A* search</span>. The evaluation
function <span 
class="cmmi-10">f</span>(<span 
class="cmmi-10">n</span>) that it uses is the combination of:
     <ul class="itemize1">
     <li class="itemize"><span 
class="cmmi-10">g</span>(<span 
class="cmmi-10">n</span>) - the cost to reach the node
     </li>
     <li class="itemize"><span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>) - the cost to get from the node to the goal state</li></ul>
<!--l. 366--><p class="noindent" >This gives <span 
class="cmmi-10">f</span>(<span 
class="cmmi-10">n</span>) = <span 
class="cmmi-10">g</span>(<span 
class="cmmi-10">n</span>) + <span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>). Since <span 
class="cmmi-10">g</span>(<span 
class="cmmi-10">n</span>) gives the path cost from the start node to
node <span 
class="cmmi-10">n </span>and <span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>) is the estimated cost from node <span 
class="cmmi-10">n </span>to the goal, <span 
class="cmmi-10">f</span>(<span 
class="cmmi-10">n</span>) = estimated cost
of the cheapest solution through <span 
class="cmmi-10">n</span>. Provided that the heuristic function <span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>) is an
<span 
class="cmbx-10">admissible </span>and <span 
class="cmbx-10">consistent </span>heuristic, A* search is both optimal and complete. <hr class="figure"><div class="figure" 
>
<img 
src="imgs/a-star.jpg" alt="PIC"  
>
</div><hr class="endfigure">
<!--l. 371--><p class="noindent" >For optimality, it is required that the heuristic <span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>) is an <span 
class="cmbx-10">admissible heuristic</span>. An
admissible heuristic is one that <span 
class="cmti-10">never overestimates </span>the cost to reach the goal.
Typically admissible heuristics think the cost of solving the problem is less than it
actually is. For example, an admissible heuristic for calculating distances is the
straight-line distance (as the crow flies) from one location to another. It is admissible
because the shortest path between any two points is a straight line, so the heuristic
cannot be an overestimate.<br 
class="newline" />A second, stronger condition is called <span 
class="cmbx-10">consistency</span>. A heuristic <span 
class="cmmi-10">h</span>(<span 
class="cmmi-10">n</span>) is consistent if,
for every node <span 
class="cmmi-10">n </span>and every successor <span 
class="cmmi-10">n</span><sup><span 
class="cmsy-7">&#x2032;</span></sup> of <span 
class="cmmi-10">n </span>generated, the estimated cost of reaching
the goal from <span 
class="cmmi-10">n </span>is no greater than the step cost of getting to <span 
class="cmmi-10">n</span><sup><span 
class="cmsy-7">&#x2032;</span></sup> + the estimated cost
of reaching the goal from <span 
class="cmmi-10">n</span><sup><span 
class="cmsy-7">&#x2032;</span></sup>. In other words:
   <table 
class="equation"><tr><td><a 
 id="x1-32001r1"></a>
   <center class="math-display" >
<img 
src="Notes0x.png" alt="h(n) &#x2264; cost(n&#x2032;)+ h(n&#x2032;)
" class="math-display" ></center></td><td class="equation-label">(1)</td></tr></table>
<!--l. 377--><p class="nopar" >
A* <span 
class="cmbx-10">guarantees </span>to find the optimal solution.<br 
class="newline" />The problem with A* is that the number of states within the goal search space is still
exponential in the length of the solution. There may also be cases in search where we
do not need to find the optimal solution and instead prefer to find any solution in less
time and space.
   <h5 class="subsubsectionHead"><span class="titlemark">2.6.3   </span> <a 
 id="x1-330002.6.3"></a>Branch and Bound</h5>
<!--l. 382--><p class="noindent" >The <span 
class="cmbx-10">Branch and bound </span>algorithm is usually used where there is a natural cost of
each node (for example the length of the path). Like A*, it exploits bounds to reduce
the amount of search needed, but unlike A*, we can search however we want (i.e, use
other search algorithms like depth-first or breadth-first as we please). Usually we
search depth-first.<br 
class="newline" />Just like A*, we look for a <span 
class="cmbx-10">bound </span>which is guaranteed lower than the true cost.
Unlike A*, we are not guaranteed an optimal solution for the first solution found.
However, with the bound we can continue search until we find the optimal solution. If
the heuristic used is cost + bound and the search used is best-first, the branch and
bound <span 
class="cmti-10">is </span>A*. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 387--><p class="noindent" ><img 
src="imgs/branch-and-bound.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 388--><p class="indent" >   </div><hr class="endfigure">
<!--l. 389--><p class="noindent" >We add a variable <span 
class="cmtt-10">best = </span><span 
class="cmsy-10">&#x221E;</span><span 
class="cmmi-10">whichisthescoreofthebestsolutionsofar.Itis</span><span 
class="cmsy-10">&#x221E;</span><span 
class="cmmi-10">initiallybecausewedon</span><span 
class="cmsy-10">&#x2032;</span><span 
class="cmmi-10">tknowifanysolutionatallispossible.Next,wesearchaccordingtowhateversearchmethodwechoose,however </span>:
     <span 
class="cmtt-10">At any node </span><span 
class="cmmi-10">n</span><span 
class="cmtt-10">, cut off the search if </span><span 
class="cmmi-10">f</span>(<span 
class="cmmi-10">n</span>) <span 
class="cmsy-10">&#x2265; </span><span 
class="cmtt-10">best</span>
     <span 
class="cmtt-10">When a solution is found at node </span><span 
class="cmmi-10">n</span><span 
class="cmtt-10">, set best </span>= <span 
class="cmmi-10">g</span>(<span 
class="cmmi-10">n</span>)
<!--l. 395--><p class="noindent" >The idea is we stop searching any deeper if the evaluated score of the node is worse than
the currently found best solution. This reduces the amount of search and we are
guaranteed to find the best solution because the best solution is one with the
minimum <span 
class="cmtt-10">best </span>score.<br 
class="newline" />We use branch and bound as A* can be infeasible in practice due to its
exponential search space requirement. If we use depth-first as our branch and
bound search algorithm, we only need a linear amount and search space
and we are still able to exploit heuristics and bounds to find the optimal
solution.
   <h4 class="subsectionHead"><span class="titlemark">2.7   </span> <a 
 id="x1-340002.7"></a>Incomplete Search</h4>
<!--l. 400--><p class="noindent" >The complete search methods we previously looked at all have a key problem, they
do <span 
class="cmti-10">exhaustive </span>searches on the search space to find the optimal solution.
This makes the search very expensive, even with cut-offs like in branch and
bound. If there is no solution, these algorithms must do a complete sweep of
the search space to verify that. It also means it&#8217;s hard to jump around the
search space even if the algorithm is working in a very unpromising part of
it.<br 
class="newline" />Additionally, in some cases of search problems, the path to the solution doesn&#8217;t
matter, that is the path to the solution is <span 
class="cmti-10">not </span>a part of the solution. In
cases where the path does not matter, we consider a different class of <span 
class="cmbx-10">local</span>
<span 
class="cmbx-10">search </span>algorithms. Local search algorithms are not systematic in the way
they search and operate using a single current node and generally move
only to neighbours of that node. There are two key advantages to these
algorithms:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem">They use very little memory - usually a constant amount
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem">They can often find reasonable solutions in large or infinite state spaces
     where systematic (complete) search algorithms are unsuitable</dd></dl>
                                                                  

                                                                  
<!--l. 407--><p class="noindent" >These <span 
class="cmbx-10">incomplete </span>search methods don&#8217;t have to be exhaustive and they exploit this by
moving around the search space much more rapidly. The risk is missing solutions that
are near where they have just been because they are jumping around. Incomplete
search methods typically involve three things:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem">A representation of a complete search state
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem">Some form of <span 
class="cmti-10">hill climbing </span>to move towards nearby solutions
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem">Some means of making <span 
class="cmti-10">bad </span>moves to jump to a different part of the search
     space</dd></dl>
<!--l. 413--><p class="noindent" >Complete and incomplete search methods are the opposite of each other. Complete
methods always have a <span 
class="cmti-10">consistent </span>but <span 
class="cmti-10">partial (incomplete) </span>search state and are trying
to get a <span 
class="cmti-10">complete </span>search state. On the other hand, incomplete methods always have a
<span 
class="cmti-10">complete </span>but <span 
class="cmti-10">inconsistent </span>search space and are trying to get a <span 
class="cmti-10">consistent </span>search
state.
   <h5 class="subsubsectionHead"><span class="titlemark">2.7.1   </span> <a 
 id="x1-350002.7.1"></a>Hill-climbing search</h5>
<!--l. 415--><p class="noindent" >The <span 
class="cmbx-10">hill-climbing </span>search algorithm is a simple algorithm that continues to try and
move in the direction of increasing value (uphill). Given a start state S, it explores all
of its neighbours and chooses the highest neighbour (or random if there is
a tie). The algorithm does not maintain a search tree and does not need
to look ahead beyond the immediate neighbours of the current node. Hill
climbing is also called <span 
class="cmbx-10">greedy local search </span>because it always goes for the
next best neighbour state without thinking ahead about where to go next.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-36001r1"></a>
                                                                  

                                                                  

<!--l. 418--><p class="noindent" ><img 
src="imgs/hill-climbing.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 419--><p class="indent" >   </div><hr class="endfigure">
<!--l. 420--><p class="noindent" >The issue is that the algorithm can get stuck for many reasons:
     <ul class="itemize1">
     <li class="itemize"><span 
class="cmbx-10">Local maxima</span>: a local maximum is a peak that is higher than each of
     its neighbouring states, but lower than the global maximum. Hill-climbing
     algorithms that come near the local maximum will go up to its peak but
     will then be stuck with nowhere else to go. It does not know to go for a
     neighbour with less value to try and find a better maximum.
     </li>
     <li class="itemize"><span 
class="cmbx-10">Ridges</span>: A ridge is a sequence of local maxima that is very difficult for
     greedy algorithms to navigate.
     </li>
     <li class="itemize"><span 
class="cmbx-10">Plateaux</span>: A plateau is a flat area, it can either be a &#8220;flat&#8221; local maximum,
     or  a  <span 
class="cmbx-10">shoulder </span>where  progress  is  still  possible.  Instead  of  halting  the
     algorithm if it reaches a plateau, it might be a good idea to allow a sideways
     move in the hop that the plateau is really a shoulder. This raises the
     percentage of problem instances solved by hill climbing, but comes at a
     cost of the algorithm taking an average of more steps for each success and
     failure.</li></ul>
<!--l. 427--><p class="noindent" >Hill climbing algorithms are therefore <span 
class="cmti-10">incomplete </span>because of this tendency to get
stuck in local maxima. <span 
class="cmbx-10">Random-restart </span>is a hill climbing variant that
conducts a series of hill climbing searches from randomly generated initial
states until the global maximum is found. If each hill climbing search has a
probability <span 
class="cmmi-10">p </span>of success, then the expected number of restarts required is
1/<span 
class="cmmi-10">p</span>.
   <h5 class="subsubsectionHead"><span class="titlemark">2.7.2   </span> <a 
 id="x1-360002.7.2"></a>WalkSAT</h5>
<!--l. 429--><p class="noindent" >We can combine a random walk where we choose neighbours at random with
hill climbing for a better incomplete algorithm. One such example is the
<span 
class="cmbx-10">WalkSAT </span>algorithm. WalkSAT only applies to the boolean satisfiability problem
seen earlier, but it uses concepts of both hill climbing and random walk.
<!--l. 430-->
<a 
 id="x1-36002"></a>
<br />
    <div class="caption" 
><span class="id">Algorithm&#x00A0;1:
    </span><span  
class="content">WalkSAT
    algorithm</span></div><!--tex4ht:label?: x1-36001r2 --><div class="lstlisting" id="listing-1"><span class="label"><a 
 id="x1-36003r1"></a><span 
class="cmr-9">1</span></span><span 
class="cmtt-9">Choose</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">a</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">random</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">complete</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">truth</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">assignment</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">T</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36004r2"></a><span 
class="cmr-9">2</span></span><span 
class="cmtt-9">while</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">T</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">leaves</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">at</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">least</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">one</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">unsatisfied</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">clause</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36005r3"></a><span 
class="cmr-9">3</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">choose</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">an</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">unsatisfied</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">clause</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">C</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">at</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">random</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36006r4"></a><span 
class="cmr-9">4</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">generate</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">a</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">random</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">number</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">r</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">between</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">0</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">and</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">1</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36007r5"></a><span 
class="cmr-9">5</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">if</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">r</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x003E;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">p</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36008r6"></a><span 
class="cmr-9">6</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">select</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">variable</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">v</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">in</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">C</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">to</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">flip</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">which</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">maximises</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">the</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">number</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">of</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">satisfied</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">clauses</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36009r7"></a><span 
class="cmr-9">7</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36010r8"></a><span 
class="cmr-9">8</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">else</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36011r9"></a><span 
class="cmr-9">9</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">select</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">variable</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">v</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">in</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">C</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">randomly</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36012r10"></a><span 
class="cmr-9">10</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36013r11"></a><span 
class="cmr-9">11</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">Set</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">T</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">=</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">[</span><span 
class="cmtt-9">T</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">with</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">v</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">set</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">to</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">the</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">opposite</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">value</span><span 
class="cmtt-9">]</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36014r12"></a><span 
class="cmr-9">12</span></span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-36015r13"></a><span 
class="cmr-9">13</span></span><span 
class="cmtt-9">return</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">T</span>
   </div>
                                                                  

                                                                  
<!--l. 445--><p class="noindent" >Line 6 is the hill climbing portion of the algorithm and line 9 gives the &#8220;bad&#8221; moves
portion for random walking. WalkSAT is very effective at local search in SAT,
however, it is used less than complete solvers like DPLL because most people want a
complete search instead.
<!--l. 448--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">2.7.3   </span> <a 
 id="x1-370002.7.3"></a>Hill-climbing variants</h5>
<!--l. 449--><p class="noindent" >There are many hill climbing variants that try to get around the problem of being
stuck at local maxima or at plateaus:
     <ul class="itemize1">
     <li class="itemize"><span 
class="cmbx-10">Allow sideways moves </span>- We have already looked at allowing sideways
     moves to try and get off a plateau without looping. Most notably, we
     should only allow <span 
class="cmmi-10">n </span>sideways moves so an infinite loop does not occur
     whenever  the  algorithm  reaches  a  flat  local  maximum  that  is  not  a
     shoulder.
     </li>
     <li class="itemize"><span 
class="cmbx-10">Stochastic hill climbing </span>- This method chooses at random from among
     the uphill moves with the probability of selection varying based on the
     steepness  of  the  uphill  move.  This  usually  converges  more  slowly  as
     it  doesn&#8217;t  always  choose  the  steepest  (best)  move,  but  it  finds  better
     solutions.
     </li>
     <li class="itemize"><span 
class="cmbx-10">First choice hill climbing </span>- This implements stochastic hill climbing
     by generating successors randomly until one is generated that is better
     than the current state. This is a good strategy when a state has many (i.e,
     thousands) of successors.
     </li>
     <li class="itemize"><span 
class="cmbx-10">Random restart </span>- We have also looked at this before as it is a variant that
     conducts a series of hill climbs and only stops when the goal is reached.
     The success of this variant depends very much on the shape and landscape
     of the space. If there are few local maxima and plateaux, random-restart
     hill climbing will find a good solution very quickly. However, it is more
     difficult with more complicated landscapes. Despite this, reasonably good
     local maximum can often be found after a small number of restarts. This
     does not give the optimal solution, but often we are do looking for the
     optimal solution.</li></ul>
<!--l. 456--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">2.7.4   </span> <a 
 id="x1-380002.7.4"></a>Simulated annealing</h5>
<!--l. 457--><p class="noindent" ><span 
class="cmbx-10">Simulated annealing </span>is a combination of random walk and hill climbing. Instead of
using uphill climbing, we switch our point of view to <span 
class="cmbx-10">gradient descent </span>where the
                                                                  

                                                                  
goal is to minimise the cost. Imagine the situation being trying to roll a ball down a
hill, but it could stop at a local minimum. The trick is to shake the surface to try and
bounce the ball out of the local minima and keep is rolling. The simulate-annealing
solution is to start by shaking hard, and then gradually reduced the intensity of the
shaking. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 460--><p class="noindent" ><img 
src="imgs/simulated-annealing.jpeg" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 461--><p class="indent" >   </div><hr class="endfigure">
<!--l. 462--><p class="noindent" >Instead of picking the <span 
class="cmti-10">best </span>move, it picks a <span 
class="cmti-10">random </span>move. If the move improves the
situation, it is always accepted. Otherwise, the algorithm accepts with some
probability <span 
class="cmmi-10">p &#x003C; </span>1. This probability decreases exponentially with the &#8220;badness&#8221; of the
move. The probability also decreases as the &#8220;temperature&#8221; goes down. Therefore bad
moves are more likely to be allowed at the start and less as we head towards a
solution. If the schedule lowers <span 
class="cmmi-10">p </span>slowly enough, then a global optimum will be found
with probability of 1.
   <h5 class="subsubsectionHead"><span class="titlemark">2.7.5   </span> <a 
 id="x1-390002.7.5"></a>Local beam search</h5>
<!--l. 466--><p class="noindent" >Instead of just keeping one node in memory as hill climbing does, <span 
class="cmbx-10">local beam</span>
<span 
class="cmbx-10">search </span>keeps track of <span 
class="cmmi-10">k </span>states. It begins with <span 
class="cmmi-10">k </span>randomly generated states. At each
step, all the neighbours of all <span 
class="cmmi-10">k </span>states are generated. If any of them is the goal
then stop, otherwise selected the <span 
class="cmmi-10">k </span>best nodes from all the neighbours and
repeat.<br 
class="newline" />At first, this just seems like running <span 
class="cmmi-10">k </span>random restarts in parallel. However, the
two algorithms are quite different. In random-restart, each search process
runs independently of the others. In local-beam search, useful information is
passed among the parallel search threads. The algorithm quickly abandons
unfruitful searches and moves its resources to where the most progress is being
made.<br 
class="newline" />In the most simple version of this algorithm, the search can quickly suffer from a lack
of diversity among the <span 
class="cmmi-10">k </span>states as they can converge to a small region of the search
space. A variant called <span 
class="cmbx-10">stochastic local beam search </span>helps this problem. Instead
of choosing <span 
class="cmmi-10">k </span>best nodes from the pool, it chooses the <span 
class="cmmi-10">k </span>nodes at random with
probability being an increasing function of its value. This is analogous to the process
of natural selection.
<!--l. 472--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">2.7.6   </span> <a 
 id="x1-400002.7.6"></a>Genetic algorithms</h5>
<!--l. 473--><p class="noindent" >A <span 
class="cmbx-10">genetic algorithm </span>is a variant of stochastic beam search in which two parent
nodes are combined to generate the child nodes. This is again an analogy to
natural selection, except we have sexual rather than asexual reproduction.
Like a beam search, GAs start with <span 
class="cmmi-10">k </span>randomly generated states, called the
<span 
class="cmbx-10">population</span>. Each node is encoded as a string, most commonly of 0s and 1s.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 476--><p class="noindent" ><img 
src="imgs/genetic-algorithm.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 477--><p class="indent" >   </div><hr class="endfigure">
<!--l. 478--><p class="noindent" >The production of the next generation of states is as follows:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem">Each  state  is  rated  by  a  <span 
class="cmbx-10">fitness  function</span>,  with  probability  <span 
class="cmmi-10">P</span>(node
     chosen) <span 
class="cmsy-10">&#x221D; </span>node fitness, possibly culling all nodes below a set fitness
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem">Next, selection nodes to combine based on the probability <span 
class="cmmi-10">p </span>so that less fit
     nodes are rarely chosen. Note than the same node could be chosen multiple
     times and some nodes could be not chosen at all.
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem">For each pair, a <span 
class="cmbx-10">crossover </span>point is chosen randomly from the positions
     in the strings
     </dd><dt class="enumerate-enumitem">
  4. </dt><dd 
class="enumerate-enumitem">The  offspring  are  created  by  crossing  over  the  parent  strings  at  the
     crosspoint point. This creates two children, though we could discard one
     and only create one child.
     </dd><dt class="enumerate-enumitem">
  5. </dt><dd 
class="enumerate-enumitem">Finally, each location of the string is subject to random <span 
class="cmbx-10">mutation </span>with
     a small independent probability.</dd></dl>
<!--l. 487--><p class="noindent" >New generations and continually produced from the previous generations until some
individual is fit enough, or a bound has been reached in terms of number of
generations or time.<br 
class="newline" />Early on, the algorithm makes larger steps as there is a wide divergence of child
nodes of the population. But as more generations pass, the steps become smaller
as we get closer to the goal as the parent nodes all share many features.
The crossover of GAs make it different from local beam search, however
it requires very careful design. Some encoding/crossover schemes give no
advantages whereas others swiftly raise search granularity. Genetic algorithms
work best when the schemata correspond to meaningful components of a
solution.
   <h3 class="sectionHead"><span class="titlemark">3   </span> <a 
 id="x1-410003"></a>Machine Learning</h3>
<!--l. 491--><p class="noindent" >An agent is <span 
class="cmbx-10">learning </span>if it improves its performance on future tasks after making
observations about the world. The first question one might ask is why would we want
an agent to learn? If we could improve the design of the agent, surely the designers
could just program in the improvements. There are three main reasons why we want
agents to learn:
                                                                  

                                                                  
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem">The designers cannot anticipate all possible situations that the agent might
     find itself in. For example a robot to navigate a maze must learn the layout
     of each new maze it encounters.
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem">The designers cannot anticipate all changes over time. A program designed
     to predict the weather must adapt to the time of year or location.
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem">Sometimes  human  programmers  have  no  idea  how  to  program  the
     solution themselves. For example, how do we code/program a computer
     to recognise faces of people?</dd></dl>
<!--l. 497--><p class="noindent" >There are also several components of an agent that can be improved by learning from
data. The improvements and the techniques used to make them depend on four
major factors:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem">Which <span 
class="cmti-10">component </span>is to be improved?
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem">What <span 
class="cmti-10">prior knowledge </span>the agent already has
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem">What <span 
class="cmti-10">representation </span>is used for the data and the component
     </dd><dt class="enumerate-enumitem">
  4. </dt><dd 
class="enumerate-enumitem">What <span 
class="cmti-10">feedback </span>is available to learn from.</dd></dl>
<!--l. 504--><p class="noindent" >Additionally, there are a number of components that these agents have which
include:
     <ul class="itemize1">
     <li class="itemize">A direct mapping from conditions on the current state to actions, in other
     words <span 
class="cmti-10">What action to take under certain conditions</span>.
     </li>
     <li class="itemize">A means to infer relevant properties of the world from the percept sequence
     that might be useful
     </li>
     <li class="itemize">Information about the way the world evolves and about the results of
     possible actions the agent can take, so <span 
class="cmti-10">how the world is affected by your</span>
     <span 
class="cmti-10">actions</span>
     </li>
     <li class="itemize">Utility  information  indicating  the  desirability  of  world  states,  how
     something is assessed along some metric.
                                                                  

                                                                  
     </li>
     <li class="itemize">Action-value information indicating the desirability of actions
     </li>
     <li class="itemize">Goals that describe classes of states whose achievement maximizes the
     agent&#8217;s utility.</li></ul>
<!--l. 513--><p class="noindent" >A key issue in machine learning is <span 
class="cmti-10">representation</span>. We can&#8217;t just go wild and learn
anything. We usually already decided on the representation and we want to learn the
values to put into that representation. For example learning to curve fit where we
have decided on a polynomial and want to learn the coefficients. Or in a neural net
where we have decided the number of layers/neurons and want to learn the weights
and thresholds.
   <h4 class="subsectionHead"><span class="titlemark">3.1   </span> <a 
 id="x1-420003.1"></a>Forms of learning</h4>
<!--l. 515--><p class="noindent" >There are four different ways that machine learning algorithms can get feedback:
<span 
class="cmbx-10">Supervised </span>learning, <span 
class="cmbx-10">Reinforcement </span>learning, <span 
class="cmbx-10">Unsupervised </span>learning and
<span 
class="cmbx-10">Semi-supervised </span>learning.
<!--l. 517--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.1   </span> <a 
 id="x1-430003.1.1"></a>Reinforcement learning</h5>
<!--l. 518--><p class="noindent" >In <span 
class="cmbx-10">reinforcement learning</span>, the agent learns from a series of reinforcements -
rewards or punishments. The learner is given a sequence of examples, but is not given
the right answer for each one. Instead there is a <span 
class="cmti-10">reward </span>or <span 
class="cmti-10">punishment </span>if the learner
gets the answer right or wrong. The learner than has to figure out how to behave to
increase rewards in the future. For example, after playing games of chess against
other agents, it is rewarded if it wins and punished if it loses. It is then up to the
agent to decide which of the actions prior to the reinforcement were most responsible
for it.<br 
class="newline" />This is quite different from supervised learning as the agent is not told what the
correct answer is. After a lot of moves in the game, it is told if it won or lost and
reinforced accordingly.
<!--l. 522--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.2   </span> <a 
 id="x1-440003.1.2"></a>Unsupervised learning</h5>
<!--l. 523--><p class="noindent" >In <span 
class="cmbx-10">unsupervised learning</span>, the agent learns patterns in the input even though no
explicit feedback is given. It is given a bunch of data but with no answers. Typically
this is used statistically and the learning task is to <span 
class="cmbx-10">cluster</span>: detecting potentially
useful clusters of input examples. For example, a taxi company learner might learn
from weather reports what is a good day for their company or a bad day and manage
demand. This can be done without ever being given labelled examples of each by a
                                                                  

                                                                  
teacher.
<!--l. 525--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.3   </span> <a 
 id="x1-450003.1.3"></a>Semi-supervised learning</h5>
<!--l. 526--><p class="noindent" ><span 
class="cmbx-10">Semi-supervised </span>learning is just like supervised learning, but the &#8220;teacher&#8221; is not
reliable for any number of reasons:
     <ul class="itemize1">
     <li class="itemize">There is no objective truth
     </li>
     <li class="itemize">The best humans can do is fallible
     </li>
     <li class="itemize">The data is subject to lying - for example pictures of people and their age,
     some people may have lied about their age
     </li>
     <li class="itemize">The data is subject to misinterpretation - Is that number a 1 or a 7?</li></ul>
<!--l. 534--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.4   </span> <a 
 id="x1-460003.1.4"></a>Supervised learning</h5>
<!--l. 535--><p class="noindent" >In supervised learning, we &#8220;teach&#8221; the learner two things: A set of example inputs
and for each example, the &#8220;right&#8221; answer. For example in a game of chess, the board
position is an example input, with the right answer being the best move in that
position. A more obvious example would be an image of a bus and the right answer
being that is or is not a bus. To measure the accuracy of learning, we can give
the agent a <span 
class="cmbx-10">test set </span>of examples that are distinct from the training set.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 538--><p class="noindent" ><img 
src="imgs/supervised-learning.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 539--><p class="indent" >   </div><hr class="endfigure">
<!--l. 540--><p class="noindent" >When the output <span 
class="cmmi-10">y </span>is one of a finite set of values (such as <span 
class="cmti-10">bus, car, truck</span>), the
learning problem is called <span 
class="cmbx-10">classification</span>. If <span 
class="cmmi-10">y </span>is a number, the learning problem is
called <span 
class="cmbx-10">regression</span>.<br 
class="newline" />The learner has to create a function that gives the correct answer to both new and
old (training) inputs. Though this is not always possible, we still want it to give as
good an answers as possible. In general, <span 
class="cmti-10">there is a tradeoff between complex functions</span>
<span 
class="cmti-10">that fit the training data well, and simpler functions that may generalise better</span>.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 546--><p class="noindent" ><img 
src="imgs/supervised-learning-fit.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 547--><p class="indent" >   </div><hr class="endfigure">
<!--l. 548--><p class="noindent" >An example problem is curve fitting, where we are trying to find a curve that fits the
data points and with this curve, predict <span 
class="cmmi-10">f</span>(<span 
class="cmmi-10">x</span>) from new inputs of <span 
class="cmmi-10">x</span>. If we
try to fit using a <span 
class="cmbx-10">linear function </span><span 
class="cmmi-10">d </span>= 1, it is clearly not perfect, but it
might be good enough (remember there is a tradeoff between simplicity
and accuracy). We could do better with a <span 
class="cmbx-10">quadratic function </span><span 
class="cmmi-10">d </span>= 2 as it
covers most of the points. To us humans this might seem quite a good fit,
however we could do <span 
class="cmti-10">even </span>better with a function <span 
class="cmmi-10">d </span>= 6 that fits all the points
perfectly. The danger here is of <span 
class="cmbx-10">overfitting </span>our data and therefore being
unable to generalise. So how do we choose from among multiple consistent
functions?<br 
class="newline" />The principle of <span 
class="cmbx-10">Occam&#8217;s Razor </span>is to prefer the <span 
class="cmti-10">simplest </span>function that is consistent
with the data. Defining simplicity is not easy, but it is clear that a degree-1
polynomial is simpler than a degree-7 polynomial. However, even with Occam&#8217;s
Razor, we still may not know which curve to prefer so we have to choose a tradeoff of
simplicity and better fitting the data.<br 
class="newline" />The term <span 
class="cmbx-10">Overfitting </span>means that there is not enough data to justify our git. It is a
critical problem in machine learning and especially supervised learning. If we overfit
our data, not only do we have a complicated prediction, but it might make nonsense
predictions when it comes to new input data.
   <h4 class="subsectionHead"><span class="titlemark">3.2   </span> <a 
 id="x1-470003.2"></a>Decision Trees</h4>
<!--l. 556--><p class="noindent" >A <span 
class="cmbx-10">decision tree </span>represents a function that takes as input a vector of attribute
values and returns a &#8220;decision&#8221; - a single output value. The input and output
values can be discrete or continuous but for now let&#8217;s focus on the discrete
values.<br 
class="newline" />A decision tree reaches it&#8217;s decision by performing a sequence of tests. Each internal
node in the tree corresponds to a test of the value of one of the input attributes. The
branches from the node are labelled with the possible values of the attribute. As a
running example, let&#8217;s decide whether to wait at this restaurant to eat or
not to wait. First we list the attributes that we will consider as part of the
input:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Alternate</span>: is there a suitable alternative restaurant nearby?
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Bar</span>: is the restaurant&#8217;s bar a comfortable place to wait in?
     </dd><dt class="enumerate-enumitem">
                                                                  

                                                                  
  3. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Fri/Sat</span>: is it a Friday or Saturday?
     </dd><dt class="enumerate-enumitem">
  4. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Hungry</span>: are we hungry?
     </dd><dt class="enumerate-enumitem">
  5. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Patrons</span>: how many people are in the restaurant? (values are <span 
class="cmti-10">None</span>, <span 
class="cmti-10">Some</span>
     and <span 
class="cmti-10">Full</span>).
     </dd><dt class="enumerate-enumitem">
  6. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Price</span>: the restaurant&#8217;s price range ($, $$, $$$)
     </dd><dt class="enumerate-enumitem">
  7. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Raining</span>: is it raining outside?
     </dd><dt class="enumerate-enumitem">
  8. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Reservation</span>: do we have a reservation?
     </dd><dt class="enumerate-enumitem">
  9. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">Type</span>: the type of restaurant (French, Italian, Thai or burgers)
     </dd><dt class="enumerate-enumitem">
 10. </dt><dd 
class="enumerate-enumitem"><span 
class="cmti-10">WaitEstimate</span>: the wait estimated by the host (0-10 minutes, 10-30, 30-60
     or <span 
class="cmmi-10">&#x003E; </span>60)</dd></dl>
<!--l. 571--><p class="noindent" >Note that every attribute has a small set of possible values; the value of <span 
class="cmti-10">WaitEstimate </span>is
not an integer, but one of four discrete values.<br 
class="newline" />Also to note is that a boolean decision tree is logically equivalent to the assertion
that the goal attribute is true if and only if the input attributes satisfy one of the
paths leading to a leaf with value <span 
class="cmti-10">True</span>. Writing this in propositional logic gives
us:
   <table 
class="equation"><tr><td><a 
 id="x1-47011r2"></a>
   <center class="math-display" >
<img 
src="Notes1x.png" alt="Goal &#x21D4; (P ath1 &#x2228;P ath2 &#x2228; ...)
" class="math-display" ></center></td><td class="equation-label">(2)</td></tr></table>
                                                                  

                                                                  
<!--l. 576--><p class="nopar" >
Each <span 
class="cmti-10">Path </span>is a conjunction of attribute-value tests required to follow that path.
Therefore, the whole expression is equivalent to DNF (disjunctive normal form),
which means that any function in propositional logic can be expressed as a decision
tree. An example path is:
   <table 
class="equation"><tr><td><a 
 id="x1-47012r3"></a>
   <center class="math-display" >
<img 
src="Notes2x.png" alt="Path1 = (P atrons == F ull&#x2227; W aitEstimate == 0- 10)
" class="math-display" ></center></td><td class="equation-label">(3)</td></tr></table>
<!--l. 580--><p class="nopar" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.1   </span> <a 
 id="x1-480003.2.1"></a>Supervised learning with decision trees</h5>
<!--l. 582--><p class="noindent" >An example for a boolean decision tree consists of an (<span 
class="cmmi-10">x</span>, <span 
class="cmmi-10">y</span>) pair where <span 
class="cmmi-10">x </span>is a vector
of values for the input attributes and <span 
class="cmmi-10">y </span>is a single boolean output value. Given a set
of training examples, we can use an algorithm to build a decision that that could
classify new unseen inputs. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 585--><p class="noindent" ><img 
src="imgs/decision-tree-table.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 586--><p class="indent" >   </div><hr class="endfigure">
<!--l. 587--><p class="noindent" >By Occam&#8217;s razor, we want a tree that is both consistent with the training examples,
but also as small as possible. However there is no efficient way to search through all
2<sup><span 
class="cmr-7">2</span><sup><span 
class="cmmi-5">n</span></sup>
   </sup> possible decision trees so we must use some heuristics. With heuristics we can
find a small (but not the smallest) consistent tree. The <span 
class="cmbx-10">decision tree learning</span>
algorithm adopts a greedy divide-and-conquer strategy: <span 
class="cmti-10">always test the most</span>
<span 
class="cmti-10">important attribute first</span>. The &#8220;most important attribute&#8221; means the one
that makes the most difference to the classification of the examples. This
way, we hope to get the correct classification with a small number of tests,
so the paths of the tree are short and the tree as a whole will be shallow.
<hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
<a 
 id="x1-48005r2"></a>
                                                                  

                                                                  

<!--l. 591--><p class="noindent" ><img 
src="imgs/decision-tree-split.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 592--><p class="indent" >   </div><hr class="endfigure">
<!--l. 593--><p class="noindent" >In our restaurant example, <span 
class="cmti-10">Type </span>is a poor attribute to choose first, because it leaves
us with four possible outcomes, each of which have the same number of positive and
negative examples. On the other hand <span 
class="cmti-10">Patrons </span>is an important attribute as the
values of <span 
class="cmti-10">None </span>and <span 
class="cmti-10">Some </span>leave example sets which can immediately be answered
definitively. After the first attribute test splits the examples up, each outcome is a
new decision tree learning problem in itself, with fewer examples and one less
attribute.<br 
class="newline" />There are four cases to consider for the recursive problems for the sub-problems in
the tree:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem">If the remaining examples are all positive or negative, then we are finished
     as we can definitively answer <span 
class="cmti-10">Yes </span>or <span 
class="cmti-10">No</span>.
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem">If there are some mixed positive or negative examples, choose the next
     best attribute to split them again.
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem">If there are no examples left, it means that no example has been observed
     for this combination of attribute values and we return a default value
     calculated from the plurality classification of all the examples that were
     used in constructing the node&#8217;s parent.
     </dd><dt class="enumerate-enumitem">
  4. </dt><dd 
class="enumerate-enumitem">If there are no attribute left, but there are still positive and negative
     examples, it means these examples have exactly the same description but
     different classifications. This can occur due to error or noise in the data.
     The best we can do in this case is return the plurality classifications for
     the remaining examples.</dd></dl>
<!--l. 603-->
<a 
 id="x1-48006"></a>
<br />
    <div class="caption" 
><span class="id">Algorithm&#x00A0;2:
    </span><span  
class="content">Decision
    tree
    learning
    algorithm</span></div><!--tex4ht:label?: x1-48005r3 --><div class="lstlisting" id="listing-2"><span class="label"><a 
 id="x1-48007r1"></a><span 
class="cmr-9">1</span></span><span 
class="cmtt-9">function</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">DTL</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">examples</span><span 
class="cmtt-9">,</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">attributes</span><span 
class="cmtt-9">,</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">default</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48008r2"></a><span 
class="cmr-9">2</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">if</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">examples</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">is</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">empty</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48009r3"></a><span 
class="cmr-9">3</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">return</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">default</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48010r4"></a><span 
class="cmr-9">4</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48011r5"></a><span 
class="cmr-9">5</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">else</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">if</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">all</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">examples</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">have</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">the</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">same</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">classification</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48012r6"></a><span 
class="cmr-9">6</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">return</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">the</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">classification</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48013r7"></a><span 
class="cmr-9">7</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48014r8"></a><span 
class="cmr-9">8</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">else</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">if</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">attributes</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">is</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">empty</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48015r9"></a><span 
class="cmr-9">9</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">return</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">PLURALITY</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">examples</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48016r10"></a><span 
class="cmr-9">10</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48017r11"></a><span 
class="cmr-9">11</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">else</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48018r12"></a><span 
class="cmr-9">12</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">best</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">=</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">CHOOSE_ATTRIBUTE</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">attributes</span><span 
class="cmtt-9">,</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">examples</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48019r13"></a><span 
class="cmr-9">13</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">tree</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">=</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">a</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">new</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">decision</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">tree</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">with</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">root</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">test</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">best</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48020r14"></a><span 
class="cmr-9">14</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">for</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">each</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">value</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">v</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">of</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">best</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48021r15"></a><span 
class="cmr-9">15</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">examples_</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">i</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">=</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">elements</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">of</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">examples</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">with</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">best</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">=</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">v</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48022r16"></a><span 
class="cmr-9">16</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">subtree</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">=</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">DTL</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">examples_</span><span 
class="cmtt-9">{</span><span 
class="cmtt-9">i</span><span 
class="cmtt-9">},</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">attributes</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">-</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">best</span><span 
class="cmtt-9">,</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">PLURALITY</span><span 
class="cmtt-9">(</span><span 
class="cmtt-9">examples</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">)</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48023r17"></a><span 
class="cmr-9">17</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">add</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">a</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">branch</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">to</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">tree</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">with</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">label</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">v</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">and</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">subtree</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">subtree</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48024r18"></a><span 
class="cmr-9">18</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48025r19"></a><span 
class="cmr-9">19</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">return</span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">tree</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48026r20"></a><span 
class="cmr-9">20</span></span><span 
class="cmtt-9">&#x00A0;</span><span 
class="cmtt-9">}</span><span 
class="cmtt-9">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48027r21"></a><span 
class="cmr-9">21</span></span><span 
class="cmtt-9">}</span>
   </div>
<!--l. 626--><p class="indent" >   The learning algorithm will always construct a decision tree that is consistent
with the examples it is given. It should also be considerably small, not having to look
                                                                  

                                                                  
at all the attributes to classify all the examples it is given. For cases it has
never seen, the algorithm will classify based on the decision tree regardless of
whether the actual answer is correct or not. With more training examples,
the learning program can correct any mistakes it might make from unseen
input.
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.2   </span> <a 
 id="x1-490003.2.2"></a>Information gain</h5>
<!--l. 629--><p class="noindent" >A perfect attribute divides the examples into sets where each are all positive or all
negative and thus will be leaves of the tree. We can use the notion of information
gain, which is defined in terms of <span 
class="cmbx-10">entropy</span>.<br 
class="newline" />Entropy is a measure of the uncertainty of a random variable and gain in
formation corresponds to a reduction in entropy. The equation for entropy is as
follows:
   <table 
class="equation"><tr><td><a 
 id="x1-49001r4"></a>
   <center class="math-display" >
<img 
src="Notes3x.png" alt="       &#x2211;           --1--     &#x2211;
H (V ) =   P (vk)log2(P (vk)) = -   P (vk)log2P(vk)
        k                     k
" class="math-display" ></center></td><td class="equation-label">(4)</td></tr></table>
<!--l. 634--><p class="nopar" >
We also define <span 
class="cmmi-10">B</span>(<span 
class="cmmi-10">q</span>) as the entropy of a Boolean random variable that is true with
probability <span 
class="cmmi-10">q</span>:
   <table 
class="equation"><tr><td><a 
 id="x1-49002r5"></a>
   <center class="math-display" >
<img 
src="Notes4x.png" alt="B(q) = - (qlog q+ (1- q)log (1- q))
            2           2
" class="math-display" ></center></td><td class="equation-label">(5)</td></tr></table>
                                                                  

                                                                  
<!--l. 638--><p class="nopar" >
If a training set contains <span 
class="cmmi-10">p </span>positive examples and <span 
class="cmmi-10">n </span>negative examples, then the
entropy of the goal attribute on the whole set is:
   <table 
class="equation"><tr><td><a 
 id="x1-49003r6"></a>
   <center class="math-display" >
<img 
src="Notes5x.png" alt="H (Goal) = B(--p--)
            p+ n
" class="math-display" ></center></td><td class="equation-label">(6)</td></tr></table>
<!--l. 642--><p class="nopar" >
The <span 
class="cmbx-10">information gain </span>from the attribute test on A is the expected reduction in
entropy:
   <table 
class="equation"><tr><td><a 
 id="x1-49004r7"></a>
   <center class="math-display" >
<img 
src="Notes6x.png" alt="              p
Gain (A ) = B (---)- Remainder (A)
             p+ n
" class="math-display" ></center></td><td class="equation-label">(7)</td></tr></table>
<!--l. 646--><p class="nopar" >
   <table 
class="equation"><tr><td><a 
 id="x1-49005r8"></a>
   <center class="math-display" >
<img 
src="Notes7x.png" alt="               &#x2211;d
Remainder (A) =    pk +-nkB (-p-)
               k=1  p+ n    p+ n
                                                                  

                                                                  
" class="math-display" ></center></td><td class="equation-label">(8)</td></tr></table>
<!--l. 649--><p class="nopar" >
For example, after <span 
class="cmti-10">Patron</span>, the information gain is 0.54 bits.
   <table 
class="equation"><tr><td><a 
 id="x1-49006r9"></a>
   <center class="math-display" >
<img 
src="Notes8x.png" alt="Gain (P atron ) = 1- [ 2-B (0)+-4B (4)+-6B (2)] = 0.541 bits
                   12   2   12   4   12   6
" class="math-display" ></center></td><td class="equation-label">(9)</td></tr></table>
<!--l. 653--><p class="nopar" >
That is:
     <ul class="itemize1">
     <li class="itemize">2/12 chance of gain of 1 bit (<span 
class="cmti-10">None</span>)
     </li>
     <li class="itemize">4/12 chance of gain of 1 bit (<span 
class="cmti-10">Some</span>)
     </li>
     <li class="itemize">6/12 chance of gain of 0.08 bits (<span 
class="cmti-10">Full</span>)</li></ul>
<!--l. 660--><p class="noindent" >The attribute with the highest information gain is the one that should be select as the
best attribute in the decision tree learning algorithm.
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.3   </span> <a 
 id="x1-500003.2.3"></a>Learning curve</h5>
<!--l. 662--><p class="noindent" >We can evaluate the accuracy of a learning algorithm with a <span 
class="cmbx-10">learning curve</span>. If we
have 100 examples, we can split into a training set and test set. We get a function
and then measure its accuracy with the test set. The curve shows that as the training
set grows, the accuracy increases. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 665--><p class="noindent" ><img 
src="imgs/decision-tree-learning-curve.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 666--><p class="indent" >   </div><hr class="endfigure">
<!--l. 667--><p class="noindent" >There are three things we can learning from the learning curve:
     <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  1. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Realisable </span>- the representation is able to learn a perfect function in a
     reasonable number of training examples
     </dd><dt class="enumerate-enumitem">
  2. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Non-realisable </span>- the representation is <span 
class="cmti-10">not </span>able to learn a perfect function
     even with infinite examples. For example if the true method is &#8220;toss a
     coin&#8221;, we can&#8217;t get better than 50% learning
     </dd><dt class="enumerate-enumitem">
  3. </dt><dd 
class="enumerate-enumitem"><span 
class="cmbx-10">Redundant </span>- If there are too many attributes, we might find it hard to
     tell between similar attributes and other attributes might get lucky on the
     training set without actually being good.</dd></dl>
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.4   </span> <a 
 id="x1-510003.2.4"></a>Cross validation</h5>
<!--l. 675--><p class="noindent" >If we don&#8217;t have limited data, we can do cross validation. The simplest approach is
one where we randomly split the available data into training and testing set. This
method is called <span 
class="cmbx-10">Holdout cross-validation</span>. The disadvantage is that it failed to
use all the available data for training.<br 
class="newline" />A cleverer approach is called <span 
class="cmbx-10">k-fold cross-validation</span>. The idea is that each
example is used both as training data and test data. First we split the data into <span 
class="cmmi-10">k</span>
equal subsets. Then we perform <span 
class="cmmi-10">k </span>rounds of training. On each round, 1/<span 
class="cmmi-10">k </span>of the data
is held out as a test set and the remaining examples used as training data. For
example if we set <span 
class="cmmi-10">k </span>= 10. We divide the data into 10 sets and use 9 of the sets for
training and 1 for testing. Do this each of the 10 possible ways. The extreme
is to have <span 
class="cmmi-10">k </span>= <span 
class="cmmi-10">n</span>, also known as <span 
class="cmbx-10">leave-one-out cross-validation</span>. These
methods all try to assess how likely our data is to do well against unseen
data.
<!--l. 679--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.3   </span> <a 
 id="x1-520003.3"></a>Neural networks</h4>
<!--l. 681--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">4   </span> <a 
 id="x1-530004"></a>Games playing</h3>
<!--l. 682--><p class="noindent" >In AI, the most common kind of games are <span 
class="cmbx-10">zero-sum </span>or <span 
class="cmbx-10">perfect information</span>
games. These are games such as chess or go, where both players know everything
there is to know about the game position. There is no hidden information and no
random events, however the two players need not have the same set of moves
                                                                  

                                                                  
available. This means deterministic, fully observable environments where two agents
act alternately and the utility values at the end of the game are always equal and
opposite. For example if one player wins the game of chess, the other player <span 
class="cmti-10">must</span>
have lost.<br 
class="newline" />Games are very interesting as often the branching factor is very high, 35<sup><span 
class="cmr-7">100</span></sup> for chess
so complete search of the game space is not feasible. Even so, an agent must be able
to make <span 
class="cmti-10">some </span>decision when it is not possible to calculate the <span 
class="cmti-10">optimal </span>decision.
Games also penalise inefficiency heavily, as we must make use of available time to
play.<br 
class="newline" />There are a few techniques that we can use in game playing algorithms. <span 
class="cmbx-10">Pruning</span>
allows us to ignore portions of the search tree that make no difference to the final
choice, or are bad moves that should never be made. Heuristic <span 
class="cmbx-10">evaluation</span>
<span 
class="cmbx-10">functions </span>allow us to approximate the true utility of a game state without doing a
complete search. A game can be formally defined as a king of search problem with
the following elements:
     <ul class="itemize1">
     <li class="itemize"><span 
class="cmmi-10">S</span><sub><span 
class="cmr-7">0</span></sub> - The <span 
class="cmbx-10">initial state </span>of the game. For example a blank board for Go, or
     the starting position for Chess.
     </li>
     <li class="itemize"><span 
class="cmcsc-10">PLAYER(</span><span 
class="cmmi-10">s</span><span 
class="cmcsc-10">) </span>- The player whose turn it is in a given state.
     </li>
     <li class="itemize"><span 
class="cmcsc-10">ACTION(</span><span 
class="cmmi-10">s</span><span 
class="cmcsc-10">) </span>- The set of legal moves in a given state.
     </li>
     <li class="itemize"><span 
class="cmcsc-10">RESULT(</span><span 
class="cmmi-10">s,a</span><span 
class="cmcsc-10">) </span>- The <span 
class="cmbx-10">transition model</span>, which defines the result of a
     move on the state.
     </li>
     <li class="itemize"><span 
class="cmcsc-10">TERMINAL-TEST(</span><span 
class="cmmi-10">s</span><span 
class="cmcsc-10">) </span>- A <span 
class="cmbx-10">terminal test </span>to test whether the game is
     over or not.
     </li>
     <li class="itemize"><span 
class="cmcsc-10">UTILITY(</span><span 
class="cmmi-10">s,p</span><span 
class="cmcsc-10">) </span>- A <span 
class="cmbx-10">utility function </span>which is a numeric value for a game
     that ends in a terminal state <span 
class="cmmi-10">s </span>or a player <span 
class="cmmi-10">p</span>. Note, this is the <span 
class="cmbx-10">payoff</span>
     and <span 
class="cmti-10">not </span>the evaluation function and only applies to terminal states. For
     example the payoff in Chess would simply be +1 for a win, 0 for a draw
     and -1 for a loss. In Backgammon, the payoff ranges from 0 to +192 as
     the score of the game.</li></ul>
<!--l. 695--><p class="noindent" >The initial state, <span 
class="cmcsc-10">ACTIONS </span>function and <span 
class="cmcsc-10">RESULT </span>function define a <span 
class="cmbx-10">game tree </span>for
the game.
                                                                  

                                                                  
   <h4 class="subsectionHead"><span class="titlemark">4.1   </span> <a 
 id="x1-540004.1"></a>Game trees</h4>
<!--l. 697--><p class="noindent" >A <span 
class="cmbx-10">game tree </span>is like a search tree where each node is a search state with full details
about the position. The edges between the nodes correspond to moves in the game.
The leaf nodes of the tree are determined positions such as win/lose/draw or a
certain number of points for or against the player. At each depth, it is one or the
other player&#8217;s turn to make a move. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 700--><p class="noindent" ><img 
src="imgs/gametree.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 701--><p class="indent" >   </div><hr class="endfigure">
<!--l. 702--><p class="noindent" >There are strong similarities between search trees and game trees. Infinite loops may
also exist in game trees, for example moving a Rook back and forth over and over
again. The key difference between a game tree and a search tree is that there is an
<span 
class="cmbx-10">opponent</span>. Because of this opponent, it is not good enough just to find a path to win
a game, the agent must have some form of winning strategy that it uses. We cannot
just look at one leaf node and typically need lots of different winning leaf
nodes.<br 
class="newline" />Because it is usually impossible to solve games completely and do a complete search
of the game tree, we usually can&#8217;t see any leaf nodes at most points during the game.
So we have to <span 
class="cmti-10">estimate </span>the cost of internal nodes with a <span 
class="cmbx-10">static evaluation function</span>
that gives a heuristic value of the node. An evaluation function is an estimate of the
true value of a node and can partially indicate the position of the game. An
example evaluation function for chess would be having each piece be worth
certain values: <span 
class="cmmi-10">Pawn </span>= 1, <span 
class="cmmi-10">Knight </span>= <span 
class="cmmi-10">Bishop </span>= 3, <span 
class="cmmi-10">Rook </span>= 5, <span 
class="cmmi-10">Queen </span>= 9,
<span 
class="cmmi-10">King </span>= 1000.
   <h4 class="subsectionHead"><span class="titlemark">4.2   </span> <a 
 id="x1-550004.2"></a>The minimax algorithm</h4>
<!--l. 707--><p class="noindent" >We consider a game with two players, <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">a</span><span 
class="small-caps">x</span> </span>and <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span></span>. <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">a</span><span 
class="small-caps">x</span> </span>wants to maximise his
score while <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span> </span>wants to minimise her score. The optimal strategy can be determined
from the <span 
class="cmbx-10">minimax value </span>of each node, which is the utility of being in the
corresponding state, <span 
class="cmti-10">assuming that both players play optimally</span>. Given the choice,
<span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">a</span><span 
class="small-caps">x</span> </span>prefers to move to a state with higher value and <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span> </span>prefers a state of lower
value. <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  

<!--l. 710--><p class="noindent" ><img 
src="imgs/minimax.png" alt="PIC"  
>
                                                                  

                                                                  
<!--l. 711--><p class="indent" >   </div><hr class="endfigure">
<!--l. 712--><p class="noindent" >The algorithm computes the minimax decision at the current node. Then it simply
uses recursion to work out the static evaluations (or minimax decisions) of all child
nodes either until the leaf nodes or down until a certain depth. Then all the values
are propagated upwards through the tree. The score of the <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">a</span><span 
class="small-caps">x</span> </span>nodes is the
maximum value of its child nodes and vice versa for the <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span> </span>nodes. Propagating all
the way back up to the current node (root node) gives the score of possible
moves from the root node and hence the best move to make. The algorithm
performs a complete depth-first exploration of the game tree, so it is space
efficient.<br 
class="newline" />The problem with the minimax algorithm is that it is <span 
class="cmti-10">horrendously </span>inefficient. If we
go to a depth <span 
class="cmmi-10">d </span>with a branching rate of <span 
class="cmmi-10">b</span>, then we must explore <span 
class="cmmi-10">b</span><sup><span 
class="cmmi-7">d</span></sup> nodes and
calculate the score at every node. This is exponential to the depth of the tree.
However, much of this work is wasted as we don&#8217;t need to know the score of those
nodes. The trick is to <span 
class="cmbx-10">prune </span>the tree to eliminate nodes in the game tree we do not
need to explore.
   <h4 class="subsectionHead"><span class="titlemark">4.3   </span> <a 
 id="x1-560004.3"></a>Alpha-Beta pruning</h4>
<!--l. 717--><p class="noindent" ><span 
class="cmbx-10">Alpha-beta pruning </span>works by pruning away branches that have no influence on
the final decision. It returns the same move as minimax would, except we
have to search less of the game tree. Alpha-beta pruning can be applied
to trees of any depth and can often prune entire subtrees rather than just
leaves.<br 
class="newline" />We have two parameters <span 
class="cmmi-10">&#x03B1;</span><span 
class="cmmi-10">&#x00A0;and&#x03B2;</span><span 
class="cmmi-10">&#x00A0;thatdescribetheboundsonthebacked</span><span 
class="cmsy-10">-</span><span 
class="cmmi-10">upvaluesthatappearonthenodesofthetree.</span>
     <span 
class="cmmi-10">&#x03B1;</span><span 
class="cmmi-10">&#x00A0; </span>= <span 
class="cmmi-10">thevalueofthebest</span>(<span 
class="cmmi-10">highestvalue</span>)<span 
class="cmmi-10">choicewehavefoundsofaratanychoicepointalongthepathfor</span><span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">a</span><span 
class="small-caps">x</span></span><span 
class="cmmi-10">.&#x03B2;</span><span 
class="cmmi-10">&#x00A0; </span>=
     <span 
class="cmmi-10">thevalueofthebest</span>(<span 
class="cmmi-10">lowestvalue</span>)<span 
class="cmmi-10">choicewehavefoundsofaratanychoicepointalongthepathfor</span><span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span></span><span 
class="cmmi-10">.</span>
     <!--l. 723--><p class="noindent" >
     <!--l. 724--><p class="noindent" >Alpha-beta search updates the values of <span 
class="cmmi-10">&#x03B1;</span><span 
class="cmmi-10">&#x00A0;and&#x03B2;</span><span 
class="cmmi-10">&#x00A0;asitgoesthroughthedepthfirstsearchandprunesanybranchesatanodeassoonasthevalueofthecurrentnodeisknowntobeworsethanthecurrent&#x03B1;</span><span 
class="cmmi-10">&#x00A0;or&#x03B2;</span><span 
class="cmmi-10">&#x00A0;valuefor</span><span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">a</span><span 
class="small-caps">x</span></span><span 
class="cmmi-10">and</span><span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span></span><span 
class="cmmi-10">respectively.Eithera&#x03B1;</span><span 
class="cmmi-10">&#x00A0;or&#x03B2;</span><span 
class="cmmi-10">&#x00A0;cutoffcanoccur.</span><hr class="figure"><div class="figure" 
><img 
src="imgs/alphabeta.png" alt="PIC"  
></div><hr class="endfigure">
<!--l. 729--><p class="noindent" >An <span 
class="cmbx-10">alpha cutoff </span>occurs at a <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span> </span>node, when <span 
class="cmmi-10">&#x03B1; </span><span 
class="cmsy-10">&#x2265; </span><span 
class="cmmi-10">&#x03B2; </span><br 
class="newline" />A <span 
class="cmbx-10">beta cutoff </span>occurs at a <span 
class="cmcsc-10"><span 
class="small-caps">m</span><span 
class="small-caps">a</span><span 
class="small-caps">x</span> </span>node, when <span 
class="cmmi-10">&#x03B2; </span><span 
class="cmsy-10">&#x2264; </span><span 
class="cmmi-10">&#x03B1;</span><br 
class="newline" />In    other    words,    at    every    node,    check    if    the    <span 
class="cmmi-10">&#x03B1;</span><span 
class="cmmi-10">&#x00A0;valueis     </span><span 
class="cmsy-10">&#x2265;</span>
 <span 
class="cmmi-10">&#x00A0;thanthe&#x03B2;</span><span 
class="cmmi-10">&#x00A0;value,ifitis,cutoffthesearchandgobackuptotheparentnode.</span><!--l. 734-->
<!--l. 769--><p class="noindent" >
                                                                  

                                                                  
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">4.4  </span></span> <a 
 id="x1-570004.4"></a><span 
class="cmtt-9">Endgame database</span></h4>
<!--l. 770--><p class="noindent" ><span 
class="cmtt-9">Sometimes, it might be overkill for game playing AIs to search through millions</span>
<span 
class="cmtt-9">of search trees just for its opening moves. For example in chess, there have</span>
<span 
class="cmtt-9">been many books on how to play the opening and endgame. Therefore our computer</span>
<span 
class="cmtt-9">program can use a look-up table to store a list of good opening moves to play.</span>
<span 
class="cmtt-9">Statistics on each set of opening moves can also be gathered in a database of</span>
<span 
class="cmtt-9">previously played games to get a statistically best opening move. This</span>
<span 
class="cmtt-9">works for the first few moves, though after around 10 moves, the agent</span>
<span 
class="cmtt-9">must go back to searching as the board position is one that is rarely</span>
<span 
class="cmtt-9">seen.</span><br 
class="newline" /><span 
class="cmtt-9">Similarly, in the endgame, there are again few possibilities. For example</span>
<span 
class="cmtt-9">in chess, only a few pieces remain on each side. Here, a computer can</span>
<span 
class="cmtt-9">simply store all the millions of combinations of pieces and positions</span>
<span 
class="cmtt-9">in a database and from that know what the best move is in </span><span 
class="cmitt-10x-x-90">any </span><span 
class="cmtt-9">endgame</span>
<span 
class="cmtt-9">position.</span><br 
class="newline" /><span 
class="cmtt-9">To generate endgame databases, there are four steps:</span>
          <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">1.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Generate every possible position. For example if there are 2</span>
          <span 
class="cmtt-9">Knights and a King for B and 1 Knight, 1 Rook, 1 King for W,</span>
          <span 
class="cmtt-9">generate all the possible combinations of positions the pieces</span>
          <span 
class="cmtt-9">can be in.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">2.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">For each position, mark as a win if W won, marks as a loss if</span>
          <span 
class="cmtt-9">W lost and mark as drawn otherwise.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">3.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">While there are new positions that are marked, for every</span>
          <span 
class="cmtt-9">unmarked position with W to move, mark as win if the move</span>
          <span 
class="cmtt-9">leads to a win and mark as loss if the move leads to a loss.</span>
          <span 
class="cmtt-9">Do the same for B.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">4.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">All unmarked positions are marked as drawn.</span></dd></dl>
<!--l. 781--><p class="noindent" ><span 
class="cmtt-9">Once all the positions are marked, we know the number of moves to win for every</span>
<span 
class="cmtt-9">position and so knowing the status of any position in the database is just a</span>
<span 
class="cmtt-9">single lookup. To find the best move in any Win/n position, it is simply any</span>
<span 
class="cmtt-9">Win/n-1 position.</span>
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">4.5  </span></span> <a 
 id="x1-580004.5"></a><span 
class="cmtt-9">Monte Carlo Tree Search</span></h4>
<!--l. 783--><p class="noindent" ><span 
class="cmtt-9">The name Monte Carlo is a general technique in Computer Science and other areas</span>
<span 
class="cmtt-9">beyond. It is used when one can&#8217;t sample more than a small percentage of the</span>
<span 
class="cmtt-9">full search space. Instead we generate random cases and use these to build up a</span>
<span 
class="cmtt-9">statistical sample of the truth. Instead of searching the tree exhaustively,</span>
                                                                  

                                                                  
<span 
class="cmtt-9">simply </span><span 
class="cmitt-10x-x-90">sample </span><span 
class="cmtt-9">it. Then play each branch you look to the end of the game,</span>
<span 
class="cmtt-9">making random moves and add the results as to who wins to the tree. This means</span>
<span 
class="cmtt-9">we don&#8217;t need a static evaluation because we know the winner/loser. </span><hr class="figure"><div class="figure" 
>
<img 
src="imgs/mcts.png" alt="PIC"  
>
</div><hr class="endfigure">
<!--l. 788--><p class="noindent" ><span 
class="cmtt-9">There are four stages to Monte Carlo Tree Search:</span>
          <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">1.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Selection - Select a node to expand using a policy. The policy</span>
          <span 
class="cmtt-9">should have a balance between </span><span 
class="cmitt-10x-x-90">exploitation </span><span 
class="cmtt-9">of known good moves</span>
          <span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">exploration </span><span 
class="cmtt-9">of less visited moves.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">2.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Expansion - Choose the heuristically &#8216;&#8216;best" move which has</span>
          <span 
class="cmtt-9">not yet been expanded and expand the selection node.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">3.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Simulation - Simulate from this new node to the </span><span 
class="cmitt-10x-x-90">end of the</span>
          <span 
class="cmitt-10x-x-90">game</span><span 
class="cmtt-9">, usually with random moves</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">4.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Back-propagation - Finally, propagate the results back up the</span>
          <span 
class="cmtt-9">tree and update the statistics of each expanded node to record</span>
          <span 
class="cmtt-9">who won the game.</span></dd></dl>
<!--l. 795--><p class="noindent" ><span 
class="cmtt-9">The Monte Carlo Tree Search algorithm has many distinct advantages which make</span>
<span 
class="cmtt-9">it very suitable for games without perfect information or games where the</span>
<span 
class="cmtt-9">search space and branching rate is too high or it is difficult to come up with</span>
<span 
class="cmtt-9">a static evaluation function. The problem with MCTS is it can take a long time</span>
<span 
class="cmtt-9">to converge to accurate assessments of moves, though we can add heuristics to</span>
<span 
class="cmtt-9">improve this. This is the main method (along with machine learning) that has</span>
<span 
class="cmtt-9">revolutionised Computer Go playing.</span>
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">4.6  </span></span> <a 
 id="x1-590004.6"></a><span 
class="cmtt-9">Solving checkers</span></h4>
<!--l. 798--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark"><span 
class="cmtt-9">5  </span></span> <a 
 id="x1-600005"></a><span 
class="cmtt-9">Automated Reasoning</span></h3>
<!--l. 799--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.1  </span></span> <a 
 id="x1-610005.1"></a><span 
class="cmtt-9">What is logic?</span></h4>
<!--l. 800--><p class="noindent" ><span 
class="cmtt-9">Logic is reasoning to deduce new facts given facts we already have and the</span>
<span 
class="cmtt-9">study of how that reasoning is done. It can be done in any language,</span>
<span 
class="cmtt-9">but it can also be done using </span><span 
class="cmitt-10x-x-90">symbols</span><span 
class="cmtt-9">. We are mostly concerned with</span>
<span 
class="cmtt-9">symbolic logic, that is, logical reasoning where we are dealing with</span>
<span 
class="cmtt-9">symbols.</span><br 
class="newline" /><span 
class="cmtt-9">Logical sentences must be expressed both according to the syntax and semantics.</span>
<span 
class="cmtt-9">The notion of syntax is what specifies all the sentences that are well formed.</span>
<span 
class="cmtt-9">For example in normal arithmetic, the sentence &#8216;&#8216;</span><span 
class="cmmi-9">x </span><span 
class="cmr-9">+ </span><span 
class="cmmi-9">y </span><span 
class="cmr-9">= 4</span><span 
class="cmtt-9">" follows proper</span>
<span 
class="cmtt-9">syntax, whereas &#8216;&#8216;</span><span 
class="cmmi-9">x</span><span 
class="cmr-9">4</span><span 
class="cmmi-9">y</span><span 
class="cmr-9">+ =</span><span 
class="cmtt-9">" does not. Semantics defines the meaning of truth</span>
<span 
class="cmtt-9">of each sentence with respects to </span><span 
class="cmitt-10x-x-90">each possible word</span><span 
class="cmtt-9">. For example the</span>
<span 
class="cmtt-9">sentence &#8216;&#8216;</span><span 
class="cmmi-9">x </span><span 
class="cmr-9">+ </span><span 
class="cmmi-9">y </span><span 
class="cmr-9">= 4</span><span 
class="cmtt-9">" is true in a world where </span><span 
class="cmmi-9">x </span><span 
class="cmr-9">= 2  </span><span 
class="cmtt-9">and </span><span 
class="cmmi-9">y </span><span 
class="cmr-9">= 2  </span><span 
class="cmtt-9">or </span><span 
class="cmmi-9">x </span><span 
class="cmr-9">= 3  </span><span 
class="cmtt-9">and</span>
<span 
class="cmmi-9">y </span><span 
class="cmr-9">= 1</span><span 
class="cmtt-9">. However, it is false in a world where </span><span 
class="cmmi-9">x </span><span 
class="cmr-9">= 1  </span><span 
class="cmtt-9">and </span><span 
class="cmmi-9">y </span><span 
class="cmr-9">= 1</span><span 
class="cmtt-9">. In standard</span>
<span 
class="cmtt-9">logics, every sentence must be either true or false, there is no in</span>
<span 
class="cmtt-9">between.</span><br 
class="newline" /><span 
class="cmtt-9">There isn&#8217;t just one kind of symbolic logic, in fact there are </span><span 
class="cmitt-10x-x-90">many</span><span 
class="cmtt-9">. There are</span>
<span 
class="cmtt-9">two main reasons why there are different kinds of logic:</span>
          <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">1.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">We might choose one logical system over another for efficiency or</span>
          <span 
class="cmtt-9">where one system is better suited for the given task. This is analogous</span>
          <span 
class="cmtt-9">to having different programming languages, where one might choose</span>
          <span 
class="cmtt-9">the language based on the task. In general, there is a tradeoff</span>
          <span 
class="cmtt-9">between the expression power of that logic system and the reasoning</span>
          <span 
class="cmtt-9">complexity of that logic. The more expressive power a logic has,</span>
          <span 
class="cmtt-9">the more it can express concepts </span><span 
class="cmitt-10x-x-90">easily </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">accurately</span><span 
class="cmtt-9">. Easily here</span>
          <span 
class="cmtt-9">means we can write things down more concisely and accurately means</span>
          <span 
class="cmtt-9">we can write things down which equate to what we really mean. For</span>
          <span 
class="cmtt-9">example propositional logic allows us only to express &#8216;&#8216;propositions"</span>
          <span 
class="cmtt-9">while first order logic allows us to express facts and relationships</span>
          <span 
class="cmtt-9">between objects.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">2.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Different logic systems give us </span><span 
class="cmitt-10x-x-90">different results</span><span 
class="cmtt-9">. We might want to</span>
          <span 
class="cmtt-9">reason in different ways in different situations to more accurately</span>
          <span 
class="cmtt-9">depict the situation. Take this sample situation: A shop has two</span>
          <span 
class="cmtt-9">copies of &#8216;&#8216;Wisden 1976", one hardback and one paperback. Another</span>
          <span 
class="cmtt-9">customer had preordered a copy but the bookseller could not remember</span>
          <span 
class="cmtt-9">which version so he got one of each and is waiting for that</span>
          <span 
class="cmtt-9">customer. The question is can we buy a copy of &#8216;&#8216;Wisden"? </span><hr class="figure"><div class="figure" 
> <img 
src="imgs/wisden.jpg" alt="PIC"  
>
          <img 
src="imgs/wisden-paper.jpg" alt="PIC"  
>
          </div><hr class="endfigure">
              <ul class="itemize1">
              <li class="itemize"><span 
class="cmtt-9">With propositional logic, the answer is yes.</span>
                 <ul class="itemize2">
                 <li class="itemize"><span 
class="cmmi-9">P</span><span 
class="cmtt-9">: other customer ordered paperback</span>
                 </li>
                 <li class="itemize"><span 
class="cmmi-9">H</span><span 
class="cmtt-9">: other customer ordered hardback</span>
                 </li>
                 <li class="itemize"><span 
class="cmmi-9">BP</span><span 
class="cmtt-9">: we can buy the paperback</span>
                 </li>
                 <li class="itemize"><span 
class="cmmi-9">BH</span><span 
class="cmtt-9">: we can buy the hardback</span></li></ul>
                                                                  

                                                                  
              <!--l. 821--><p class="noindent" ><span 
class="cmtt-9">The other customer ordered one but not both, so it follows</span>
              <span 
class="cmtt-9">that:</span>
                 <ul class="itemize2">
                 <li class="itemize"><span 
class="cmmi-9">P </span><span 
class="cmsy-9">&#x2192; </span><span 
class="cmmi-9">BH</span>
                 </li>
                 <li class="itemize"><span 
class="cmmi-9">H </span><span 
class="cmsy-9">&#x2192; </span><span 
class="cmmi-9">BP</span>
                 </li>
                 <li class="itemize"><span 
class="cmmi-9">P </span><span 
class="cmsy-9">&#x2194;¬</span><span 
class="cmmi-9">H</span></li></ul>
              <!--l. 827--><p class="noindent" ><span 
class="cmtt-9">This gives the result </span><span 
class="cmmi-9">BP </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">BH</span><span 
class="cmtt-9">, so we can buy either the</span>
              <span 
class="cmtt-9">hardback or the paperback.</span>
              </li>
              <li class="itemize"><span 
class="cmtt-9">However, if we use constructive logic, it is not enough to know</span>
              <span 
class="cmtt-9">something is true, we have to able to </span><span 
class="cmitt-10x-x-90">prove </span><span 
class="cmtt-9">it and the</span>
              <span 
class="cmtt-9">proof must be &#8216;&#8216;constructive". For example if we want</span>
              <span 
class="cmtt-9">to prove </span><span 
class="cmmi-9">P </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">Q</span><span 
class="cmtt-9">, we must </span><span 
class="cmitt-10x-x-90">either </span><span 
class="cmtt-9">be able to prove </span><span 
class="cmmi-9">P  </span><span 
class="cmtt-9">or</span>
              <span 
class="cmtt-9">be able to prove </span><span 
class="cmmi-9">Q</span><span 
class="cmtt-9">. Following the Wisden example, it</span>
              <span 
class="cmtt-9">is not enough to know </span><span 
class="cmmi-9">BP </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">BH</span><span 
class="cmtt-9">, we have to know </span><span 
class="cmitt-10x-x-90">which</span>
              <span 
class="cmtt-9">one:</span>
                 <ul class="itemize2">
                 <li class="itemize"><span 
class="cmtt-9">We can&#8217;t prove </span><span 
class="cmmi-9">BP  </span><span 
class="cmtt-9">because </span><span 
class="cmmi-9">P  </span><span 
class="cmtt-9">might be true</span>
                 </li>
                 <li class="itemize"><span 
class="cmtt-9">We can&#8217;t prove </span><span 
class="cmmi-9">BH </span><span 
class="cmtt-9">because </span><span 
class="cmmi-9">H </span><span 
class="cmtt-9">might be true</span>
                 </li>
                 <li class="itemize"><span 
class="cmtt-9">So we can&#8217;t give a constructive proof of </span><span 
class="cmmi-9">BP </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">BH</span></li></ul>
              </li></ul>
          <!--l. 835--><p class="noindent" ><span 
class="cmtt-9">In this example, constructive logic is a better match to reality as</span>
          <span 
class="cmtt-9">the bookseller cannot s By this logic system, the result is that we</span>
          <span 
class="cmitt-10x-x-90">cannot </span><span 
class="cmtt-9">buy either version of the book.ell either copy because he had</span>
          <span 
class="cmtt-9">to keep them both for the other customer to get whichever he</span>
          <span 
class="cmtt-9">wanted.</span></dd></dl>
<!--l. 838--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.1.1  </span></span> <a 
 id="x1-620005.1.1"></a><span 
class="cmtt-9">Entailment</span></h5>
<!--l. 839--><p class="noindent" ><span 
class="cmtt-9">To talk about logical reasoning, we can use the concept of logical entailment</span>
<span 
class="cmtt-9">between sentences. The idea is that a sentence </span><span 
class="cmitt-10x-x-90">follows logically </span><span 
class="cmtt-9">from another</span>
<span 
class="cmtt-9">sentence and can be written as:</span>
<table 
class="equation"><tr><td><a 
 id="x1-62001r10"></a>
<center class="math-display" >
                                                                  

                                                                  
<img 
src="Notes9x.png" alt="&#x03B1;  &#x22A8;&#x03B2;
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(10)</span></td></tr></table>
<!--l. 842--><p class="nopar" >
<span 
class="cmtt-9">This means that the sentence </span><span 
class="cmmi-9">&#x03B1; </span><span 
class="cmtt-9">entails the sentence </span><span 
class="cmmi-9">&#x03B2;</span><span 
class="cmtt-9">. The formal definition</span>
<span 
class="cmtt-9">of entailment is </span><span 
class="cmmi-9">&#x03B1; </span><span 
class="msam-10x-x-90">&#x22A8; </span><span 
class="cmmi-9">&#x03B2; </span><span 
class="cmtt-9">if and only if, in every model in which </span><span 
class="cmmi-9">&#x03B1; </span><span 
class="cmtt-9">is true, </span><span 
class="cmmi-9">&#x03B2; </span><span 
class="cmtt-9">is</span>
<span 
class="cmtt-9">also true.</span><br 
class="newline" /><span 
class="cmtt-9">Now, if we have some Knowledge base &#8216;&#8216;KB" of a set of sentences in either</span>
<span 
class="cmtt-9">propositional or first order logic, and another sentence S in that logic</span>
<span 
class="cmtt-9">system, it can be said that </span><span 
class="cmmi-9">KB </span><span 
class="msam-10x-x-90">&#x22A8; </span><span 
class="cmmi-9">S</span><span 
class="cmtt-9">. So in any interpretation of any model that</span>
<span 
class="cmtt-9">makes KB true, S must also be true.</span><br 
class="newline" /><span 
class="cmtt-9">The symbol </span><span 
class="cmsy-9">&#x22A2; </span><span 
class="cmmi-9">isthe</span><span 
class="cmitt-10x-x-90">single turnstile</span><span 
class="cmmi-9">andrepresents</span><span 
class="cmtt-9">syntactic entailment</span><span 
class="cmmi-9">whereasthe </span><span 
class="msam-10x-x-90">&#x22A8;</span>
<span 
class="cmr-9">(</span><span 
class="cmmi-9">doubleturnstile</span><span 
class="cmr-9">)</span><span 
class="cmmi-9">represents</span><span 
class="cmtt-9">semantic entailment</span><span 
class="cmmi-9">.</span><span 
class="cmtt-9">Syntactic entailment:</span><span 
class="cmmi-9">&#x03B1; </span><span 
class="cmsy-9">&#x22A2; </span><span 
class="cmmi-9">&#x03B2; </span><span 
class="cmtt-9">says</span>
<span 
class="cmtt-9">that a sentence </span><span 
class="cmmi-9">&#x03B2; </span><span 
class="cmtt-9">is </span><span 
class="cmitt-10x-x-90">provable </span><span 
class="cmtt-9">from the set of assumption </span><span 
class="cmmi-9">&#x03B1;</span><span 
class="cmtt-9">. </span><br 
class="newline" /><span 
class="cmtt-9">Semantic entailment: </span><span 
class="cmmi-9">&#x03B1; </span><span 
class="msam-10x-x-90">&#x22A8; </span><span 
class="cmmi-9">&#x03B2; </span><span 
class="cmtt-9">says that a sentence is </span><span 
class="cmitt-10x-x-90">true </span><span 
class="cmtt-9">in all models of</span>
<span 
class="cmmi-9">&#x03B1;</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">G</span><span 
class="cmtt-9">ödel&#8217;s Completeness theorem proved that for the right set of inference rules,</span>
<span 
class="cmtt-9">if </span><span 
class="cmmi-9">KB </span><span 
class="msam-10x-x-90">&#x22A8; </span><span 
class="cmmi-9">S </span><span 
class="cmtt-9">then </span><span 
class="cmmi-9">KB </span><span 
class="cmsy-9">&#x22A2; </span><span 
class="cmmi-9">S</span><span 
class="cmtt-9">. In other words, if something is true in all</span>
<span 
class="cmtt-9">models then we can prove it. We say that the set of inference rules</span>
<span 
class="cmtt-9">is </span><span 
class="cmitt-10x-x-90">complete</span><span 
class="cmtt-9">. For the reverse case we also need that if </span><span 
class="cmmi-9">KB </span><span 
class="cmsy-9">&#x22A2; </span><span 
class="cmmi-9">S </span><span 
class="cmtt-9">then</span>
<span 
class="cmmi-9">KB </span><span 
class="msam-10x-x-90">&#x22A8; </span><span 
class="cmmi-9">S</span><span 
class="cmtt-9">. This is called soundness and is usually easier to prove than</span>
<span 
class="cmtt-9">completeness.</span>
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.2  </span></span> <a 
 id="x1-630005.2"></a><span 
class="cmtt-9">Propositional logic</span></h4>
<!--l. 855--><p class="noindent" ><span 
class="cmtt-9">Propositional logic is a logic system that studies ways of joining and/or</span>
<span 
class="cmtt-9">modifying entire propositions, statements or sentences to form more complicated</span>
<span 
class="cmtt-9">propositions, statements or sentences, as well as the logical relationship and</span>
<span 
class="cmtt-9">properties that are derived from these methods of combining or altering</span>
<span 
class="cmtt-9">statements.</span>
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.2.1  </span></span> <a 
 id="x1-640005.2.1"></a><span 
class="cmtt-9">Syntax</span></h5>
<!--l. 857--><p class="noindent" ><span 
class="cmtt-9">Atomic sentences in propositional logic consist of a single propositional</span>
<span 
class="cmtt-9">symbol. Each symbol represents a propositions that can either be true or</span>
<span 
class="cmtt-9">false. Symbols start with an uppercase and may contain subscripts and</span>
<span 
class="cmtt-9">other letters, for example: </span><span 
class="cmmi-9">P</span><span 
class="cmtt-9">, </span><span 
class="cmmi-9">Q</span><span 
class="cmtt-9">, </span><span 
class="cmmi-9">R</span><sub><span 
class="cmr-6">1</span><span 
class="cmmi-6">,</span><span 
class="cmr-6">2</span></sub><span 
class="cmtt-9">, </span><span 
class="cmmi-9">W</span><sub><span 
class="cmmi-6">s</span></sub>  <span 
class="cmtt-9">and </span><span 
class="cmmi-9">West</span><span 
class="cmtt-9">. These symbols</span>
<span 
class="cmtt-9">are usually arbitrary but can have mnemonic value. </span><span 
class="cmitt-10x-x-90">True </span><span 
class="cmtt-9">is the always</span>
<span 
class="cmtt-9">true proposition and </span><span 
class="cmitt-10x-x-90">False </span><span 
class="cmtt-9">is the always false proposition. There are</span>
                                                                  

                                                                  
<span 
class="cmtt-9">also five commonly used logical connectives to connect propositions: </span><hr class="figure"><div class="figure" 
>
<img 
src="imgs/propositional-symbols.png" alt="PIC"  
>
</div><hr class="endfigure">
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmsy-9">¬ </span><span 
class="cmtt-9">(not). This is the negation, for example </span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P  </span><span 
class="cmtt-9">is the negation</span>
          <span 
class="cmtt-9">of </span><span 
class="cmmi-9">P</span><span 
class="cmtt-9">. A literal is either an atomic sentence (positive</span>
          <span 
class="cmtt-9">literal) or a negative atomic sentence (negative literal).</span>
          </li>
          <li class="itemize"><span 
class="cmsy-9">&#x2227; </span><span 
class="cmtt-9">(and). This is called a conjunction. For</span>
          <span 
class="cmtt-9">example </span><span 
class="cmmi-9">P </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">Q </span><span 
class="cmtt-9">is a conjunction of the literals P and Q. It can</span>
          <span 
class="cmtt-9">be thought of as the AND logic gate.</span>
          </li>
          <li class="itemize"><span 
class="cmsy-9">&#x2228; </span><span 
class="cmtt-9">(or). This is called a disjunction, we can be thought of as</span>
          <span 
class="cmtt-9">the OR logic gate.</span>
          </li>
          <li class="itemize"><span 
class="cmsy-9">&#x2295; </span><span 
class="cmtt-9">(xor). Just like the XOR logic gate, this is an exclusive</span>
          <span 
class="cmtt-9">OR.</span>
          </li>
          <li class="itemize"><span 
class="cmsy-9">&#x2192; </span><span 
class="cmtt-9">(implies). A sentence</span>
          <span 
class="cmtt-9">such as </span><span 
class="cmmi-9">P </span><span 
class="cmsy-9">&#x2192; </span><span 
class="cmmi-9">Q </span><span 
class="cmtt-9">is called an implication. Implications are also</span>
          <span 
class="cmtt-9">known as if-then statements. </span><span 
class="cmmi-9">p  </span><span 
class="cmsy-9">&#x2192; </span><span 
class="cmmi-9">q </span><span 
class="cmtt-9">is logically equivalent to</span>
          <span 
class="cmsy-9">¬</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">p </span><span 
class="cmsy-9">&#x2227;¬</span><span 
class="cmmi-9">q</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">and by De Morgan&#8217;s law equivalent to </span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">p </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">q</span><span 
class="cmtt-9">.</span></li></ul>
<!--l. 869--><p class="noindent" ><span 
class="cmtt-9">Two propositional formulae, </span><span 
class="cmmi-9">A </span><span 
class="cmtt-9">and </span><span 
class="cmmi-9">B </span><span 
class="cmtt-9">are called equivalent if </span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2194; </span><span 
class="cmmi-9">B </span><span 
class="cmtt-9">is a</span>
<span 
class="cmtt-9">tautology. The symbol </span><span 
class="cmsy-9">&#x2194; </span><span 
class="cmtt-9">stands for if and only if. A logic formula can also</span>
<span 
class="cmtt-9">have properties which can be one or more of the following:</span>
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmtt-9">Valid or Tautology - Every way of assigning the variables true</span>
          <span 
class="cmtt-9">or false makes the formula true</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Invalid - Not valid, there is at least one assignment which</span>
          <span 
class="cmtt-9">makes the formula untrue</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Satisfiable - At least one assignment of true/false that makes</span>
          <span 
class="cmtt-9">the formula true. It follows then that all valid formulae are</span>
          <span 
class="cmtt-9">also satisfiable.</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Unsatisfiable - No assignment of true/false can make the</span>
          <span 
class="cmtt-9">formula true.</span></li></ul>
<!--l. 876--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.2.2  </span></span> <a 
 id="x1-650005.2.2"></a><span 
class="cmtt-9">Conjunctive normal form</span></h5>
                                                                  

                                                                  
<!--l. 877--><p class="noindent" ><span 
class="cmtt-9">CNF (Conjunctive normal form) is a restricted form of propositional logic</span>
<span 
class="cmtt-9">formula. By converting propositional logic to CNF, we can make more specialised</span>
<span 
class="cmtt-9">and efficient algorithms that work only on CNF.</span><br 
class="newline" /><span 
class="cmtt-9">A formula in CNF is a conjunction (</span><span 
class="cmsy-9">&#x2227;</span><span 
class="cmtt-9">) of disjunctions (</span><span 
class="cmsy-9">&#x2228;</span><span 
class="cmtt-9">). In other words, all</span>
<span 
class="cmtt-9">CNF formula follow something like the following:</span>
<table 
class="equation"><tr><td><a 
 id="x1-65001r11"></a>
<center class="math-display" >
<img 
src="Notes10x.png" alt="(¬A &#x2228; B&#x2228; C)&#x2227; (D &#x2228;E )&#x2227;(F &#x2228;G &#x2228; ¬H)...
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(11)</span></td></tr></table>
<!--l. 882--><p class="nopar" >
<span 
class="cmtt-9">Each formula in parenthesis () is a clause and every variable is a literal</span>
<span 
class="cmtt-9">which can negated. </span><span 
class="cmitt-10x-x-90">Any </span><span 
class="cmtt-9">propositional formula has an </span><span 
class="cmitt-10x-x-90">equivalent </span><span 
class="cmtt-9">CNF. The</span>
<span 
class="cmtt-9">order of clauses and literals does not matter, nor do any repeats. A</span>
<span 
class="cmtt-9">CNF including an empty clause (), that is it contains a clause with no</span>
<span 
class="cmtt-9">literals:</span>
<table 
class="equation"><tr><td><a 
 id="x1-65002r12"></a>
<center class="math-display" >
<img 
src="Notes11x.png" alt="(A &#x2228; B)&#x2227; ()&#x2227; (B &#x2228;¬C )
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(12)</span></td></tr></table>
<!--l. 886--><p class="nopar" >
<span 
class="cmtt-9">is unsatisfiable (always false). An empty CNF (not empty clause) is a tautology</span>
<span 
class="cmtt-9">and always true. An empty CNF means an empty </span><span 
class="cmitt-10x-x-90">set of clauses</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">There is a simple procedure to follow to convert from any propositional logic</span>
<span 
class="cmtt-9">formulae to CNF. The steps are as follows:</span>
                                                                  

                                                                  
          <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">1.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Eliminate any </span><span 
class="cmsy-9">&#x2192;</span><span 
class="cmtt-9">, </span><span 
class="cmsy-9">&#x2194;</span><span 
class="cmtt-9">, </span><span 
class="cmsy-9">&#x2295; </span><span 
class="cmtt-9">into </span><span 
class="cmsy-9">&#x2227;</span><span 
class="cmtt-9">, </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmtt-9">and </span><span 
class="cmsy-9">¬ </span><span 
class="cmtt-9">with the following</span>
          <span 
class="cmtt-9">rules:</span>
              <ul class="itemize1">
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2192; </span><span 
class="cmmi-9">B </span><span 
class="cmtt-9">becomes </span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">B</span>
              </li>
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2194; </span><span 
class="cmmi-9">B </span><span 
class="cmtt-9">becomes </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">B</span><span 
class="cmr-9">) </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227;¬</span><span 
class="cmmi-9">B</span><span 
class="cmr-9">)</span>
              </li>
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2295; </span><span 
class="cmmi-9">B </span><span 
class="cmtt-9">becomes </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227;¬</span><span 
class="cmmi-9">B</span><span 
class="cmr-9">) </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">B</span><span 
class="cmr-9">)</span></li></ul>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">2.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Move any </span><span 
class="cmsy-9">¬ </span><span 
class="cmtt-9">into the brackets using De Morgan&#8217;s laws:</span>
              <ul class="itemize1">
              <li class="itemize"><span 
class="cmsy-9">¬</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">B</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">becomes </span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228;¬</span><span 
class="cmmi-9">B</span>
              </li>
              <li class="itemize"><span 
class="cmsy-9">¬</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">B</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">becomes </span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227;¬</span><span 
class="cmmi-9">B</span></li></ul>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">3.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Distribute </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmtt-9">over </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmtt-9">where possible</span>
              <ul class="itemize1">
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">B </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">C</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">becomes </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">B</span><span 
class="cmr-9">) </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">C</span><span 
class="cmr-9">)</span></li></ul>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">4.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Finally, clean up any 1s and 0s with the Zero and Unit laws as well</span>
          <span 
class="cmtt-9">as the 0 and 1 complement laws.</span>
              <ul class="itemize1">
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmr-9">0 = 0</span>
              </li>
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmr-9">1 = 1</span>
              </li>
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmr-9">0 = </span><span 
class="cmmi-9">A</span>
              </li>
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmr-9">1 = 1</span>
              </li>
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2227;¬</span><span 
class="cmmi-9">A </span><span 
class="cmr-9">= 0</span>
              </li>
              <li class="itemize"><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228;¬</span><span 
class="cmmi-9">A </span><span 
class="cmr-9">= 1</span></li></ul>
          </dd></dl>
<!--l. 916--><p class="noindent" ><span 
class="cmtt-9">With the formula in CNF, it is much harder to read, but it can be used as input</span>
<span 
class="cmtt-9">into resolution algorithms.</span>
                                                                  

                                                                  
<!--l. 918--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.2.3  </span></span> <a 
 id="x1-660005.2.3"></a><span 
class="cmtt-9">Reductio ad absurdum</span></h5>
<!--l. 919--><p class="noindent" ><span 
class="cmtt-9">Reductio ad absurdum is &#8216;&#8216;reduction to an absurdity". If we deduce false then</span>
<span 
class="cmtt-9">the original clause set must be unsatisfiable, so we add the </span><span 
class="cmitt-10x-x-90">opposite </span><span 
class="cmtt-9">of what</span>
<span 
class="cmtt-9">we want to prove and if we deduce the empty clause then the opposite is</span>
<span 
class="cmtt-9">unsatisfaible. This means that what we wanted to prove originally must be</span>
<span 
class="cmtt-9">true.</span>
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.3  </span></span> <a 
 id="x1-670005.3"></a><span 
class="cmtt-9">First-Order logic</span></h4>
<!--l. 921--><p class="noindent" ><span 
class="cmtt-9">While propositional logic allows us to talk about propositions, first order</span>
<span 
class="cmtt-9">logic allows us to talk about </span><span 
class="cmitt-10x-x-90">objects</span><span 
class="cmtt-9">. Objects can be almost anything, however,</span>
<span 
class="cmtt-9">they have no semantics are just regarded as atomics. Models in first order</span>
<span 
class="cmtt-9">logic represent contexts in which we can analyse truth of sentences and</span>
<span 
class="cmtt-9">interpretations tell us how to interpret sentences in the context of a model.</span>
<span 
class="cmtt-9">Therefore sentences are true or false </span><span 
class="cmitt-10x-x-90">with respect </span><span 
class="cmtt-9">to a model and an</span>
<span 
class="cmtt-9">interpretation.</span>
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.3.1  </span></span> <a 
 id="x1-680005.3.1"></a><span 
class="cmtt-9">Syntax</span></h5>
<!--l. 923--><p class="noindent" ><span 
class="cmtt-9">The basic syntactic elements of first order logic are the symbols that</span>
<span 
class="cmtt-9">stand for objects, relations and functions. There are three kinds of</span>
<span 
class="cmtt-9">symbols:</span>
          <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">1.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Constant symbols - which stand for objects</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">2.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Predicate symbols - which stand for relations</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">3.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Function symbols - which stand for functions</span></dd></dl>
<!--l. 929--><p class="noindent" ><span 
class="cmtt-9">A function is simply a mapping between objects, for example g(that</span><span 
class="cmtt-9">_cat) =</span>
<span 
class="cmtt-9">that</span><span 
class="cmtt-9">_chair. A function can have any finite number of arguments including zero</span>
<span 
class="cmtt-9">arguments. Strictly speaking, functions must be total functions, which means</span>
<span 
class="cmtt-9">there must be a definite atom for any set of inputs. This can be a bit</span>
<span 
class="cmtt-9">problematic because sometimes we don&#8217;t want this. For example if we have a</span>
<span 
class="cmtt-9">function </span><span 
class="cmmi-9">LeftLeg</span><span 
class="cmr-9">()  </span><span 
class="cmtt-9">that takes as input any object as maps its left leg to it,</span>
<span 
class="cmtt-9">then each left leg object must also have a valid mapping in this function but</span>
<span 
class="cmtt-9">that does not make sense. The solution to this is to have an &#8216;&#8216;invisible"</span>
<span 
class="cmtt-9">object that is not anything really which the functions always map to in any</span>
<span 
class="cmtt-9">other case.</span><br 
class="newline" /><span 
class="cmtt-9">A predicate in first order logic is a statement that may be true or false</span>
<span 
class="cmtt-9">depending on the values of its variables. So it is like a truth function or a</span>
<span 
class="cmtt-9">relation. It is like a function and can have multiple arguments. An example</span>
<span 
class="cmtt-9">predicate would be:</span>
                                                                  

                                                                  
<table 
class="equation"><tr><td><a 
 id="x1-68004r13"></a>
<center class="math-display" >
<img 
src="Notes12x.png" alt="P(X,Y ):  true iff the object X is directly above object Y
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(13)</span></td></tr></table>
<!--l. 934--><p class="nopar" >
<span 
class="cmtt-9">For any set of inputs, the result of the predicate must be either true of</span>
<span 
class="cmtt-9">false. </span><hr class="figure"><div class="figure" 
> <img 
src="imgs/predicate.png" alt="PIC"  
>
</div><hr class="endfigure">
<!--l. 940--><p class="noindent" ><span 
class="cmtt-9">Below is the entire basic syntax of first order logic in Backus-Naur form:</span>
<img 
src="Notes13x.png" alt="&#x27E8;Sentence&#x27E9;            &#x2192;  &#x27E8;AtomicSentence&#x27E9; | &#x27E8;ComplexSentence&#x27E9;

&#x27E8;AtomicSentence&#x27E9;      &#x2192;  &#x27E8;Predicate&#x27E9;
                    |  &#x27E8;Predicate&#x27E9;(&#x27E8;Term&#x27E9;,...)
                    |  &#x27E8;Term &#x27E9; = &#x27E8;Term &#x27E9;
&#x27E8;ComplexSentence&#x27E9;     &#x2192;  (&#x27E8;Sentence&#x27E9;)
                    |  [&#x27E8;Sentence&#x27E9;]
                    |  ¬ &#x27E8;Sentence&#x27E9;
                    |  &#x27E8;Sentence&#x27E9; &#x2227; &#x27E8;Sentence&#x27E9;
                    |  &#x27E8;Sentence&#x27E9; &#x2228; &#x27E8;Sentence&#x27E9;
                    |  &#x27E8;Sentence&#x27E9; &#x21D2;  &#x27E8;Sentence&#x27E9;
                    |  &#x27E8;Sentence&#x27E9; &#x21D4;  &#x27E8;Sentence&#x27E9;
                    |  &#x27E8;Quantifier&#x27E9; &#x27E8;Variable&#x27E9;, ...  &#x27E8;Sentence&#x27E9;

&#x27E8;Term &#x27E9;              &#x2192;|  &#x27E8;F&#x27E8;Cuonncsttioann&#x27E9;t(&#x27E9;&#x27E8;Term &#x27E9;,...)
                    |  &#x27E8;Variable&#x27E9;

&#x27E8;Quantifier&#x27E9;          &#x2192;  &#x2200; | &#x2203;

&#x27E8;Constant&#x27E9;           &#x2192;  A | X1 | John | ...
&#x27E8;Variable&#x27E9;            &#x2192;  a | x | s | ...

&#x27E8;Predicate&#x27E9;           &#x2192;  True  | False | Raining | P | ...

&#x27E8;Function &#x27E9;           &#x2192;  f() | LeftLeg(x) | ...  " > <span 
class="cmtt-9">A</span>
<span 
class="cmtt-9">term is something that refers to an object. Constant symbols are therefore</span>
                                                                  

                                                                  
<span 
class="cmtt-9">terms but it is not always convenient to have a distinct symbol to name every</span>
<span 
class="cmtt-9">object. There are three kinds of terms:</span>
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmtt-9">Constant/Atomic term - a constant that is on its own</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Variable - a variable that is on its own</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Compound term - a function of other terms, i.e,</span>
          <span 
class="cmitt-10x-x-90">function(term)</span><span 
class="cmtt-9">. It can be arbitrarily nested, but it still</span>
          <span 
class="cmtt-9">just denotes one object.</span></li></ul>
<!--l. 978--><p class="noindent" ><span 
class="cmtt-9">Equality where </span><span 
class="cmmi-9">a </span><span 
class="cmr-9">= </span><span 
class="cmmi-9">b </span><span 
class="cmtt-9">is true iff the object referred to by </span><span 
class="cmmi-9">a </span><span 
class="cmtt-9">is the </span><span 
class="cmitt-10x-x-90">same </span><span 
class="cmtt-9">object</span>
<span 
class="cmtt-9">as the object referred to by </span><span 
class="cmmi-9">b</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">There are also two kinds of sentences:</span>
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmtt-9">Atomic sentence - an atomic sentence (or atom) is formed from</span>
          <span 
class="cmtt-9">a predicate symbol optionally followed by a parenthisised list</span>
          <span 
class="cmtt-9">of terms such as </span><span 
class="cmitt-10x-x-90">Brother(Richard, John)</span><span 
class="cmtt-9">. In other words, a</span>
          <span 
class="cmtt-9">predicate applied to the correct number of terms. Atomic</span>
          <span 
class="cmtt-9">sentences can also have complex terms as arguments, such as</span>
          <span 
class="cmitt-10x-x-90">Married(Father(Richard), Mother(John)</span><span 
class="cmtt-9">. The key is that an</span>
          <span 
class="cmtt-9">atomic sentence is true in a given model if the relation</span>
          <span 
class="cmtt-9">referred to by the predicate symbol holds among the objects</span>
          <span 
class="cmtt-9">referred to by the arguments.</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Complex sentence - a complex sentence is made from other</span>
          <span 
class="cmtt-9">sentences using connectives. Each sentence is true or false so</span>
          <span 
class="cmtt-9">we can use logical connectives like in propositional logic to</span>
          <span 
class="cmtt-9">connect the sentences. There is also a final form of complex</span>
          <span 
class="cmtt-9">sentence that uses quantifiers.</span></li></ul>
<!--l. 985--><p class="noindent" ><span 
class="cmtt-9">Quantifiers can be used to make new sentences. It lets us express properties of</span>
<span 
class="cmtt-9">entire collections of objects, instead of just enumerating the objects by name.</span>
<span 
class="cmtt-9">There are two standard quantifiers in first order logic: universal (</span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmtt-9">) and</span>
<span 
class="cmtt-9">existential (</span><span 
class="cmsy-9">&#x2203;</span><span 
class="cmtt-9">).</span><br 
class="newline" /><span 
class="cmtt-9">The universal quantifier (</span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmtt-9">) is used for sentences such as &#8216;&#8216;All kings are</span>
<span 
class="cmtt-9">persons". That statement can be written in first order logic as:</span>
<table 
class="equation"><tr><td><a 
 id="x1-68005r14"></a>
                                                                  

                                                                  
<center class="math-display" >
<img 
src="Notes14x.png" alt="&#x2200;x King(x)&#x21D2; Person(x)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(14)</span></td></tr></table>
<!--l. 990--><p class="nopar" >
<span 
class="cmtt-9">In English, this would mean &#8216;&#8216;For all </span><span 
class="cmmi-9">x</span><span 
class="cmtt-9">, if </span><span 
class="cmmi-9">x </span><span 
class="cmtt-9">is a king, then </span><span 
class="cmmi-9">x </span><span 
class="cmtt-9">is a person."</span>
<span 
class="cmtt-9">In essence, the sentence </span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;P  </span><span 
class="cmtt-9">where </span><span 
class="cmmi-9">P  </span><span 
class="cmtt-9">is any logical expression, says that </span><span 
class="cmmi-9">P</span>
<span 
class="cmtt-9">is true for </span><span 
class="cmitt-10x-x-90">every </span><span 
class="cmtt-9">possible value of </span><span 
class="cmmi-9">x</span><span 
class="cmtt-9">. The truth table definition of </span><span 
class="cmsy-9">&#x21D2; </span><span 
class="cmtt-9">is</span>
<span 
class="cmtt-9">perfect for writing general rules with universal quantifiers as we can say</span>
<span 
class="cmtt-9">&#8216;&#8216;</span><span 
class="cmmi-9">x </span><span 
class="cmsy-9">&#x2192; </span><span 
class="cmtt-9">the crown" and therefore &#8216;&#8216;the crown is a king </span><span 
class="cmsy-9">&#x21D2; </span><span 
class="cmtt-9">the crown is a</span>
<span 
class="cmtt-9">person". This is true because the crown is not a king and therefore not a</span>
<span 
class="cmtt-9">person. A common mistake is to use </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmtt-9">instead of </span><span 
class="cmsy-9">&#x21D2; </span><span 
class="cmtt-9">for universality, giving the</span>
<span 
class="cmtt-9">statement:</span>
<table 
class="equation"><tr><td><a 
 id="x1-68006r15"></a>
<center class="math-display" >
<img 
src="Notes15x.png" alt="&#x2200;x King(x)&#x2227; Person(x)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(15)</span></td></tr></table>
<!--l. 994--><p class="nopar" >
<span 
class="cmtt-9">This would not be what we want to represent as it gives statements such</span>
<span 
class="cmtt-9">as:</span>
<table 
class="equation"><tr><td><a 
 id="x1-68007r16"></a>
<center class="math-display" >
<img 
src="Notes16x.png" alt="The crown is a king &#x2227;The crown is a person
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(16)</span></td></tr></table>
<!--l. 998--><p class="nopar" >
<table 
class="equation"><tr><td><a 
 id="x1-68008r17"></a>
                                                                  

                                                                  
<center class="math-display" >
<img 
src="Notes17x.png" alt="John&#8217;s left leg is a king &#x2227;John&#8217;s left leg is a person
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(17)</span></td></tr></table>
<!--l. 1001--><p class="nopar" >
<br 
class="newline" /><span 
class="cmtt-9">The Existential quantifier (</span><span 
class="cmsy-9">&#x2203;</span><span 
class="cmtt-9">) makes a statement about </span><span 
class="cmitt-10x-x-90">some </span><span 
class="cmtt-9">object in the</span>
<span 
class="cmtt-9">universe without naming it. For example to say that King John has a crown on</span>
<span 
class="cmtt-9">his head, it would be:</span>
<table 
class="equation"><tr><td><a 
 id="x1-68009r18"></a>
<center class="math-display" >
<img 
src="Notes18x.png" alt="&#x2203;x Crown(x)&#x2227;OnHead (x,John)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(18)</span></td></tr></table>
<!--l. 1006--><p class="nopar" >
<span 
class="cmtt-9">This gives statements that are true iff S(X) is true for at least one possible</span>
<span 
class="cmtt-9">value of X. In other words, iff S(X) is true when we replace X in it by some</span>
<span 
class="cmtt-9">object that exists. Just as </span><span 
class="cmsy-9">&#x21D2; </span><span 
class="cmtt-9">is a natural connective for the universal</span>
<span 
class="cmtt-9">quantifier, </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmtt-9">is the natural connective for the existential quantifier. Using</span>
<span 
class="cmsy-9">&#x21D2; </span><span 
class="cmtt-9">with </span><span 
class="cmsy-9">&#x2203; </span><span 
class="cmtt-9">leads to a very weak statement, for example:</span>
<table 
class="equation"><tr><td><a 
 id="x1-68010r19"></a>
<center class="math-display" >
<img 
src="Notes19x.png" alt="&#x2203;x Crown (x) &#x21D2; OnHead(x,John)
                                                                  

                                                                  
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(19)</span></td></tr></table>
<!--l. 1010--><p class="nopar" >
<span 
class="cmtt-9">leads to the following statements:</span>
<table 
class="equation"><tr><td><a 
 id="x1-68011r20"></a>
<center class="math-display" >
<img 
src="Notes20x.png" alt="Richard is a crown&#x21D2; Richard is on John&#8217;s head
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(20)</span></td></tr></table>
<!--l. 1014--><p class="nopar" >
<table 
class="equation"><tr><td><a 
 id="x1-68012r21"></a>
<center class="math-display" >
<img 
src="Notes21x.png" alt="John&#8217;s left leg is a crown&#x21D2; John&#8217;s left leg is on John&#8217;s head
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(21)</span></td></tr></table>
<!--l. 1017--><p class="nopar" >
<span 
class="cmtt-9">Remember, implication is true if both the premise and conclusion are true, </span><span 
class="cmitt-10x-x-90">or</span>
<span 
class="cmitt-10x-x-90">if the premise is false</span><span 
class="cmtt-9">. So if the premise &#8216;&#8216;John&#8217;s left leg is a crown" is</span>
<span 
class="cmtt-9">false, then the first assertion is true and the existential is satisfied.</span>
<span 
class="cmtt-9">So the existential implication is true </span><span 
class="cmitt-10x-x-90">whenever any </span><span 
class="cmtt-9">object fails to</span>
<span 
class="cmtt-9">satisfy the premise. This is why an implication (</span><span 
class="cmsy-9">&#x21D2;</span><span 
class="cmtt-9">) is not suitable for</span>
<span 
class="cmtt-9">existential quantifiers just like how </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmtt-9">is not suitable for universal</span>
<span 
class="cmtt-9">quantifiers.</span><br 
class="newline" /><span 
class="cmtt-9">The two quantifiers are actually closely connected with each other through</span>
<span 
class="cmtt-9">negation. For example, asserting that everyone dislikes pumpkin is the same</span>
<span 
class="cmtt-9">as asserting there does not exist someone who likes pumpkin and vice</span>
<span 
class="cmtt-9">versa.</span>
<table 
class="equation"><tr><td><a 
 id="x1-68013r22"></a>
                                                                  

                                                                  
<center class="math-display" >
<img 
src="Notes22x.png" alt="&#x2200;x ¬Likes(x,Pumpkin)&#x2261; ¬&#x2203;x Likes(x,Pumpkin )
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(22)</span></td></tr></table>
<!--l. 1023--><p class="nopar" >
<span 
class="cmtt-9">Further, &#8216;&#8216;Everyone likes ice cream" means that there is no one who does not</span>
<span 
class="cmtt-9">like ice cream.</span>
<table 
class="equation"><tr><td><a 
 id="x1-68014r23"></a>
<center class="math-display" >
<img 
src="Notes23x.png" alt="&#x2200;x Likes(x,IceCream )&#x2261; ¬&#x2203;x ¬Likes(x,IceCream)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(23)</span></td></tr></table>
<!--l. 1027--><p class="nopar" >
<span 
class="cmtt-9">Additionally, </span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;</span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmmi-9">y</span><span 
class="cmmi-9">&#x00A0;s</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">x,y</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">always means the same as </span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmmi-9">y</span><span 
class="cmmi-9">&#x00A0;</span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;s</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">x,y</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">. Similarly,</span>
<span 
class="cmsy-9">&#x2203;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;</span><span 
class="cmsy-9">&#x2203;</span><span 
class="cmmi-9">y</span><span 
class="cmmi-9">&#x00A0;x</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">x,y</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">always means the same as </span><span 
class="cmsy-9">&#x2203;</span><span 
class="cmmi-9">y</span><span 
class="cmmi-9">&#x00A0;</span><span 
class="cmsy-9">&#x2203;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;s</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">x,y</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">. However </span><span 
class="cmsy-9">&#x2203;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;</span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmmi-9">y</span><span 
class="cmmi-9">&#x00A0;s</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">x,y</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">often</span>
<span 
class="cmtt-9">doesn&#8217;t means the same as </span><span 
class="cmsy-9">&#x2200;</span><span 
class="cmmi-9">y</span><span 
class="cmmi-9">&#x00A0;</span><span 
class="cmsy-9">&#x2203;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;s</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">x,y</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">. For example, &#8216;&#8216;Everybody loves</span>
<span 
class="cmtt-9">somebody" means that for every person, there is someone that person</span>
<span 
class="cmtt-9">loves.</span>
<table 
class="equation"><tr><td><a 
 id="x1-68015r24"></a>
<center class="math-display" >
<img 
src="Notes24x.png" alt="&#x2200;x &#x2203;y Loves(x,y)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(24)</span></td></tr></table>
                                                                  

                                                                  
<!--l. 1031--><p class="nopar" >
<span 
class="cmtt-9">On the other hand, to say &#8216;&#8216;There is someone who is loved by everyone" it</span>
<span 
class="cmtt-9">is</span>
<table 
class="equation"><tr><td><a 
 id="x1-68016r25"></a>
<center class="math-display" >
<img 
src="Notes25x.png" alt="&#x2203;x &#x2200;y Loves(x,y)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(25)</span></td></tr></table>
<!--l. 1035--><p class="nopar" >
<span 
class="cmtt-9">We can see that the order of the quantifications is important as it means</span>
<span 
class="cmtt-9">different things with different order. Putting parenthesis around the equations</span>
<span 
class="cmtt-9">helps makes this clearer, i.e.</span>
<table 
class="equation"><tr><td><a 
 id="x1-68017r26"></a>
<center class="math-display" >
<img 
src="Notes26x.png" alt="&#x2203;x (&#x2200;y Loves(x,y))
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(26)</span></td></tr></table>
<!--l. 1039--><p class="nopar" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.3.2  </span></span> <a 
 id="x1-690005.3.2"></a><span 
class="cmtt-9">CNF in first order logic</span></h5>
<!--l. 1041--><p class="noindent" ><span 
class="cmtt-9">CNF also turns out to be useful in first order logic, and we can convert</span>
<span 
class="cmtt-9">sentences from first order logic to CNF in a similar way to how we did it</span>
<span 
class="cmtt-9">for propositional logic, this allows us to do first order resolution.</span>
<span 
class="cmtt-9">Unsurprisingly, it is much more complicated. One thing to note is that every</span>
<span 
class="cmtt-9">sentence in first order logic can be converted into an inferentially</span>
<span 
class="cmtt-9">equivalent CNF sentence, </span><span 
class="cmitt-10x-x-90">not </span><span 
class="cmtt-9">a fully equivalent CNF sentence. Inferentially</span>
<span 
class="cmtt-9">equivalent means that the CNF sentence is unsatisfiable iff the original is</span>
<span 
class="cmtt-9">unsatisfiable. This gives us a basis for proofs by contradiction on the CNF</span>
                                                                  

                                                                  
<span 
class="cmtt-9">sentences.</span><br 
class="newline" /><span 
class="cmtt-9">The way to convert from first order logic to CNF is similar to the procedure</span>
<span 
class="cmtt-9">for propositional logic, but we add a few new steps. The main difference is</span>
<span 
class="cmtt-9">the need to eliminate existential quantifiers. The steps marked in </span><span id="textcolor1"><span 
class="cmtt-9">red</span></span>
<span 
class="cmtt-9">are the steps that are in addition to the normal propositional logic</span>
<span 
class="cmtt-9">procedure.</span>
          <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">1.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Eliminate implications (</span><span 
class="cmsy-9">&#x21D2;</span><span 
class="cmtt-9">) and any other connectives except</span>
          <span 
class="cmtt-9">for </span><span 
class="cmsy-9">&#x2227;&#x2228;¬ </span><span 
class="cmtt-9">just like in propositional logic.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">2.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Move negation inwards just like in propositional logic.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">3.</span> </dt><dd 
class="enumerate-enumitem"><span id="textcolor2"><span 
class="cmtt-9">Standardise quantified variable names apart</span></span><span 
class="cmtt-9">. This is to make sure</span>
          <span 
class="cmtt-9">the same variable name is not used to avoid confusion when</span>
          <span 
class="cmtt-9">quantifiers are dropped. For example</span>
          <table 
class="equation"><tr><td><a 
 id="x1-69004r27"></a>
          <center class="math-display" >
          <img 
src="Notes27x.png" alt="(&#x2203;x P (x))&#x2228; (&#x2203;x Q(x)) becomes (&#x2203;x P (x))&#x2228; (&#x2203;y Q(y))
          " class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(27)</span></td></tr></table>
          <!--l. 1050--><p class="nopar" >
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">4.</span> </dt><dd 
class="enumerate-enumitem"><span id="textcolor3"><span 
class="cmtt-9">Skolemise</span></span><span 
class="cmtt-9">. Skolemisation is the process of removing existential</span>
          <span 
class="cmtt-9">quantifiers by elimination. There are two different types of</span>
          <span 
class="cmtt-9">skolemisation:</span>
              <ul class="itemize1">
              <li class="itemize"><span 
class="cmtt-9">Replace with a skolem constant. This is where we can replace</span>
              <span 
class="cmtt-9">an existential variable X with a skolem constant. For</span>
              <span 
class="cmtt-9">example:</span>
              <table 
class="equation"><tr><td><a 
 id="x1-69006r28"></a>
                                                                  

                                                                  
              <center class="math-display" >
              <img 
src="Notes28x.png" alt="&#x2203;X  norwegian(X )&#x2227;mathematician(X)
              " class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(28)</span></td></tr></table>
              <!--l. 1056--><p class="nopar" >
              <span 
class="cmtt-9">becomes</span>
              <table 
class="equation"><tr><td><a 
 id="x1-69007r29"></a>
              <center class="math-display" >
              <img 
src="Notes29x.png" alt="norwegian(skolem)&#x2227; mathematician(skolem)
              " class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(29)</span></td></tr></table>
              <!--l. 1060--><p class="nopar" >
              <span 
class="cmtt-9">This works for any formulae in the form </span><span 
class="cmsy-9">&#x2203;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">x</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">. The </span><span 
class="cmitt-10x-x-90">skolem</span>
              <span 
class="cmtt-9">variable is a </span><span 
class="cmitt-10x-x-90">new model </span><span 
class="cmtt-9">where &#8216;&#8216;skolem" satisfies the</span>
              <span 
class="cmtt-9">existential sentence.</span>
              </li>
              <li class="itemize"><span 
class="cmtt-9">Replace with skolem functions. If the sentence we are trying to</span>
              <span 
class="cmtt-9">convert to CNF does not have an existential in the form</span>
              <span 
class="cmsy-9">&#x2203;</span><span 
class="cmmi-9">x</span><span 
class="cmmi-9">&#x00A0;P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">x</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">, then we must use skolem functions instead of skolem</span>
              <span 
class="cmtt-9">constants. We can think of skolem functions as a function that</span>
              <span 
class="cmtt-9">takes an </span><span 
class="cmmi-9">X </span><span 
class="cmtt-9">as input and returns the object which satisfies the</span>
              <span 
class="cmtt-9">outer function like so:</span>
              <table 
class="equation"><tr><td><a 
 id="x1-69008r30"></a>
              <center class="math-display" >
              <img 
src="Notes30x.png" alt="&#x2200;x person(x)&#x21D2; &#x2203;y mother(x,y)
              " class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(30)</span></td></tr></table>
                                                                  

                                                                  
              <!--l. 1065--><p class="nopar" >
              <span 
class="cmtt-9">becomes</span>
              <table 
class="equation"><tr><td><a 
 id="x1-69009r31"></a>
              <center class="math-display" >
              <img 
src="Notes31x.png" alt="&#x2200;x person(x)&#x21D2; mother(x,skolemf(x))
              " class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(31)</span></td></tr></table>
              <!--l. 1069--><p class="nopar" >
              <span 
class="cmtt-9">The general rule is that the arguments of the skolem function</span>
              <span 
class="cmtt-9">(skolemf()) are all the universally quantified variables in</span>
              <span 
class="cmtt-9">whose scope the existential quantifier appears. A Skolemised</span>
              <span 
class="cmtt-9">sentence is satisfiable exactly when the original sentence is</span>
              <span 
class="cmtt-9">satisfiable.</span></li></ul>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">5.</span> </dt><dd 
class="enumerate-enumitem"><span id="textcolor4"><span 
class="cmtt-9">Drop universal quantifiers</span></span><span 
class="cmtt-9">. At this point, all remaining variables</span>
          <span 
class="cmtt-9">must be universally quantified so we can just drop the universal</span>
          <span 
class="cmtt-9">quantifiers.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">6.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Finally, distribute </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmtt-9">over </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmtt-9">like in propositional logic.</span></dd></dl>
<!--l. 1075--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.4  </span></span> <a 
 id="x1-700005.4"></a><span 
class="cmtt-9">Resolution</span></h4>
<!--l. 1076--><p class="noindent" ><span 
class="cmtt-9">Resolution is a </span><span 
class="cmitt-10x-x-90">sound </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">complete </span><span 
class="cmtt-9">proof system that uses first order</span>
<span 
class="cmtt-9">predicate logic sentences in CNF form. The entire reason to convert first</span>
<span 
class="cmtt-9">order logic sentences to CNF is to use resolution. The basic idea of</span>
<span 
class="cmtt-9">resolution is simple, but gets more complicated because of first order</span>
<span 
class="cmtt-9">logic.</span>
<!--l. 1078--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.4.1  </span></span> <a 
 id="x1-710005.4.1"></a><span 
class="cmtt-9">Resolution in propositional logic</span></h5>
<!--l. 1079--><p class="noindent" ><span 
class="cmtt-9">The basic idea of resolution is we can deduce a new clause from a pair of</span>
<span 
class="cmtt-9">clauses where remove any complementary literals. Complementary literals are</span>
<span 
class="cmtt-9">simply literals where one is the identical negation of the other, for example</span>
<span 
class="cmmi-9">P  </span><span 
class="cmtt-9">and </span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">Suppose we have the two clauses </span><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">R</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">and </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">P </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">Q</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">. We can remove the</span>
<span 
class="cmtt-9">complementary literals </span><span 
class="cmmi-9">P,</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P  </span><span 
class="cmtt-9">and deduce a new clause </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">Q</span><span 
class="cmsy-9">&#x2228;</span><span 
class="cmmi-9">R</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">. The result is the</span>
<span 
class="cmtt-9">resolvent clause or the resolvent. One more aspect of the resolution rule is</span>
<span 
class="cmtt-9">that the resulting clause should only contain one copy of each literal. For</span>
<span 
class="cmtt-9">example if we resolve </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">B</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">with </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228;¬</span><span 
class="cmmi-9">B</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">, we get </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">A</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">which is just </span><span 
class="cmmi-9">A</span><span 
class="cmtt-9">.</span>
<span 
class="cmtt-9">With resolution, for any sentences </span><span 
class="cmmi-9">&#x03B1; </span><span 
class="cmtt-9">and </span><span 
class="cmmi-9">&#x03B2; </span><span 
class="cmtt-9">in propositional logic, we can</span>
<span 
class="cmtt-9">decide whether </span><span 
class="cmmi-9">&#x03B1; </span><span 
class="msam-10x-x-90">&#x22A8; </span><span 
class="cmmi-9">&#x03B2;</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">To show that </span><span 
class="cmmi-9">KB </span><span 
class="msam-10x-x-90">&#x22A8; </span><span 
class="cmmi-9">&#x03B1;</span><span 
class="cmtt-9">, we show that </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">KB </span><span 
class="cmsy-9">&#x2227;¬</span><span 
class="cmmi-9">&#x03B1;</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">is unsatisfiable, that is the</span>
<span 
class="cmtt-9">resolution of </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">KB </span><span 
class="cmsy-9">&#x2227;¬</span><span 
class="cmmi-9">&#x03B1;</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">is the empty clause. The steps for this algorithm are</span>
<span 
class="cmtt-9">as follows:</span>
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmtt-9">Given </span><span 
class="cmmi-9">KB </span><span 
class="cmtt-9">and </span><span 
class="cmmi-9">&#x03B1; </span><span 
class="cmtt-9">in CNF form, create the clause set </span><span 
class="cmmi-9">CS </span><span 
class="cmtt-9">where</span>
          <span 
class="cmmi-9">CS </span><span 
class="cmr-9">= </span><span 
class="cmmi-9">KB </span><span 
class="cmsy-9">&#x222A;¬</span><span 
class="cmmi-9">&#x03B1;</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Repeat:</span>
              <ul class="itemize2">
              <li class="itemize"><span 
class="cmtt-9">Pick any pair of clauses </span><span 
class="cmmi-9">C</span><sub><span 
class="cmr-6">1</span></sub>  <span 
class="cmtt-9">and </span><span 
class="cmmi-9">C</span><sub><span 
class="cmr-6">2</span></sub>  <span 
class="cmtt-9">from </span><span 
class="cmmi-9">CS</span>
              </li>
              <li class="itemize"><span 
class="cmtt-9">If it is not a tautology, then create the resolvent</span>
              <span 
class="cmtt-9">clause of the pair and add it to </span><span 
class="cmmi-9">CS</span></li></ul>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Until:</span>
              <ul class="itemize2">
              <li class="itemize"><span 
class="cmtt-9">Either a resolvent clause is the empty clause</span>
              </li>
              <li class="itemize"><span 
class="cmtt-9">There are no possible resolvents that are not already in</span>
              <span 
class="cmtt-9">CS</span></li></ul>
          </li>
          <li class="itemize"><span 
class="cmtt-9">If we found an empty clause, we have proved </span><span 
class="cmmi-9">KB </span><span 
class="msam-10x-x-90">&#x22A8; </span><span 
class="cmmi-9">&#x03B1;</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">If we ran out of resolvents, we have proved </span><span 
class="cmmi-9">KB </span><span 
class="msbm-10x-x-90">&#x22AD; </span><span 
class="cmmi-9">&#x03B1;</span></li></ul>
<!--l. 1099--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">5.4.2  </span></span> <a 
 id="x1-720005.4.2"></a><span 
class="cmtt-9">Resolution in first order logic</span></h5>
<!--l. 1100--><p class="noindent" ><span 
class="cmtt-9">Resolution is first order predicate logic is more complicated than in</span>
<span 
class="cmtt-9">propositional logic. The key complication is the addition of the </span><span 
class="cmitt-10x-x-90">quantified</span>
<span 
class="cmitt-10x-x-90">variables</span><span 
class="cmtt-9">. First order literals are complementary is one unifies with the</span>
<span 
class="cmtt-9">negation of the other. Below are some examples:</span>
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,b,c</span><span 
class="cmr-9">)) </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,b,c</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">resolves to </span><span 
class="cmr-9">()</span>
                                                                  

                                                                  
          </li>
          <li class="itemize"><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,b,c</span><span 
class="cmr-9">)) </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,c,c</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">cannot be resolves as </span><span 
class="cmmi-9">b</span><span 
class="cmmi-9">&#x2260;</span><span 
class="cmmi-9">c</span>
          </li>
          <li class="itemize"><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,b,c</span><span 
class="cmr-9">)) </span><span 
class="cmsy-9">&#x2227;</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,X,c</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">where </span><span 
class="cmmi-9">X </span><span 
class="cmtt-9">is a variable can be resolved to</span>
          <span 
class="cmr-9">()  </span><span 
class="cmtt-9">by setting </span><span 
class="cmmi-9">X </span><span 
class="cmr-9">= </span><span 
class="cmmi-9">b</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Similarly, </span><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,skolf</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">b</span><span 
class="cmr-9">)</span><span 
class="cmmi-9">,c</span><span 
class="cmr-9">)) </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,X,c</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">can also be resolved by</span>
          <span 
class="cmtt-9">setting </span><span 
class="cmmi-9">X </span><span 
class="cmr-9">= </span><span 
class="cmmi-9">skolf</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">b</span><span 
class="cmr-9">)</span>
          </li>
          <li class="itemize"><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,b,c</span><span 
class="cmr-9">)) </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,X,X</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">cannot be resolved as </span><span 
class="cmmi-9">X </span><span 
class="cmtt-9">can only be set</span>
          <span 
class="cmtt-9">to either </span><span 
class="cmmi-9">b </span><span 
class="cmtt-9">or </span><span 
class="cmmi-9">c</span><span 
class="cmtt-9">, not both.</span>
          </li>
          <li class="itemize"><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">Y,f</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">Y </span><span 
class="cmr-9">)</span><span 
class="cmmi-9">,f</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">f</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">Y </span><span 
class="cmr-9">)))  </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a,X,f</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">X</span><span 
class="cmr-9">))  </span><span 
class="cmtt-9">can be resolved by setting</span>
          <span 
class="cmmi-9">Y </span><span 
class="cmr-9">= </span><span 
class="cmmi-9">a </span><span 
class="cmtt-9">and </span><span 
class="cmmi-9">X </span><span 
class="cmr-9">= </span><span 
class="cmmi-9">f</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a</span><span 
class="cmr-9">)</span></li></ul>
<!--l. 1109--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark"><span 
class="cmtt-9">6  </span></span> <a 
 id="x1-730006"></a><span 
class="cmtt-9">Bayesian probabilistic reasoning</span></h3>
<!--l. 1110--><p class="noindent" ><span 
class="cmtt-9">Sometimes, logical agents have to handle </span><span 
class="cmitt-10x-x-90">uncertainty</span><span 
class="cmtt-9">, either due to partial</span>
<span 
class="cmtt-9">observation, non-determinism or a combination. For example, consider a simple</span>
<span 
class="cmtt-9">logical rule:</span>
<table 
class="equation"><tr><td><a 
 id="x1-73001r32"></a>
<center class="math-display" >
<img 
src="Notes32x.png" alt="Toothache  &#x21D2;  Cavity
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(32)</span></td></tr></table>
<!--l. 1113--><p class="nopar" >
<span 
class="cmtt-9">However this rule is wrong, not all people with toothaches have cavities. Some</span>
<span 
class="cmtt-9">might have a gum disease or another problem. This is a problem as we would need</span>
<span 
class="cmtt-9">to add an almost unlimited list of possible problems. We could turn the rule</span>
<span 
class="cmtt-9">into a causal rule, that is:</span>
<table 
class="equation"><tr><td><a 
 id="x1-73002r33"></a>
                                                                  

                                                                  
<center class="math-display" >
<img 
src="Notes33x.png" alt="Cavity  &#x21D2;  Toothache
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(33)</span></td></tr></table>
<!--l. 1117--><p class="nopar" >
<span 
class="cmtt-9">But this rule isn&#8217;t right either as not all cavities cause pain. The</span>
<span 
class="cmtt-9">issue with either rule is we would need to make it exhaustive. Trying</span>
<span 
class="cmtt-9">to use logic for this is therefore quite difficult for the following</span>
<span 
class="cmtt-9">reasons:</span>
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmtt-9">Laziness - It requires too much work to complete the list</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Theoretical ignorance - Not having enough of a complete theory</span>
          <span 
class="cmtt-9">for the domain</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Practical ignorance - Even if we know all the rules, we might</span>
          <span 
class="cmtt-9">be uncertain about a particular case</span></li></ul>
<!--l. 1124--><p class="noindent" ><span 
class="cmtt-9">The connection in our rule therefore does not follow a logical consequence in</span>
<span 
class="cmtt-9">either direction. The agent&#8217;s knowledge can only provide at best a degree of</span>
<span 
class="cmtt-9">belief. The tool we can use to deal with these degrees and beliefs and</span>
<span 
class="cmtt-9">uncertainties is probability theory.</span>
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.1  </span></span> <a 
 id="x1-740006.1"></a><span 
class="cmtt-9">Probability theory</span></h4>
<!--l. 1126--><p class="noindent" ><span 
class="cmtt-9">We can go through probability theory with a simple example of a dice roll. To</span>
<span 
class="cmtt-9">roll a six-sided dice, what is the probability it ends up showing a</span>
<span 
class="cmtt-9">6?</span>
<table 
class="equation"><tr><td><a 
 id="x1-74001r34"></a>
<center class="math-display" >
<img 
src="Notes34x.png" alt="         1
P(d= 6)= 6
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(34)</span></td></tr></table>
                                                                  

                                                                  
<!--l. 1129--><p class="nopar" >
<span 
class="cmtt-9">Probability theory does not require complete knowledge of the probabilities of</span>
<span 
class="cmtt-9">each possible world. All probabilities in our world range from 0 to 1 and</span>
<span 
class="cmtt-9">always sum to 1. It doesn&#8217;t make sense to have a probability greater than 1,</span>
<span 
class="cmtt-9">that is it is </span><span 
class="cmitt-10x-x-90">more </span><span 
class="cmtt-9">than certain the event will occur. Probabilities can also</span>
<span 
class="cmtt-9">be added with simple arithmetic, for example, what is the probability that it</span>
<span 
class="cmtt-9">shows a 1 or a 6?</span>
<table 
class="equation"><tr><td><a 
 id="x1-74002r35"></a>
<center class="math-display" >
<img 
src="Notes35x.png" alt="P(d= 1)+ P(d= 6)= 1 + 1= 2
                  6   6  6
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(35)</span></td></tr></table>
<!--l. 1133--><p class="nopar" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.1.1  </span></span> <a 
 id="x1-750006.1.1"></a><span 
class="cmtt-9">Definitions</span></h5>
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmtt-9">An event A is any subset of the sample space </span><span 
class="cmr-9">&#x03A9;</span><span 
class="cmtt-9">, for example A</span>
          <span 
class="cmtt-9">= 1,3,5 is an event of rolling an odd number.</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">Each </span><span 
class="cmmi-9">&#x03C9; </span><span 
class="cmtt-9">in </span><span 
class="cmr-9">&#x03A9;  </span><span 
class="cmtt-9">is an atomic event. For example in the set</span>
          <span 
class="cmtt-9">1,2,3,4,5,6 the possible events for </span><span 
class="cmmi-9">&#x03C9; </span><span 
class="cmtt-9">are 1, 2, 3, 4, 5, 6.</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">A random variable (r.v.) is a function from the sample space</span>
          <span 
class="cmtt-9">to some set of values. For example r.v. Odd would be</span>
          <span 
class="cmtt-9">Odd(1)=Odd(3)=Odd(5) = true and others = false.</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">A Proposition is used to refer to the event where the</span>
          <span 
class="cmtt-9">underlying random variable is true, for example </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">Odd</span><span 
class="cmr-9">)  =  0</span><span 
class="cmmi-9">.</span><span 
class="cmr-9">5</span><span 
class="cmtt-9">.</span>
          <span 
class="cmtt-9">We can then combine propositions using logical</span>
          <span 
class="cmtt-9">connections like in propositional logic like </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">Odd </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">Even</span><span 
class="cmr-9">) = 1</span>
          <span 
class="cmtt-9">or </span><span 
class="cmr-9">(</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">Odd </span><span 
class="cmsy-9">&#x2227; </span><span 
class="cmmi-9">Even</span><span 
class="cmr-9">) = 0</span></li></ul>
                                                                  

                                                                  
<!--l. 1141--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.1.2  </span></span> <a 
 id="x1-760006.1.2"></a><span 
class="cmtt-9">Kolmogorov&#8217;s Axioms for Probability</span></h5>
<!--l. 1143--><p class="noindent" >
          <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">1.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">All probabilities are between 0 and 1. In other words for any</span>
          <span 
class="cmtt-9">event A, </span><span 
class="cmr-9">0 </span><span 
class="cmsy-9">&#x2264; </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A</span><span 
class="cmr-9">) </span><span 
class="cmsy-9">&#x2264; </span><span 
class="cmr-9">1</span><span 
class="cmtt-9">.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">2.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Probability of truth = 1, Probability of false = 0.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">3.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Negation is defined as </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">a</span><span 
class="cmr-9">) = 1 </span><span 
class="cmsy-9">- </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">a</span><span 
class="cmr-9">)</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">4.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmtt-9">Sum rules probabilities is as follows:</span>
          <table 
class="equation"><tr><td><a 
 id="x1-76005r36"></a>
          <center class="math-display" >
          <img 
src="Notes36x.png" alt="P (a &#x2228;b)= P(a)+ P(b)- P (a &#x2227;b)
          " class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(36)</span></td></tr></table>
          <!--l. 1149--><p class="nopar" >
          </dd></dl>
<!--l. 1151--><p class="noindent" ><span 
class="cmtt-9">Particular to note is Kolmogorov&#8217;s fourth axiom. This can be illustrated in a</span>
<span 
class="cmtt-9">Venn diagram. </span><hr class="figure"><div class="figure" 
> <img 
src="imgs/kolmogorov-fourth.png" alt="PIC"  
>
</div><hr class="endfigure">
<!--l. 1156--><p class="noindent" ><span 
class="cmtt-9">The area which is either in A or B is the area of A + the area of B - the area</span>
<span 
class="cmtt-9">of A and B to avoid counting it twice. This is a typical inclusion-exclusion</span>
<span 
class="cmtt-9">result.</span>
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.1.3  </span></span> <a 
 id="x1-770006.1.3"></a><span 
class="cmtt-9">Conditional probability</span></h5>
<!--l. 1159--><p class="noindent" ><span 
class="cmtt-9">We can go through probability theory with a simple example of a dice roll. To</span>
<span 
class="cmtt-9">roll a six-sided dice, what is the probability it ends up showing a</span>
<span 
class="cmtt-9">6? Probabilities such as </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">d </span><span 
class="cmr-9">= 1)  </span><span 
class="cmtt-9">are called unconditional or prior</span>
<span 
class="cmtt-9">probabilities. This refers to degrees of belief in propositions </span><span 
class="cmitt-10x-x-90">in the absence</span>
<span 
class="cmitt-10x-x-90">of any other information</span><span 
class="cmtt-9">. However, usually we have some information, or</span>
<span 
class="cmtt-9">evidence. For example if we are rolling two dice, the first die may</span>
<span 
class="cmtt-9">already be showed as a 5 and we are waiting for the next die. In this</span>
<span 
class="cmtt-9">case, we are not interested in the unconditional probability of rolling</span>
                                                                  

                                                                  
<span 
class="cmtt-9">doubles, but the conditional or posterior probability of rolling doubles</span>
<span 
class="cmitt-10x-x-90">given </span><span 
class="cmtt-9">that the first die is a 5. This probability would then be written</span>
<span 
class="cmtt-9">as:</span>
<table 
class="equation"><tr><td><a 
 id="x1-77001r37"></a>
<center class="math-display" >
<img 
src="Notes37x.png" alt="P(doubles|d1 =5)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(37)</span></td></tr></table>
<!--l. 1163--><p class="nopar" >
<span 
class="cmtt-9">This is pronounced probability of doubles </span><span 
class="cmitt-10x-x-90">given </span><span 
class="cmtt-9">the first dice is a 5. They</span>
<span 
class="cmtt-9">assertion that </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">doubles</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">d</span><sub><span 
class="cmr-6">1</span></sub> <span 
class="cmr-9">= 5) = 0</span><span 
class="cmmi-9">.</span><span 
class="cmr-9">06  </span><span 
class="cmtt-9">does </span><span 
class="cmitt-10x-x-90">not </span><span 
class="cmtt-9">mean that &#8216;&#8216;Whenever </span><span 
class="cmitt-10x-x-90">doubles </span><span 
class="cmtt-9">is</span>
<span 
class="cmtt-9">true, conclude that </span><span 
class="cmmi-9">d</span><sub><span 
class="cmr-6">1</span></sub>  <span 
class="cmtt-9">is true with probability 0.06". Instead, it means that</span>
<span 
class="cmtt-9">&#8216;&#8216;Whenever </span><span 
class="cmmi-9">d</span><sub><span 
class="cmr-6">1</span></sub> <span 
class="cmr-9">= 5  </span><span 
class="cmtt-9">is true and </span><span 
class="cmitt-10x-x-90">we have no further information</span><span 
class="cmtt-9">, conclude that</span>
<span 
class="cmitt-10x-x-90">doubles </span><span 
class="cmtt-9">is true with probability 0.06". This extra condition is important. For</span>
<span 
class="cmtt-9">example if we had the further information that </span><span 
class="cmmi-9">doubles </span><span 
class="cmtt-9">was not rolled, we would</span>
<span 
class="cmtt-9">get the equation </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">doubles</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">d</span><sub><span 
class="cmr-6">1</span></sub> <span 
class="cmr-9">= 5 </span><span 
class="cmsy-9">&#x2227;¬</span><span 
class="cmmi-9">doubles</span><span 
class="cmr-9">) = 0</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">Conditional probabilities are defined in terms of unconditional probabilities</span>
<span 
class="cmtt-9">as follows:</span>
<table 
class="equation"><tr><td><a 
 id="x1-77002r38"></a>
<center class="math-display" >
<img 
src="Notes38x.png" alt="       P-(a&#x2227;b)
P(a|b)=   P(b)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(38)</span></td></tr></table>
<!--l. 1169--><p class="nopar" >
<span 
class="cmtt-9">while </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">b</span><span 
class="cmr-9">) </span><span 
class="cmmi-9">&#x003E; </span><span 
class="cmr-9">0</span><span 
class="cmtt-9">. A useful alternative formulation of the conditional probability</span>
<span 
class="cmtt-9">rule is the product rule:</span>
<table 
class="equation"><tr><td><a 
 id="x1-77003r39"></a>
                                                                  

                                                                  
<center class="math-display" >
<img 
src="Notes39x.png" alt="P (a &#x2227;b)= P (a|b)P(b)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(39)</span></td></tr></table>
<!--l. 1173--><p class="nopar" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.1.4  </span></span> <a 
 id="x1-780006.1.4"></a><span 
class="cmtt-9">Probabilistic inference</span></h5>
<!--l. 1175--><p class="noindent" ><span 
class="cmtt-9">The simple method of probabilistic inference is the computation of posterior</span>
<span 
class="cmtt-9">probabilities for query propositions given observed evidence. We can use a full</span>
<span 
class="cmtt-9">joint distribution as the knowledge base to answer questions.</span>
<div class="center" 
>
<!--l. 1176--><p class="noindent" >
<div class="tabular"> <table id="TBL-4" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-4-1g"><col 
id="TBL-4-1"></colgroup><colgroup id="TBL-4-2g"><col 
id="TBL-4-2"></colgroup><colgroup id="TBL-4-3g"><col 
id="TBL-4-3"></colgroup><colgroup id="TBL-4-4g"><col 
id="TBL-4-4"></colgroup><colgroup id="TBL-4-5g"><col 
id="TBL-4-5"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-1-1"  
class="td11">         </td><td colspan="2" style="white-space:nowrap; text-align:center;" id="TBL-4-1-2"  
class="td11">     <div class="multicolumn"  style="white-space:nowrap; text-align:center;"><span 
class="cmitt-10x-x-90">toothache</span></div>    </td><td colspan="2" style="white-space:nowrap; text-align:center;" id="TBL-4-1-4"  
class="td11">    <div class="multicolumn"  style="white-space:nowrap; text-align:center;"><span 
class="cmsy-9">¬</span><span 
class="cmitt-10x-x-90">toothache</span></div></td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-2-1"  
class="td11"> </td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-2-2"  
class="td11"> <span 
class="cmitt-10x-x-90">catch </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-2-3"  
class="td11"> <span 
class="cmsy-9">¬</span><span 
class="cmitt-10x-x-90">catch </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-2-4"  
class="td11"> <span 
class="cmitt-10x-x-90">catch </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-2-5"  
class="td11"> <span 
class="cmsy-9">¬</span><span 
class="cmitt-10x-x-90">catch  </span></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-3-1"  
class="td11">  <span 
class="cmitt-10x-x-90">cavity  </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-3-2"  
class="td11"> <span 
class="cmtt-9">0.108 </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-3-3"  
class="td11">  <span 
class="cmtt-9">0.012  </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-3-4"  
class="td11"> <span 
class="cmtt-9">0.072 </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-3-5"  
class="td11">  <span 
class="cmtt-9">0.008  </span></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-4-1"  
class="td11"> <span 
class="cmsy-9">¬</span><span 
class="cmitt-10x-x-90">cavity  </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-4-2"  
class="td11">  <span 
class="cmtt-9">0.16  </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-4-3"  
class="td11">   <span 
class="cmtt-9">0.64   </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-4-4"  
class="td11"> <span 
class="cmtt-9">0.144 </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-4-4-5"  
class="td11">  <span 
class="cmtt-9">0.576  </span></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-5-1"  
class="td11">          </td>
</tr></table></div></div>
<!--l. 1189--><p class="noindent" ><span 
class="cmtt-9">The domain of the three boolean variables </span><span 
class="cmitt-10x-x-90">Toothache</span><span 
class="cmtt-9">, </span><span 
class="cmitt-10x-x-90">Cavity </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">Catch </span><span 
class="cmtt-9">can be</span>
<span 
class="cmtt-9">expressed in a table with the probabilities. The sum of the probabilities is 1</span>
<span 
class="cmtt-9">as per the axiom and we can directly calculate the probability of any</span>
<span 
class="cmtt-9">proposition by identifying the possible worlds in which the proposition is true</span>
<span 
class="cmtt-9">and adding up their probabilities. For example the six possible worlds where</span>
<span 
class="cmmi-9">cavity </span><span 
class="cmsy-9">&#x2228; </span><span 
class="cmmi-9">toothache </span><span 
class="cmtt-9">is true are:</span>
<table 
class="equation"><tr><td><a 
 id="x1-78001r40"></a>
<center class="math-display" >
<img 
src="Notes40x.png" alt="P(cavity&#x2228; toothache)= 0.108 +0.012+ 0.072+ 0.008+ 0.016+ 0.064
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(40)</span></td></tr></table>
                                                                  

                                                                  
<!--l. 1192--><p class="nopar" >
<span 
class="cmtt-9">We can use the rule for conditional probabilities to get an expression in terms</span>
<span 
class="cmtt-9">of unconditional probabilities and then evaluate the expression from the full</span>
<span 
class="cmtt-9">joint distribution. For example, we can compute the probability of a cavity,</span>
<span 
class="cmtt-9">given evidence of a toothache as follows:</span>
<table 
class="equation"><tr><td><a 
 id="x1-78002r41"></a>
<center class="math-display" >
<img 
src="Notes41x.png" alt="P (cavity|toothache)= P-(cavity-&#x2227;toothache)
                      P(toothache)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(41)</span></td></tr></table>
<!--l. 1196--><p class="nopar" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.1.5  </span></span> <a 
 id="x1-790006.1.5"></a><span 
class="cmtt-9">Independence</span></h5>
<!--l. 1198--><p class="noindent" ><span 
class="cmtt-9">Suppose we have the probability rule </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">B</span><span 
class="cmr-9">) = </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">. This means that the event</span>
<span 
class="cmtt-9">of </span><span 
class="cmmi-9">B </span><span 
class="cmtt-9">happening does not affect the probability of the event </span><span 
class="cmmi-9">A </span><span 
class="cmtt-9">happening. We</span>
<span 
class="cmtt-9">can say then that </span><span 
class="cmmi-9">A </span><span 
class="cmtt-9">is independent of </span><span 
class="cmmi-9">B</span><span 
class="cmtt-9">. Independence is also equivalent the</span>
<span 
class="cmtt-9">other way round, if </span><span 
class="cmmi-9">A </span><span 
class="cmtt-9">is independent of </span><span 
class="cmmi-9">B</span><span 
class="cmtt-9">, then </span><span 
class="cmmi-9">B </span><span 
class="cmtt-9">is also independent of</span>
<span 
class="cmmi-9">A</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">With independence between two events we can get new formulations for the</span>
<span 
class="cmtt-9">previous conditional rules:</span>
<table 
class="equation"><tr><td><a 
 id="x1-79001r42"></a>
<center class="math-display" >
<img 
src="Notes42x.png" alt="P(A|B)= P (A ) or P(A &#x2227;B )= P(A)P(B)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(42)</span></td></tr></table>
                                                                  

                                                                  
<!--l. 1203--><p class="nopar" >
<span 
class="cmtt-9">Of course this rule can be continually expanded to more variables provided they</span>
<span 
class="cmtt-9">are all independent of each other.</span>
<table 
class="equation"><tr><td><a 
 id="x1-79002r43"></a>
<center class="math-display" >
<img 
src="Notes43x.png" alt="P (A &#x2227;B &#x2227; C &#x2227;...&#x2227; Z)= P(A)P(B )P (C )...P(Z)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(43)</span></td></tr></table>
<!--l. 1207--><p class="nopar" >
<span 
class="cmtt-9">A key thing to understand about probability is that it is </span><span 
class="cmitt-10x-x-90">not </span><span 
class="cmtt-9">about causation.</span>
<hr class="figure"><div class="figure" 
> <img 
src="imgs/correlation.png" alt="PIC"  
>
</div><hr class="endfigure">
<!--l. 1213--><p class="noindent" ><span 
class="cmtt-9">An intuitive example is given </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmsy-9">¬</span><span 
class="cmmi-9">cavity</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">toothache</span><span 
class="cmr-9">) = 0</span><span 
class="cmmi-9">.</span><span 
class="cmr-9">4</span><span 
class="cmtt-9">, we don&#8217;t think</span>
<span 
class="cmtt-9">that having a toothache has a 0.4 chance of </span><span 
class="cmitt-10x-x-90">causing </span><span 
class="cmtt-9">one not to have a</span>
<span 
class="cmtt-9">cavity.</span>
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.2  </span></span> <a 
 id="x1-800006.2"></a><span 
class="cmtt-9">Bayes&#8217; Rule</span></h4>
<!--l. 1216--><p class="noindent" ><span 
class="cmtt-9">The equation for Bayes&#8217; rule is as follows:</span>
<table 
class="equation"><tr><td><a 
 id="x1-80001r44"></a>
<center class="math-display" >
<img 
src="Notes44x.png" alt="P(A|B )= P-(B-|A)P(A)-
           P(B )
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(44)</span></td></tr></table>
<!--l. 1219--><p class="nopar" >
<span 
class="cmtt-9">Those the rule may not seem very useful, it allows us to compute the single</span>
<span 
class="cmtt-9">term </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">B</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">in terms of the three terms: </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">B</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">A</span><span 
class="cmr-9">)</span><span 
class="cmmi-9">,P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">B</span><span 
class="cmr-9">)</span><span 
class="cmmi-9">,P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">A</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">. We can often</span>
<span 
class="cmtt-9">perceive as evidence the </span><span 
class="cmitt-10x-x-90">effect </span><span 
class="cmtt-9">of some unknown </span><span 
class="cmitt-10x-x-90">cause </span><span 
class="cmtt-9">and we would like to</span>
<span 
class="cmtt-9">determine that cause. So then Bayes&#8217; rule becomes:</span>
                                                                  

                                                                  
<table 
class="equation"><tr><td><a 
 id="x1-80002r45"></a>
<center class="math-display" >
<img 
src="Notes45x.png" alt="P(cause|effect) = P(effect|cause)P(cause)-
                      P(effect)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(45)</span></td></tr></table>
<!--l. 1223--><p class="nopar" >
<span 
class="cmtt-9">The conditional probability </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmitt-10x-x-90">effect</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">cause</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">quantifies the relationship in the</span>
<span 
class="cmtt-9">causal direction, whereas </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">cause</span><span 
class="cmsy-9">|</span><span 
class="cmitt-10x-x-90">effect</span> <span 
class="cmr-9">)  </span><span 
class="cmtt-9">describes the diagnostic direction.</span>
<span 
class="cmtt-9">For example in medical diagnosis, we often have conditional probabilities on</span>
<span 
class="cmtt-9">causal relationships, that is </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">disease</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">symptoms</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">It is often easier in practice to compute </span><span 
class="cmitt-10x-x-90">causal conditional probabilities</span><span 
class="cmtt-9">, for</span>
<span 
class="cmtt-9">example what is the probability that I have these symptoms given I have this</span>
<span 
class="cmtt-9">disease. Then, what we want is to compute the </span><span 
class="cmitt-10x-x-90">diagnostic conditional</span>
<span 
class="cmitt-10x-x-90">probabilities</span><span 
class="cmtt-9">, for example what is the probability that I have this disease</span>
<span 
class="cmtt-9">given I am showing these symptoms. Most notably, </span><span 
class="cmitt-10x-x-90">diagnostic knowledge is more</span>
<span 
class="cmitt-10x-x-90">fragile than casual knowledge</span><span 
class="cmtt-9">. If there is a sudden epidemic of a disease,</span>
<span 
class="cmtt-9">the probability of the disease (the prior probability) will increase</span>
<span 
class="cmtt-9">and therefore the diagnostic probability (</span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">disease</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">symptoms</span><span 
class="cmr-9">)</span><span 
class="cmtt-9">) will</span>
<span 
class="cmtt-9">increase. However the casual information </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">symptoms</span><span 
class="cmsy-9">|</span><span 
class="cmmi-9">disease</span><span 
class="cmr-9">)  </span><span 
class="cmtt-9">remains</span>
<span 
class="cmtt-9">unaffected by the epidemic, because it simply reflects the way the disease</span>
<span 
class="cmtt-9">works.</span>
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.2.1  </span></span> <a 
 id="x1-810006.2.1"></a><span 
class="cmtt-9">Combining evidence and conditional independence</span></h5>
<!--l. 1228--><p class="noindent" ><span 
class="cmtt-9">Now that we have seen Bayes&#8217; rule with one piece of evidence, we can explore</span>
<span 
class="cmtt-9">what happens when we are giving two or more pieces of evidence. In the</span>
<span 
class="cmtt-9">toothache example, what can we conclude given more evidence?</span>
<table 
class="equation"><tr><td><a 
 id="x1-81001r46"></a>
<center class="math-display" >
<img 
src="Notes46x.png" alt="P (cavity|toothache&#x2227; catch)= P-(toothache&#x2227;-catch|cavity)P(cavity)
                               P(toothache&#x2227;catch)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(46)</span></td></tr></table>
<!--l. 1231--><p class="nopar" >
<span 
class="cmtt-9">For this to work, we need to know the conditional probabilities of the</span>
<span 
class="cmtt-9">conjunction </span><span 
class="cmmi-9">toothache</span><span 
class="cmsy-9">&#x2227;</span><span 
class="cmmi-9">catch </span><span 
class="cmtt-9">for each value of </span><span 
class="cmmi-9">cavity</span><span 
class="cmtt-9">. This might be feasible for</span>
<span 
class="cmtt-9">two values, but it does not scale up to more. Rather than have large full join</span>
<span 
class="cmtt-9">distributions and read off the table, we can use the concept of independence</span>
<span 
class="cmtt-9">again.</span><br 
class="newline" /><span 
class="cmtt-9">It would be nice if </span><span 
class="cmitt-10x-x-90">toothache </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">catch </span><span 
class="cmtt-9">were independent, but they are not.</span>
<span 
class="cmtt-9">However, these variables </span><span 
class="cmitt-10x-x-90">are </span><span 
class="cmtt-9">independent </span><span 
class="cmitt-10x-x-90">given the presence or absence of a</span>
<span 
class="cmitt-10x-x-90">cavity</span><span 
class="cmtt-9">. In other words, each is directly caused by the cavity, but neither</span>
<span 
class="cmtt-9">has a direct effect on the other. We can express this in the following</span>
<span 
class="cmtt-9">equation:</span>
<table 
class="equation"><tr><td><a 
 id="x1-81002r47"></a>
<center class="math-display" >
<img 
src="Notes47x.png" alt="P (toothache&#x2227; catch|cavity)= P (toothache|cavity)P(catch|cavity)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(47)</span></td></tr></table>
<!--l. 1237--><p class="nopar" >
<span 
class="cmtt-9">This is known as conditional independence of </span><span 
class="cmitt-10x-x-90">toothache </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">catch </span><span 
class="cmtt-9">given </span><span 
class="cmitt-10x-x-90">cavity</span><span 
class="cmtt-9">.</span>
<span 
class="cmtt-9">Now we can substitute is back into our Bayes&#8217; equation and get</span>
<table 
class="equation"><tr><td><a 
 id="x1-81003r48"></a>
<center class="math-display" >
<img 
src="Notes48x.png" alt="P (cavity|toothache&#x2227; catch)= P-(toothache|cavity)P(catch|cavity)P(cavity)
                                  P (toothache&#x2227; catch
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(48)</span></td></tr></table>
                                                                  

                                                                  
<!--l. 1242--><p class="nopar" >
<br 
class="newline" /><span 
class="cmtt-9">The general definition of conditional independence of two variables A and B,</span>
<span 
class="cmtt-9">given a third variable C is as follows:</span>
<table 
class="equation"><tr><td><a 
 id="x1-81004r49"></a>
<center class="math-display" >
<img 
src="Notes49x.png" alt="P (A &#x2227; B|C )= P(A|C)P(B|C)
" class="math-display" ></center></td><td class="equation-label"><span 
class="cmr-9">(49)</span></td></tr></table>
<!--l. 1247--><p class="nopar" >
<span 
class="cmtt-9">Conditional independence allows probabilistic systems to scale up to more</span>
<span 
class="cmtt-9">variables. Moreover, they are more commonly available than absolute</span>
<span 
class="cmtt-9">independence assertions. Conceptually, </span><span 
class="cmitt-10x-x-90">cavity </span><span 
class="cmtt-9">separates </span><span 
class="cmitt-10x-x-90">toothache </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">cavity</span>
<span 
class="cmtt-9">because it is a direct cause of both of them.</span>
<!--l. 1250--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.3  </span></span> <a 
 id="x1-820006.3"></a><span 
class="cmtt-9">Bayesian Networks</span></h4>
<!--l. 1251--><p class="noindent" ><span 
class="cmtt-9">We saw previously that a full joint probability distribution can answer any</span>
<span 
class="cmtt-9">question about the domain, but it does not scale well with a larger number of</span>
<span 
class="cmtt-9">variables. Furthermore it is tedious to specify possible worlds one by one.</span>
<span 
class="cmtt-9">Independence and conditional independence can help reduce the number</span>
<span 
class="cmtt-9">of probabilities that need to be specified to define the full joint</span>
<span 
class="cmtt-9">distribution.</span><br 
class="newline" /><span 
class="cmtt-9">With that in mind, we can introduce a new data structure to represent</span>
<span 
class="cmtt-9">dependencies among variables: a Bayesian network. A Bayesian network can</span>
<span 
class="cmtt-9">represent any full join probability distribution as a directed graph in which</span>
<span 
class="cmtt-9">each node is annotated with quantitative probability information. The full</span>
<span 
class="cmtt-9">specification is as follows:</span>
          <ul class="itemize1">
          <li class="itemize"><span 
class="cmtt-9">Each node corresponds to a random variable, which can be</span>
          <span 
class="cmtt-9">discrete or continuous.</span>
          </li>
          <li class="itemize"><span 
class="cmtt-9">A set of arrows (directed links) connects pairs of node. An</span>
          <span 
class="cmtt-9">arrow from node X to node Y says that X is the </span><span 
class="cmitt-10x-x-90">parent </span><span 
class="cmtt-9">of Y.</span>
          <span 
class="cmtt-9">The graph has no directed cycles and so is called a directed</span>
          <span 
class="cmtt-9">acyclic graph or DAG.</span>
                                                                  

                                                                  
          </li>
          <li class="itemize"><span 
class="cmtt-9">Each node </span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub>  <span 
class="cmtt-9">has a conditional probability distribution</span>
          <span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub><span 
class="cmsy-9">|</span><span 
class="cmmi-9">Parents</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub><span 
class="cmr-9">))  </span><span 
class="cmtt-9">that quantifies the effect of the parents on</span>
          <span 
class="cmtt-9">the node</span></li></ul>
<!--l. 1259--><p class="noindent" ><span 
class="cmtt-9">The set of nodes and links in the network specifies the conditional</span>
<span 
class="cmtt-9">independence relationships that hold in the domain. An arrow between X and Y</span>
<span 
class="cmtt-9">means that X has a </span><span 
class="cmitt-10x-x-90">direct influence </span><span 
class="cmtt-9">on Y. If nodes do not have an arrow</span>
<span 
class="cmtt-9">between them, that is to say they independent or conditionally independent. </span><hr class="figure"><div class="figure" 
>
<img 
src="imgs/toothache-bayes-network.png" alt="PIC"  
>
</div><hr class="endfigure">
<!--l. 1264--><p class="noindent" ><span 
class="cmtt-9">In our toothache example, the conditional independence of </span><span 
class="cmitt-10x-x-90">Toothache </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">Catch</span>
<span 
class="cmtt-9">given </span><span 
class="cmitt-10x-x-90">Cavity </span><span 
class="cmtt-9">is expressed from the topology of the network. The absence of a</span>
<span 
class="cmtt-9">link between </span><span 
class="cmitt-10x-x-90">Toothache </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">Catch </span><span 
class="cmtt-9">indicates the conditional independence. The</span>
<span 
class="cmtt-9">network therefore represents the fact that </span><span 
class="cmitt-10x-x-90">Cavity </span><span 
class="cmtt-9">is a direct cause of</span>
<span 
class="cmitt-10x-x-90">Toothache </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">Catch</span><span 
class="cmtt-9">, but there is no direct causal relationship between</span>
<span 
class="cmitt-10x-x-90">Toothache </span><span 
class="cmtt-9">and </span><span 
class="cmitt-10x-x-90">Catch</span><span 
class="cmtt-9">.</span><br 
class="newline" /><span 
class="cmtt-9">Let&#8217;s consider another example of earthquakes and burglaries:</span>
          <!--l. 1269--><p class="noindent" ><span 
class="cmitt-10x-x-90">We have a new burglar alarm installed at home. It is</span>
          <span 
class="cmitt-10x-x-90">fairly reliable at detecting a burglary, but also respond</span>
          <span 
class="cmitt-10x-x-90">on occasion to minor earthquakes. We also have two</span>
          <span 
class="cmitt-10x-x-90">neighbours, John and Mary, who have promised to call us</span>
          <span 
class="cmitt-10x-x-90">if they hear the alarm. John nearly always calls, but</span>
          <span 
class="cmitt-10x-x-90">sometimes confuses the telephone ringing with the alarm</span>
          <span 
class="cmitt-10x-x-90">and calls then too. Mary, on the other hand, often misses</span>
          <span 
class="cmitt-10x-x-90">the alarm altogether. Given the evidence of who has or</span>
          <span 
class="cmitt-10x-x-90">has not called, we would like to estimate the probability</span>
          <span 
class="cmitt-10x-x-90">of a burglary.</span>
<!--l. 1271--><p class="noindent" ><span 
class="cmtt-9">Our network structure should show how burglary and earthquakes direct affect</span>
<span 
class="cmtt-9">the probability of the alarm going off, and whether John or Mary calls depends</span>
<span 
class="cmtt-9">only on the alarm. Further, their calling is not affect by the burglaries or</span>
<span 
class="cmtt-9">earthquakes directly. </span><hr class="figure"><div class="figure" 
> <img 
src="imgs/burglar-bayes-network.png" alt="PIC"  
>
</div><hr class="endfigure">
<!--l. 1277--><p class="noindent" ><span 
class="cmtt-9">The tables in the network are conditional probability tables. Each row in the</span>
<span 
class="cmtt-9">table contains the conditional probability of each node value for a</span>
<span 
class="cmtt-9">conditioning case. A conditioning case is just a possible combination of values</span>
<span 
class="cmtt-9">for the parent nodes. The cases should be exhaustive, however once the</span>
<span 
class="cmtt-9">probability of a true value </span><span 
class="cmitt-10x-x-90">p </span><span 
class="cmtt-9">is known, the false value is simply </span><span 
class="cmr-9">1 </span><span 
class="cmsy-9">-</span><span 
class="cmmi-9">p </span><span 
class="cmtt-9">and so</span>
<span 
class="cmtt-9">we can omit the second value.</span>
<!--l. 1280--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark"><span 
class="cmtt-9">6.3.1  </span></span> <a 
 id="x1-830006.3.1"></a><span 
class="cmtt-9">Constructing Bayesian networks</span></h5>
                                                                  

                                                                  
<!--l. 1281--><p class="noindent" ><span 
class="cmtt-9">Now we must learn how to </span><span 
class="cmitt-10x-x-90">construct </span><span 
class="cmtt-9">Bayesian networks so that the resulting</span>
<span 
class="cmtt-9">joint distribution is a good representation of a given domain. A Bayesian</span>
<span 
class="cmtt-9">network is a correct representation of the domain only if each node is</span>
<span 
class="cmtt-9">conditionally independent of its other predecessors in the node ordering, given</span>
<span 
class="cmtt-9">its parents. This gives the following methodology for constructing new</span>
<span 
class="cmtt-9">networks:</span>
          <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">1.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmitt-10x-x-90">Nodes</span><span 
class="cmtt-9">: First determine the set of variables that are required</span>
          <span 
class="cmtt-9">to model the domain, the order them</span> <span 
class="cmmi-9">X</span><sub><span 
class="cmr-6">1</span></sub><span 
class="cmmi-9">,...,X</span><sub><span 
class="cmmi-6">n</span></sub><span 
class="cmtt-9">. Any order</span>
          <span 
class="cmtt-9">orders, however more compact networks will be created if the</span>
          <span 
class="cmtt-9">ordering is such that causes precede effects.</span>
          </dd><dt class="enumerate-enumitem">
       <span 
class="cmtt-9">2.</span> </dt><dd 
class="enumerate-enumitem"><span 
class="cmitt-10x-x-90">Links</span><span 
class="cmtt-9">: For </span><span 
class="cmmi-9">i </span><span 
class="cmr-9">= 1  </span><span 
class="cmtt-9">to </span><span 
class="cmmi-9">n </span><span 
class="cmtt-9">do:</span>
              <ul class="itemize1">
              <li class="itemize"><span 
class="cmtt-9">Choose from </span><span 
class="cmmi-9">X</span><sub><span 
class="cmr-6">1</span></sub><span 
class="cmmi-9">,...X</span><sub><span 
class="cmmi-6">i</span><span 
class="cmsy-6">-</span><span 
class="cmr-6">1</span></sub><span 
class="cmtt-9">, a minimal set of</span>
              <span 
class="cmtt-9">parents for </span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub>  <span 
class="cmtt-9">such that the equation </span><span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub><span 
class="cmsy-9">|</span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span><span 
class="cmsy-6">-</span><span 
class="cmr-6">1</span></sub><span 
class="cmmi-9">,...,X</span><sub><span 
class="cmr-6">1</span></sub><span 
class="cmr-9">)  =</span>
              <span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub><span 
class="cmsy-9">|</span><span 
class="cmmi-9">Parents</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub><span 
class="cmr-9">))  </span><span 
class="cmtt-9">is satisfied</span>
              </li>
              <li class="itemize"><span 
class="cmtt-9">For each parent insert a link from the parent to </span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub>
              </li>
              <li class="itemize"><span 
class="cmitt-10x-x-90">CPTs</span><span 
class="cmtt-9">: Write down the conditional probability table</span>
              <span 
class="cmmi-9">P</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub><span 
class="cmsy-9">|</span><span 
class="cmmi-9">Parents</span><span 
class="cmr-9">(</span><span 
class="cmmi-9">X</span><sub><span 
class="cmmi-6">i</span></sub><span 
class="cmr-9">))</span></li></ul>
          </dd></dl>
 
</body></html> 

                                                                  

                                                                  
                                                                  


