* CS5014 Machine Learning
** Lecture 2 - Types of machine learning
*** Types of machine learning
- Supervised 
  - classification
  - regression

- Unsupervised
  - clustering 

*** Supervised Learning
- Use *labelled dataset* to predict the value for unknown outputs given data form the same input features
- =y= the output, what we want to predict
- =x= input, the feature set (labelled data)

Error in model is difference between the model and actual value of dataset

**** Regression example
Given the length of time to grow a pumpkin, what is its diameter:
- Data to predict is usually continuous, if discrete, have to round predicted value to the closest discrete value
- Do not extrapolate beyond the limits of the data as we will be making assumption of something unknown

**** Overfitting
If overfitting to dataset, harder to generalise outside of given dataset
- makes the model less likely to work for test data (prediction)
- fit too much to training dataset rather than new datasets

**** Classification (Logistic regression)
+ Decision boundary :: Everything on one side is one classification and another classification on the other side

Possible to have hyper-dimensional decision boundaries (more the 3 features that we care about) which cannot be easily visualised.

+ Classification :: the probability it is of some class, not it *is* this class based on distance from the decision boundary.

*** Unsupervised Learning
Using *unlabelled datasets*
- Don't know what the output is to predict values for
- Can be used to discover things we don't know about
- Automatically create clusters in the data to separate them into distinct groups

** Lecture 3 - Linear regression intro
*** Basic equations
+ Input feature :: Thing we are measuring to predict the outcome (eg, x/y variable where we use $x$ to predict $y$, $x$ is the input feature)
		   
In linear line equation $y = mx + c$, $m$ and $c$ are $\theta$, where $\theta$ are the *parameters*.

In other words, a linear function $f()$ is defined as
\begin{equation}
f(X, \theta) = \theta_{0} + \theta_{1}X_{1}
\end{equation}
Often we refer to all parameters as \theta without referring to each individual \theta_{1}, \theta{2}...

+ Loss function :: Used to evaluate the quality of the fit without manual inspection. Also known as the cost function. The goal is to reduce the error of the loss function.

The idea is then to try lots of different \theta values with the model and compute the error each time. 

*** Best model automatically
1. To get the "best" model automatically, we aim to minimise the cost function.
\begin{equation}
L(\theta) = \frac{1}{2m}\sum_{i = 1}^{m}(Y - f(X,\theta))^{2}
\end{equation}
2. We can plot the cost function to see the minimum of the cost function give values of $x$.

*** Gradient descent
1. Start with initial values of \theta
2. Change \theta values and calculate the lost function until the lost function is at a minimum
   - The issue, is how we know change the parameters such that the error of the lost function decreases

The look at the derivative of the lost function to see if we should increase or decrease the value of \theta. For example if the gradient is negative, we want to increase the \theta values and recalcualte the gradient. 

The *learning rate* \alpha dictates how big an increment to move each step. This provides better more accurate results but is much slower because more steps are needed. (Tradeoff computation time vs accuracy of model)

**** Issue of gradient descent
There is a problem with gradient descent is a local optimum value where it gets stuck at a trough without finding the global minimum.
- Can try different initial values to relieve this issue.
- Can add momentum to roll over the local minima. 

** Lecture 4 - Algoithms and Python 
Running a simple linear regression example in python to see difference between theory and practice.
- There are many practical considerations compared to theory, especially to do with preparing data to prevent floating point or division by zero errors.
- Also issues with reshaping data matrices to fit or noramlising data.
*** Importing needed modules
#+BEGIN_SRC python
  import numpy as np
  import matplotlib.pyplot as plt
  from mpl_toolkits.mplot3d import Axes3D
#+END_SRC
`matplotlib` for plotting and `Axes3D` for 3D plots.

*** Loading data
- Use `numpy` to get a matrix from text(data) file.
#+BEGIN_SRC python
  # Load matrix from file
  data = np.loadtxt("data.txt")

  # Extract inputs and outputs
  x = data[:, 1]
  y = data[:, 2]

  x = np.c_[np.ones_like(x), x]
#+END_SRC
- We prepend a column of 1s to the data to make for a nicer dot product.

*** Plotting the data
Create a plot of the data with `matplotlib`
#+BEGIN_SRC python
  # Get figure and axes
  fig, ax = plt.subplots()

  # Create scatter plot
  ax.scatter(x[:,1], y)
  ax.set_xlabel("Height")
  ax.set_ylabel("Weight")

  plt.show()
#+END_SRC
- Visualising the data helps to quickly check if our model may fit. For example, we can see if the general trend is roughly a line so we know a linear model could work

*** Define a model

#+BEGIN_SRC python
  def f(x, theta):
      y_hat = []

      for sample in x:
          y_acc = 0
          for i in range(len(sample)):
              y_acc = y_acc + theta[i] * sample[i]

          y_hat.append(y_acc)

      return np.array(y_hat)
#+END_SRC

*** Plot the model against the data
- Put some arbitrary parameters for theta and plot both the model and the data to see visually how they compare
- Now we need our cost function and optimise with the function to incrementally calculate our parameters

*** Define a cost function

#+BEGIN_SRC python
  # Return the cost (one value per sample)
  def squaredError(y, y_hat):
      return (y - y)hat)**2

  # Return mean squared error (average over all samples)
  def meanSquaredError(y, y_hat):
      return squaredError(y, y_hat).mean()
#+END_SRC
    
** Lecture 5 - Regression part 2
*** More than 1 feature
Linear model can have many features (multivariate linear regression). It can be extended to $n$ features with $n$ \theta parameters.
\begin{equation} 
f(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ... \theta_{n}x_{n}
\end{equation}

To represent our $X$s and $\theta$s, we can put them in matrix form:
\begin{equation}
X = \begin{bmatrix}
X_{0} \\
X_{1} \\
X_{2} \\
\vdots \\
X_{n}
\end{bmatrix}
\end{equation}

Update all $\theta$s at once every step rather than seperately.

*** Feature scaling
Because there can be very different scales between different features, the larger values may dominate the model. This ensures that features(inputs) have the same input range. This can typically be done by normalising the values to get all scales inputs to be between -1 and 1.
- Values can be a bit out, no necessarily -1 and 1, as long as they are in the same kind of ranges 
- Eg, $-3 \leq x \leq 3$ is okay, but not $-100 \leq x \leq 100$

The simplest way to scale is to divide all values by the maximum.

*** Polynomial regression
Have to define model equation with different polynomials for input/x values.
For example:
\begin{equation}
f(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3}
\end{equation}
becomes
\begin{equation}
f(x) = \theta_{0} + \theta{1}x_{1} + \theta_{2}(x_{2})^{2} + \theta_{3}(x_{3})^_{3}
\end{equation}

** Lecture 6 - Normal equation and logisitical regression
*** Normal equation
Another approach to determine \theta values analytically with the equation:
\begin{equation}
\theta = (X^{T}X)^{-1}X^{T}y
\end{equation}
Solves by finding when the gradient of the loss function is 0 and therefore a trough. Doesn't really do this as it would find local minima and any maxima.
*** Logistic regression
- Classifying, eg cancer or not cancer.
- Use a sigmoid function, value betwen 0 and 1, like a switch to decide between binary values

** Lecture 7 - Logisitic regression 2
*** Decision boundaries
\begin{equation}
h(x) = g(\theta^{T}x) = \frac{1}{1 + e^{-\thetaTx}}
\end{equation}
$h(x)$ is the probability that y = 1 on input x.
- Probability is never 0 or 1, but could be very close, like 0.999999. This is because we are never 100% sure of every classification. Just a very high probability
*** Cost function for logisitic regression
Need different cost function from linear regression. 
- Use two log functions so the value of the cost function tends to 0 for correct predictions and tends to \infinity for incorrect predictions.

Optimising parameter \theta
$x^{(i)} is not power, is index to input values x[i]

*** Gradient descent for logisitic regression
Use same gradient descent to incrementaly reduce error

*** Multiclass for logisitic regression
One-vs-all method:
- When there are multiple classes, only look for one. Either 1 class or all other classes (multiclass -> binary)
- Repeat for all other classes
- Generate different probability for each classification and get the highest one
** Lecture 8 - Regularization
*** Preventing overfitting
- Reduce features manually by choosing which ones to use from dataset (perhaps use less)
- Reduce automatically by changing feature space

*Regularization* where we want to prevent overfitting but keep all features
- do so by reducing the effect of each feature

*** Regularization for linear regression
Idea is to penalise parameters.
eg, $ + 1000\theta_{3} + 1000\theta_{4}$. This will cause the \theta parameters to be minimised because of the increased constant, but does not completely remove the features.

\lambda is the regularization parameter
- If \lambda is too large, then we may underfit as we have minimised the \theta parameters too much
- If \lambda is too small, then we may continue ot overfit as we have not minimised \thetas enough

Regularization parameter can also be added to the normal equation

*** Process
- Generate as simple a model as possible
- Check errors (cost function) is low
- Use trained model and test data to see if it works
- If not, try other models (higher order functions)
*** Regularization for logisitic regression

** Lecture 9 - Evaluation
*** Source of errors
- Not enough data
  -> overfit to data and cannot generalise
- Fine tuning parameters on 'test' data
- Testing data only tested once
- Use validation set for fine tuning parameters 

Coefficient of determination (R^{2}) can be negative as its not really squared
- If negative, that means the model is not very good

*** Classification errors
Good thing about classification is that there are only a certain kinds of errors, because the classification is either right or wrong, no need to worry about how close/far the predicted value is.

- Use number of correct classifications as proportion to the total is the accuracy
- Precision and recall depend on your priorities, for example may have more bias towards false positives in some cases like cancer.

*** Evaluating performance of models


*** Cross-validation
- Cross validation uses validation data to tune model without looking at test data
- Do many validation sets, eg 5-fold cross validation -> 5 sets with 1 different set as validation each time

*** Common pitfalls

** Lecture 10 - Data preparation
Need some way to turn qualitative input features such as colours into numerical values.
- Using enumeration causes implied simiarlity between features, eg red - 1, blue - 2 means distance between the two colours have a value

** Lecture 11 - Basis Expansions
*** Basis expansion
- Define basis functions $h_{m}(X)$ on input $X$
- Functions used to construct larger space of functions. 
For example
$h_{1}(X) = 1$, $h_{2}(X) = X$, $h_{3}(X) = X^{2}$
forms the basis of any 2nd degree polynomial as a linear combination.
- Instead of linear model $f(X) + \beta_{0} + \beta_{1}X$

Ridge and lasso regression when using linear regression with different regularisation
- Only need to optimise for one parameter (regularisation \lambda) rather than all \thetas
- Lasso regression could reduce weights to 0

** Lecture 12 - Bayesian classification
- Histogram very crude and sensitive because if points lie on histogram boundaries then could be low/high
probabilistic models are known as non-parametric models
-> Do not need to fit an ideal model by training and finding parameters
-> No need to train, but more time taken to process data

** Lecture 13 - Maximal Margin Classifiers

** Lecture 15 - Neutral Networks part 1


* CS4402 Constraint Programming
** Lecture 2
*** Constraint Modelling
1. Ill-defined problem statement - human level issue where objective is not clear
2. Well-defined problem statement
*Abstract constraint model* 
- View abstractly to extract common features/patterns among different problems 
- More types of decision variables with sequences/sets etc...
- Constraints will have to operate on these variables (eg, set operations)
3. Solver-independednt constraint model
4. Solver-specific constraint model

Savile row maps steps 3 and 4. *Constraint modelling* is to do with mapping from 2 to 3.

*** Sequences
- Sequences typically appear in planning problems. 
- Infinite sequences are an issue as we need a finite-domain to create a constraint solver problem
- Try to be smart and find a bound by reading the problem carefully.

** Lecture 3
*** Sets
- Equilavent sets: {1,2,3} and {2,3,1}. Modelling step for sets again introduce *equivalence class* of assignments.
- Example solution, canonical element must be in ascending order

Can use *occurence representation* as this representation does not introduce equivalence classes.
Switches multiply by values

**** Example
Explicit:
|    1 |    2 |    3 | ... |    n |
| 0..9 | 0..9 | 0..9 | ... | 0..9 |

Switches (occurence)
|   1 |   2 |   3 | ... | n   |
| 0,1 | 0,1 | 0,1 | ... | 0,1 |

*Implied constraint* Add implied constraints explicitly even if other constraints already form it in the model to help the solver.

** Lecture 4
*** Multisets
Like sets, but repetition is allowed
- Have to be careful as domain can be unbounded due to infinite repetition

*** Relation
Cross product of set to get tuples which can be assigned true or false

3d matrix for relation representation (A x B x C)
- Not enough space to express more relations
eg if there is (1,2,4) relation, cannot place (1,2,5) relation -> Therefore need 3 dimensions

|   |       2 | 3 | 4 |
| 1 | {4,5,6} |   |   |
| 2 |         |   |   |
| 3 |         |   |   |
Values of A on the rows and B on the columns to express relation to values of C.

*** Function
Mapping set of inputs to outputs

*Total functon* - everything is directly mapped

Again use dummy value 0 for partial explicit injection. Not very messy, but not as nice as occurence relation.

*Surjection* - All elements of codomain are used

** Lecture 5 - Symmetry
- Symmetry preserves solutions
If there is a solution, symmetrical instances will also have solutions and vice versa.

Use symmetry breaking constraints to remove symmetry from the problem, so we can prune the search tree

Because of symmetry permutations, requires factorial number of constraints
** Week 6 tutorial
Q2)
[0,0,1]
[1,1,0]

[0,1,1]
[1,0,0]

Q3)
[0,1,1]
[1,0,0]
[1,0,0]

[1,1,0]
[0,0,1]
[0,0,1]

[0,0,1]
[0,0,1]
[1,1,0]

[0,0,1]
[0,0,1]
[1,1,0]

** Lecture 6
*** Branching

** Practical 1 notes
Part 3 mazes:
- maze with one path but many steps
- maze with many paths but few steps
- see what happens with more steps than required for solution
- 

Extension: Model the problem from a different viewpoint (different variables/solutions) and compare against given model


** Lecture 11 - Search
There are other methods of search that are nor systematic
- does not guarantee going through entire space, but searches much faster

*** Definitions
d-way branching
- a child node in the tree for each value in the domain

2-way/binary branching
- only ever have two branches
- on left branch, make assignment of variable
- on right branch, csp where domain doesn't have the left branch's assignment
  - remove left assignment from domain rather than do another assignment

- Removals from domains must be undone when coming back up from the search tree

*** Methods

*** AC3
- Instead of having to do many full iterations like in AC1, only add new arc checks to the queue if a change in a domain affects other domains with connected arcs.
- Fully connected structures are the pathological case because every domain has to be revised (added to the queue) every time, but still more efficient that always revising every single arc


** Lecture 12
*** Forward checking 2 way version (to implement for practical)
#+BEGIN_SRC 
var = selectVar(varList)
val = selectVal(var.domain)
#+END_SRC

Both selection of variable from list and value chosen from domain can be chosen using some heuristics

**** Left branch
#+BEGIN_SRC 
reviseFutureArcs(varList, var)
#+END_SRC
reviseFutureArcs can do some pruning, so it must be undone if the branch returns without successful complete assignment


**** Right branch
Still have to reviseFutureArcs even after deleting the value from the domain
- Because deletion could cause issues with other arcs (consistency)

**** Implement basic backtracking
Can implement basic backtracking in addition to forward checking/pruning
- Use to check that answers are correct
- Use for empirical testing to compare the two methods

**** Implement different heuristics
- Different heuristics (simple/complex) can be implemented and compared for practical


*** Heuristics
**** Static variable heuristics

**** Dynamic heuristics
Heuristics that change based on the current state
- Eg, smallest-domain as it changes based on removed/assigned values in variable domain

* CS4204 Concurrency
** Lecture 1
*** Books
- Haskell Craft of functional programming, Simon Thompson
- Parallel Haskell: Lightweight parallelism for heavyweight functional programs
- Programming in Haskell, Graham Hutton

*** Multicore
- *Processor*: logical rather than physical hardware (processing unit)
- Shared data bus: contention as all cores use same bus to shared memory

*** Manycore
- Chain multiple multicores to talk to each other
- 

*** Concurrency vs parallelism
- Concurrency is an illusion of parallelism
- It is possible for a concurrent program to run on a single core but not the other way around
- 

** Lecture 2
*** Tuple
(1,"a", 1.2)
- Each element can be of different types
#+BEGIN_SRC haskell
(1, "a", 1.2) :: (Num a, Fractional b) => (a, [char], b)
#+END_SRC

*** Lists
[1,2,3]
- Lists must be of the same type
- Must always have a time, unlike tuple
#+BEGIN_SRC haskell
() :: () -- empty tuple
[] :: [t] -- empty list
#+END_SRC
 
** Lecture 3 Basic parallel haskell constructs
*** Concurrency vs parallelism
Concurrency is the illusion that tasks are happening at the same time
- eg doing tasks fast enough to appear things are happening in parallel
- Concurrent threads can be executed sequentially *or* in parallel
- Certain things cannot happen in concurrency, for example need for locks because the sequence matters


Parallelism is really doing tasks in parallel.
- We want to be efficieny in terms of energy. Do more with less energy(battery)
- Parallelism should be interleaved without any issue
- Parallelism is executing simultaneously on separate hardware devices.

*** Threads vs processes
Process made up of many threads.
Eg, Jh group vs each individual student

Process thread model don't have registers so that it is not bound to hardware. 
- If threads have a register set, then only so many threads can be run on some hardware.

*Threads* run at the basic level on the system. They work together to form a process.

*Filaments* wind together to form threads. Computation with no communication.
- Filaments don't neccesarily need context switching if they are small enough because they will terminate with a result.
- Still dangerous if large computation

*** Granularity
How big or small something is. 
- Is it fine grained or coarse grained
- Too coarse grained means not enough parallelism, too fine grained maybe too much parallelsim
- Main issue with parallelism is to determine dependencies between tasks.

*** Amdahl's Law
- The need for sequential operations heavily dominates computation
- Need a lot of parallelism to get any good speed up

*** Parallel Haskell
Evaluate and die
- No need to spark new threads if not enough hardware
- Shared spark pool leads to single point of contention
** Lecture 4 Locks and lock-free
*** System model - shared memory
*Memory hierarchy*
- Software threads, scheduled by the OS
- Processing unit, hardware thread of execution
- Physical resources shared through hyperthreading
- Chips running the hardware threads accessing shared memory

*Correctness* Must write to correct places in memory or things go wrong quickly!

*** Locks
Basic building block mechanism. 
- ~compareAndSwap~ used to build more complicated locking mechanisms
- Implemented in the assembly languages

- Need for locks means there is somewhere in the program where there is contention
- Perhaps better to reduce contention rather than having to worry about locks

**** Issues
- Busywait
#+BEGIN_SRC 
while(accquire_lock(lock)) {
    //do things
}
#+END_SRC
Process keeps looping try to get the lock when another process currently holds the lock
- To improve, can loop around waiting without looping the test and set
- Do not try to testAndSet until the lock is free, rather than trying even when it is held
- Better because only need read access (can be shared among other threads)
- All threads rush to accquire the lock once it is freed

Use exponential back-off algorithms to prevent everyone trying to accquire at the same time
- Bias towards most recent threads as they will have smaller #c# valuse

Lock master as a 'scheduler' to choose first threads go enxt


- If have to wait for a *very* long time, may want to put thread to sleep instead of letting it busy loop
- Only worth if the cost of context switching is less than the waste computation of busy looping

*** Queue-based locks - Smoother way of having threads accquire locks - No thundering herd as only the thread at the tail of the queue gets woken up 
*** Lock-free algorithms
- Allow recovery when threads who 'own' the lock die without releasing the lock

** Lecture 5 Patterns and data parallelism
*** Parallel patterns
*Patterns can have different implementations*
Eg map has a parallel pattern:
Parallel map - spark for each element in the map
Task farm - only as many parallel threads as there are parallel workers

*** Data parallelism
Apply parallelism to each element of data
- regular structure
- easy to reason about performance

Data parallelism works well on GPUs because of fine-grain

*** Task parallelism
Parallelism from the control of the program rather than structure of the data.
Eg, divide and conquer


*** BSP
- Synchronisation by barrier
  - Wait until all exchange is done before any thread starts again
  - Potentially very expensive is many cores are bloced waiting for the end of exchange

- Easy to construct timing model for BSP
  - Get time for each superstep and time for each exchange



Threadscope for system usage visualisation
ghc -eventlog
./prog +RTS -ls -lf


-feager-blackholing

- Irregular computation means some cores may be idle/waiting, so not full usage

** Lecture 6 - Task parallelism
- Because task parallelism is less regular, harder to find patterns like data parallelism
- Unfortunately not all problems in the real world are data parallel, so need to figure out how to parallelise by task

*** Producer/consumer
- Only works with streams of results
- Otherwise it is just sequential


*** Divide and conquer
- Divide and conquer with threshold
  > Below threshold do sequentially

*** Task Farm
Allocate all tasks to workers
- can get on with the next task if one worker finishes early
- better load balance as can choose number of workers to number of cores

flag

** Lecture 7 - Skeletons
Skeleton - specific implementation of patterns of parallelism
- Patterns are design time ideas (thoughts)
- Skeletons are specific implementations of skeletons

Should think of patterns and not skeletons, otherwise will be driven to using/sticking with skeleton rather than how the pattern should be applied
