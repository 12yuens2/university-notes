* CS5014 Machine Learning
** Lecture 2 - Types of machine learning
*** Types of machine learning
- Supervised 
  - classification
  - regression

- Unsupervised
  - clustering 

*** Supervised Learning
- Use *labelled dataset* to predict the value for unknown outputs given data form the same input features
- =y= the output, what we want to predict
- =x= input, the feature set (labelled data)

Error in model is difference between the model and actual value of dataset

**** Regression example
Given the length of time to grow a pumpkin, what is its diameter:
- Data to predict is usually continuous, if discrete, have to round predicted value to the closest discrete value
- Do not extrapolate beyond the limits of the data as we will be making assumption of something unknown

**** Overfitting
If overfitting to dataset, harder to generalise outside of given dataset
- makes the model less likely to work for test data (prediction)
- fit too much to training dataset rather than new datasets

**** Classification (Logistic regression)
+ Decision boundary :: Everything on one side is one classification and another classification on the other side

Possible to have hyper-dimensional decision boundaries (more the 3 features that we care about) which cannot be easily visualised.

+ Classification :: the probability it is of some class, not it *is* this class based on distance from the decision boundary.

*** Unsupervised Learning
Using *unlabelled datasets*
- Don't know what the output is to predict values for
- Can be used to discover things we don't know about
- Automatically create clusters in the data to separate them into distinct groups

** Lecture 3 - Linear regression intro
*** Basic equations
+ Input feature :: Thing we are measuring to predict the outcome (eg, x/y variable where we use $x$ to predict $y$, $x$ is the input feature)
		   
In linear line equation $y = mx + c$, $m$ and $c$ are $\theta$, where $\theta$ are the *parameters*.

In other words, a linear function $f()$ is defined as
\begin{equation}
f(X, \theta) = \theta_{0} + \theta_{1}X_{1}
\end{equation}
Often we refer to all parameters as \theta without referring to each individual \theta_{1}, \theta{2}...

+ Loss function :: Used to evaluate the quality of the fit without manual inspection. Also known as the cost function. The goal is to reduce the error of the loss function.

The idea is then to try lots of different \theta values with the model and compute the error each time. 

*** Best model automatically
1. To get the "best" model automatically, we aim to minimise the cost function.
\begin{equation}
L(\theta) = \frac{1}{2m}\sum_{i = 1}^{m}(Y - f(X,\theta))^{2}
\end{equation}
2. We can plot the cost function to see the minimum of the cost function give values of $x$.

*** Gradient descent
1. Start with initial values of \theta
2. Change \theta values and calculate the lost function until the lost function is at a minimum
   - The issue, is how we know change the parameters such that the error of the lost function decreases

The look at the derivative of the lost function to see if we should increase or decrease the value of \theta. For example if the gradient is negative, we want to increase the \theta values and recalcualte the gradient. 

The *learning rate* \alpha dictates how big an increment to move each step. This provides better more accurate results but is much slower because more steps are needed. (Tradeoff computation time vs accuracy of model)

**** Issue of gradient descent
There is a problem with gradient descent is a local optimum value where it gets stuck at a trough without finding the global minimum.
- Can try different initial values to relieve this issue.
- Can add momentum to roll over the local minima. 

** Lecture 4 - Algoithms and Python 
Running a simple linear regression example in python to see difference between theory and practice.
- There are many practical considerations compared to theory, especially to do with preparing data to prevent floating point or division by zero errors.
- Also issues with reshaping data matrices to fit or noramlising data.
*** Importing needed modules
#+BEGIN_SRC python
  import numpy as np
  import matplotlib.pyplot as plt
  from mpl_toolkits.mplot3d import Axes3D
#+END_SRC
`matplotlib` for plotting and `Axes3D` for 3D plots.


*** Loading data
- Use `numpy` to get a matrix from text(data) file.
#+BEGIN_SRC python
  # Load matrix from file
  data = np.loadtxt("data.txt")

  # Extract inputs and outputs
  x = data[:, 1]
  y = data[:, 2]

  x = np.c_[np.ones_like(x), x]
#+END_SRC
- We prepend a column of 1s to the data to make for a nicer dot product.

*** Plotting the data
Create a plot of the data with `matplotlib`
#+BEGIN_SRC python
  # Get figure and axes
  fig, ax = plt.subplots()

  # Create scatter plot
  ax.scatter(x[:,1], y)
  ax.set_xlabel("Height")
  ax.set_ylabel("Weight")

  plt.show()
#+END_SRC
- Visualising the data helps to quickly check if our model may fit. For example, we can see if the general trend is roughly a line so we know a linear model could work

*** Define a model

#+BEGIN_SRC python
  def f(x, theta):
      y_hat = []

      for sample in x:
          y_acc = 0
          for i in range(len(sample)):
              y_acc = y_acc + theta[i] * sample[i]

          y_hat.append(y_acc)

      return np.array(y_hat)
#+END_SRC

*** Plot the model against the data
- Put some arbitrary parameters for theta and plot both the model and the data to see visually how they compare
- Now we need our cost function and optimise with the function to incrementally calculate our parameters

*** Define a cost function

#+BEGIN_SRC python
  # Return the cost (one value per sample)
  def squaredError(y, y_hat):
      return (y - y)hat)**2

  # Return mean squared error (average over all samples)
  def meanSquaredError(y, y_hat):
      return squaredError(y, y_hat).mean()
#+END_SRC

* CS4402 Constraint Programming
** Lecture 2
*** Constraint Modelling
1. Ill-defined problem statement - human level issue where objective is not clear
2. Well-defined problem statement
*Abstract constraint model* 
- View abstractly to extract common features/patterns among different problems 
- More types of decision variables with sequences/sets etc...
- Constraints will have to operate on these variables (eg, set operations)
3. Solver-independednt constraint model
4. Solver-specific constraint model

Savile row maps steps 3 and 4. *Constraint modelling* is to do with mapping from 2 to 3.

*** Sequences
- Sequences typically appear in planning problems. 
- Infinite sequences are an issue as we need a finite-domain to create a constraint solver problem
- Try to be smart and find a bound by reading the problem carefully.

** Lecture 3
*** Sets
- Equilavent sets: {1,2,3} and {2,3,1}. Modelling step for sets again introduce *equivalence class* of assignments.
- Example solution, canonical element must be in ascending order

Can use *occurence representation* as this representation does not introduce equivalence classes.



* CS4204 Concurrency
** Lecture 1
*** Books
- Haskell Craft of functional programming, Simon Thompson
- Parallel Haskell: Lightweight parallelism for heavyweight functional programs

*** Multicore
- *Processor*: logical rather than physical hardware (processing unit)
- Shared data bus: contention as all cores use same bus to shared memory

*** Manycore
- Chain multiple multicores to talk to each other
- 

*** Concurrency vs parallelism
- Concurrency is an illusion of parallelism
- It is possible for a concurrent program to run on a single core but not the other way around
- 

